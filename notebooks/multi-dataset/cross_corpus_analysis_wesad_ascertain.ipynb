{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING MODULES\n",
    "import glob\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "cvx_path = os.path.abspath(os.path.join('..', '..', 'cvxEDA', 'src'))\n",
    "module_path = os.path.abspath(os.path.join('..', '..', 'src'))\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy.signal as ss\n",
    "import shap\n",
    "import sys\n",
    "sys.path.append(module_path)\n",
    "\n",
    "import tools.data_reader_wesad as dr_w\n",
    "import tools.data_reader_ascertain as dr_asc\n",
    "import tools.display_tools as dt\n",
    "import tools.preprocessing as preprocessing\n",
    "import train\n",
    "\n",
    "from scipy.fft import fft, fftfreq, fftshift\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold\n",
    "from sklearn.preprocessing import normalize\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import cvxopt.solvers\n",
    "cvxopt.solvers.options['show_progress'] = False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \n",
    "    category=RuntimeWarning\n",
    ")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)# IMPORTING MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    train.Metrics.BPM, \n",
    "    train.Metrics.RMSSD, \n",
    "    train.Metrics.HF_RR, \n",
    "    train.Metrics.LF_RR, \n",
    "    # train.Metrics.IBI, \n",
    "    train.Metrics.SDNN, \n",
    "    train.Metrics.MEAN_SCL, \n",
    "    train.Metrics.SCR_RATE\n",
    "]\n",
    "\n",
    "model_phases_wesad = dr_w.Phases.PHASE_ORDER\n",
    "\n",
    "wesad_label_type = \"stai\"\n",
    "asc_label_type = dr_asc.SelfReports.AROUSAL\n",
    "\n",
    "models = {\n",
    "    # \"SVM\": SVC(C=10, gamma=1),  # C=10, gamma=1\n",
    "    # \"KNN\": KNeighborsClassifier(n_neighbors=7),\n",
    "    # \"DT\": DecisionTreeClassifier(),\n",
    "    \"LogReg\": LogisticRegression(max_iter=1000),\n",
    "    # \"Bayes\": GaussianNB(),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=50, max_features=3),\n",
    "    \"XGB\": XGBClassifier(use_label_encoder=False, objective=\"binary:logistic\", eval_metric=\"error\")\n",
    "}\n",
    "\n",
    "threshold = \"fixed\"\n",
    "asc_threshold = \"fixed\"\n",
    "test_size = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    subject  Base_STAI  TSST_STAI  Medi_1_STAI   Fun_STAI  Medi_2_STAI\n",
      "0         2  33.333333  66.666667    23.333333  26.666667    20.000000\n",
      "1         4  30.000000  56.666667    50.000000  26.666667    43.333333\n",
      "2         5  33.333333  60.000000    26.666667  40.000000    26.666667\n",
      "3         6  30.000000  50.000000    30.000000  33.333333    20.000000\n",
      "4         7  26.666667  60.000000    30.000000  23.333333    36.666667\n",
      "5         8  43.333333  60.000000    46.666667  43.333333    40.000000\n",
      "6         9  30.000000  46.666667    23.333333  33.333333    23.333333\n",
      "7        10  36.666667  66.666667    33.333333  26.666667    40.000000\n",
      "8        11  30.000000  63.333333    26.666667  20.000000    26.666667\n",
      "9        13  46.666667  70.000000    26.666667  26.666667    53.333333\n",
      "10       14  43.333333  60.000000    43.333333  40.000000    43.333333\n",
      "11       15  40.000000  63.333333    30.000000  33.333333    30.000000\n",
      "12       16  43.333333  63.333333    30.000000  36.666667    33.333333\n",
      "13       17  43.333333  73.333333    33.333333  23.333333    46.666667\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(train)\n",
    "importlib.reload(dr_w)\n",
    "\n",
    "stai_scores, dim_scores_arousal, dim_scores_valence = train.Train_WESAD.get_labels(model_phases_wesad)\n",
    "print(stai_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train:\n",
      "1    1522\n",
      "0     262\n",
      "Name: label, dtype: int64\n",
      "y_test:\n",
      "0    55\n",
      "1    15\n",
      "Name: label, dtype: int64\n",
      "Model LogReg, Predictions: [0 1], [12 58]\n",
      "Model RF, Predictions: [0 1], [11 59]\n",
      "Model XGB, Predictions: [0 1], [ 6 64]\n",
      "\n",
      "[('mean_SCL', 0.36503474097110644), ('SCR_rate', 0.34790615148435833), ('lf_rr', 0.32803677158340006), ('hf_rr', -0.0401397663341208), ('bpm', -0.1510485020310252), ('lf_hf_ratio', -0.7198460377967504), ('sdnn', -1.1643226626094145), ('rmssd', -1.4541963127225255)]\n",
      "\n",
      "\n",
      "[('sdnn', 0.14675102292348913), ('mean_SCL', 0.1444727662080973), ('rmssd', 0.13888356700995622), ('bpm', 0.12992775049903244), ('lf_hf_ratio', 0.12639099335884268), ('hf_rr', 0.10864950355766104), ('lf_rr', 0.10589296441551807), ('SCR_rate', 0.0990314320274031)]\n",
      "\n",
      "\n",
      "[('sdnn', 0.13990077), ('lf_rr', 0.13073882), ('bpm', 0.12886035), ('mean_SCL', 0.12775761), ('rmssd', 0.12377887), ('hf_rr', 0.122103505), ('SCR_rate', 0.114196084), ('lf_hf_ratio', 0.11266404)]\n",
      "\n",
      "Model evaluation metrics for LogReg:\n",
      "\tAccuracy: 0.3\n",
      "\tPrecision: 0.20689655172413793\n",
      "\tRecall: 0.8\n",
      "\tF1-score: 0.3287671232876712\n",
      "\tAUC score: 0.48181818181818187\n",
      "----------------------------------------\n",
      "\n",
      "Model evaluation metrics for RF:\n",
      "\tAccuracy: 0.2571428571428571\n",
      "\tPrecision: 0.1864406779661017\n",
      "\tRecall: 0.7333333333333333\n",
      "\tF1-score: 0.2972972972972973\n",
      "\tAUC score: 0.4303030303030303\n",
      "----------------------------------------\n",
      "\n",
      "Model evaluation metrics for XGB:\n",
      "\tAccuracy: 0.24285714285714285\n",
      "\tPrecision: 0.203125\n",
      "\tRecall: 0.8666666666666667\n",
      "\tF1-score: 0.32911392405063294\n",
      "\tAUC score: 0.4696969696969697\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN ON ASCERTAIN AND TEST ON WESAD -- ALL\n",
    "importlib.reload(train)\n",
    "importlib.reload(dr_w)\n",
    "importlib.reload(dt)\n",
    "\n",
    "\n",
    "x_a, y_a = train.Train_ASCERTAIN.get_ascertain_data(metrics, verbose=False, label_type=asc_label_type, threshold=asc_threshold, normalize=True)\n",
    "x_b, y_b = train.Train_WESAD.get_wesad_data(metrics, model_phases_wesad, verbose=False, label_type=wesad_label_type, threshold=threshold, normalize=True)\n",
    "# drop subjects with noisy data\n",
    "inds = pd.isnull(x_a).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_a = x_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_a = y_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "\n",
    "# x = x[x['subject'] != 8.0]\n",
    "# y = y[y['subject'] != 8.0]\n",
    "\n",
    "x_a = x_a.drop([\"phaseId\"], axis=1)\n",
    "x_b = x_b.drop([\"phaseId\"], axis=1)\n",
    "\n",
    "# make sure subjects from different datasets aren't labeled with the same index\n",
    "x_b[\"subject\"] = x_b[\"subject\"] + 500\n",
    "y_b[\"subject\"] = y_b[\"subject\"] + 500\n",
    "\n",
    "acc_results = {\n",
    "    # \"SVM\": [],\n",
    "    \"LogReg\": [],\n",
    "    \"RF\": [],\n",
    "    \"XGB\": []\n",
    "}\n",
    "reports = {\n",
    "    # \"SVM\": [],\n",
    "    \"LogReg\": [],\n",
    "    \"RF\": [],\n",
    "    \"XGB\": []\n",
    "}\n",
    "num_iters = 1\n",
    "get_importance = True\n",
    "for _ in range(num_iters):\n",
    "    out = train.Train_Multi_Dataset.train_across_datasets(models, x_a, y_a, x_b, y_b, by_subject=True, save_metrics=True, test_size=test_size, is_resample=False, get_importance=True, drop_subject=True)\n",
    "    for model_name in acc_results:\n",
    "        acc_results[model_name].append(out[model_name][0])\n",
    "        reports[model_name].append(out[model_name][1])\n",
    "        if get_importance:\n",
    "            try:\n",
    "                print(\"\")\n",
    "                feature_imp = list(zip(metrics + [\"lf_hf_ratio\"], out[model_name][2]))\n",
    "                feature_imp = sorted(feature_imp, key=lambda x: x[1], reverse=True)\n",
    "                print(feature_imp)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                # print(out[model_name][0][2])\n",
    "            print(\"\")\n",
    "\n",
    "for model_name in acc_results.keys():\n",
    "    print(f\"Model evaluation metrics for {model_name}:\")\n",
    "    for i in range(len(reports[model_name])):\n",
    "        report = reports[model_name][i]\n",
    "        acc = acc_results[model_name][i]\n",
    "        p = report[\"precision\"]\n",
    "        r = report[\"recall\"]\n",
    "        f1 = report[\"f1\"]\n",
    "        auc = report[\"auc\"]\n",
    "        print(f\"\\tAccuracy: {acc}\\n\\tPrecision: {p}\\n\\tRecall: {r}\\n\\tF1-score: {f1}\\n\\tAUC score: {auc}\\n\" + \"-\"*40)\n",
    "    print(\"\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WESAD PHASES TSST --------------------------------------------------\n",
      "y_train:\n",
      "1    1522\n",
      "0     262\n",
      "Name: label, dtype: int64\n",
      "y_test:\n",
      "1    13\n",
      "0     1\n",
      "Name: label, dtype: int64\n",
      "Model LogReg, Predictions: [0 1], [ 3 11]\n",
      "Model RF, Predictions: [0 1], [5 9]\n",
      "Model XGB, Predictions: [0 1], [ 3 11]\n",
      "\n",
      "[('mean_SCL', 0.36503474097110644), ('SCR_rate', 0.34790615148435833), ('lf_rr', 0.32803677158340006), ('hf_rr', -0.0401397663341208), ('bpm', -0.1510485020310252), ('lf_hf_ratio', -0.7198460377967504), ('sdnn', -1.1643226626094145), ('rmssd', -1.4541963127225255)]\n",
      "\n",
      "\n",
      "[('rmssd', 0.15077761138908052), ('mean_SCL', 0.14480298749439535), ('sdnn', 0.13587238624778714), ('bpm', 0.13324666819105518), ('lf_hf_ratio', 0.12488157389503242), ('lf_rr', 0.11382069222463745), ('hf_rr', 0.1055030567068924), ('SCR_rate', 0.09109502385111956)]\n",
      "\n",
      "\n",
      "[('sdnn', 0.13990077), ('lf_rr', 0.13073882), ('bpm', 0.12886035), ('mean_SCL', 0.12775761), ('rmssd', 0.12377887), ('hf_rr', 0.122103505), ('SCR_rate', 0.114196084), ('lf_hf_ratio', 0.11266404)]\n",
      "\n",
      "Model evaluation metrics for LogReg:\n",
      "\tAccuracy: 0.7142857142857143\n",
      "\tPrecision: 0.9090909090909091\n",
      "\tRecall: 0.7692307692307693\n",
      "\tF1-score: 0.8333333333333333\n",
      "\tAUC score: 0.38461538461538464\n",
      "----------------------------------------\n",
      "\n",
      "Model evaluation metrics for RF:\n",
      "\tAccuracy: 0.5714285714285714\n",
      "\tPrecision: 0.8888888888888888\n",
      "\tRecall: 0.6153846153846154\n",
      "\tF1-score: 0.7272727272727274\n",
      "\tAUC score: 0.3076923076923077\n",
      "----------------------------------------\n",
      "\n",
      "Model evaluation metrics for XGB:\n",
      "\tAccuracy: 0.7142857142857143\n",
      "\tPrecision: 0.9090909090909091\n",
      "\tRecall: 0.7692307692307693\n",
      "\tF1-score: 0.8333333333333333\n",
      "\tAUC score: 0.38461538461538464\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN ON ASCERTAIN AND TEST ON WESAD\n",
    "importlib.reload(train)\n",
    "importlib.reload(dr_w)\n",
    "importlib.reload(dt)\n",
    "\n",
    "\n",
    "# for j, phases_wesad in enumerate(model_phases_wesad):\n",
    "for j, phases_wesad in enumerate([dr_w.Phases.TSST]):\n",
    "    print(f\"WESAD PHASES {phases_wesad} \" + \"-\"*50)\n",
    "    phases_wesad = [phases_wesad]\n",
    "    x_a, y_a = train.Train_ASCERTAIN.get_ascertain_data(metrics, verbose=False, label_type=asc_label_type, threshold=asc_threshold, normalize=True)\n",
    "    x_b, y_b = train.Train_WESAD.get_wesad_data(metrics, phases_wesad, verbose=False, label_type=wesad_label_type, threshold=threshold, normalize=True)\n",
    "    # drop subjects with noisy data\n",
    "    inds = pd.isnull(x_a).any(axis=1).to_numpy().nonzero()[0]\n",
    "    x_a = x_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "    y_a = y_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "\n",
    "    # x = x[x['subject'] != 8.0]\n",
    "    # y = y[y['subject'] != 8.0]\n",
    "\n",
    "    x_a = x_a.drop([\"phaseId\"], axis=1)\n",
    "    x_b = x_b.drop([\"phaseId\"], axis=1)\n",
    "\n",
    "    # make sure subjects from different datasets aren't labeled with the same index\n",
    "    x_b[\"subject\"] = x_b[\"subject\"] + 500\n",
    "    y_b[\"subject\"] = y_b[\"subject\"] + 500\n",
    "\n",
    "    acc_results = {\n",
    "        # \"SVM\": [],\n",
    "        \"LogReg\": [],\n",
    "        \"RF\": [],\n",
    "        \"XGB\": []\n",
    "    }\n",
    "    reports = {\n",
    "        # \"SVM\": [],\n",
    "        \"LogReg\": [],\n",
    "        \"RF\": [],\n",
    "        \"XGB\": []\n",
    "    }\n",
    "    num_iters = 1\n",
    "    get_importance = True\n",
    "    for _ in range(num_iters):\n",
    "        out = train.Train_Multi_Dataset.train_across_datasets(models, x_a, y_a, x_b, y_b, by_subject=True, save_metrics=True, test_size=test_size, is_resample=False, get_importance=True, drop_subject=True)\n",
    "        for model_name in acc_results:\n",
    "            acc_results[model_name].append(out[model_name][0])\n",
    "            reports[model_name].append(out[model_name][1])\n",
    "            if get_importance:\n",
    "                try:\n",
    "                    print(\"\")\n",
    "                    feature_imp = list(zip(metrics + [\"lf_hf_ratio\"], out[model_name][2]))\n",
    "                    feature_imp = sorted(feature_imp, key=lambda x: x[1], reverse=True)\n",
    "                    print(feature_imp)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    # print(out[model_name][0][2])\n",
    "                print(\"\")\n",
    "\n",
    "    for model_name in acc_results.keys():\n",
    "        print(f\"Model evaluation metrics for {model_name}:\")\n",
    "        for i in range(len(reports[model_name])):\n",
    "            report = reports[model_name][i]\n",
    "            acc = acc_results[model_name][i]\n",
    "            p = report[\"precision\"]\n",
    "            r = report[\"recall\"]\n",
    "            f1 = report[\"f1\"]\n",
    "            auc = report[\"auc\"]\n",
    "            print(f\"\\tAccuracy: {acc}\\n\\tPrecision: {p}\\n\\tRecall: {r}\\n\\tF1-score: {f1}\\n\\tAUC score: {auc}\\n\" + \"-\"*40)\n",
    "        print(\"\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WESAD PHASES ['Base', 'TSST', 'Medi_1', 'Fun', 'Medi_2'] --------------------------------------------------\n",
      "Ratio of positive to negative labels (0.2727272727272727) is under 0.333, oversampling positive class.\n",
      "Ratio of negative to positive labels (0.1721419185282523) is under 0.333, oversampling negative class.\n",
      "y_train:\n",
      "0    55\n",
      "1    18\n",
      "Name: label, dtype: int64\n",
      "y_test:\n",
      "1    1522\n",
      "0     506\n",
      "Name: label, dtype: int64\n",
      "Model LogReg, Predictions: [0 1], [1809  219]\n",
      "Model RF, Predictions: [0 1], [1326  702]\n",
      "Model XGB, Predictions: [0 1], [1516  512]\n",
      "\n",
      "[('bpm', 2.2287445898502254), ('mean_SCL', 1.1528805233322104), ('lf_rr', 0.8425883509363788), ('SCR_rate', 0.7385693994498729), ('rmssd', 0.46094413761750336), ('hf_rr', 0.08875769211211976), ('sdnn', -0.2568591619994259), ('lf_hf_ratio', -0.557280685356431)]\n",
      "\n",
      "\n",
      "[('bpm', 0.2966986088978107), ('mean_SCL', 0.25402405048705834), ('hf_rr', 0.1243883015937786), ('rmssd', 0.09737139386210984), ('SCR_rate', 0.07597812270734387), ('sdnn', 0.07119236540409597), ('lf_rr', 0.05601164878684688), ('lf_hf_ratio', 0.02433550826095578)]\n",
      "\n",
      "\n",
      "[('mean_SCL', 0.31245425), ('bpm', 0.28467944), ('hf_rr', 0.10528031), ('lf_rr', 0.09850226), ('rmssd', 0.07392444), ('SCR_rate', 0.07284088), ('lf_hf_ratio', 0.042620927), ('sdnn', 0.009697447)]\n",
      "\n",
      "Model evaluation metrics for LogReg:\n",
      "\tAccuracy: 0.3091715976331361\n",
      "\tPrecision: 0.776255707762557\n",
      "\tRecall: 0.1116951379763469\n",
      "\tF1-score: 0.19529006318207925\n",
      "\tAUC score: 0.5074285966561576\n",
      "----------------------------------------\n",
      "\n",
      "Model evaluation metrics for RF:\n",
      "\tAccuracy: 0.40927021696252464\n",
      "\tPrecision: 0.7307692307692307\n",
      "\tRecall: 0.33705650459921155\n",
      "\tF1-score: 0.4613309352517986\n",
      "\tAUC score: 0.48176935901897333\n",
      "----------------------------------------\n",
      "\n",
      "Model evaluation metrics for XGB:\n",
      "\tAccuracy: 0.3609467455621302\n",
      "\tPrecision: 0.720703125\n",
      "\tRecall: 0.24244415243101183\n",
      "\tF1-score: 0.36283185840707965\n",
      "\tAUC score: 0.47991772838941904\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN ON WESAD AND TEST ON ASCERTAIN\n",
    "importlib.reload(train)\n",
    "importlib.reload(dr_w)\n",
    "importlib.reload(dt)\n",
    "\n",
    "\n",
    "print(f\"WESAD PHASES {model_phases_wesad} \" + \"-\"*50)\n",
    "x_a, y_a = train.Train_WESAD.get_wesad_data(metrics, model_phases_wesad, verbose=False, label_type=wesad_label_type, threshold=threshold, normalize=True)\n",
    "x_b, y_b = train.Train_ASCERTAIN.get_ascertain_data(metrics, verbose=False, label_type=asc_label_type, threshold=asc_threshold, normalize=True)\n",
    "# drop subjects with noisy data\n",
    "inds = pd.isnull(x_b).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_b = x_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_b = y_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "\n",
    "x_a = x_a.drop([\"phaseId\"], axis=1)\n",
    "x_b = x_b.drop([\"phaseId\"], axis=1)\n",
    "\n",
    "# make sure subjects from different datasets aren't labeled with the same index\n",
    "x_b[\"subject\"] = x_b[\"subject\"] + 500\n",
    "y_b[\"subject\"] = y_b[\"subject\"] + 500\n",
    "\n",
    "acc_results = {\n",
    "    # \"SVM\": [],\n",
    "    \"LogReg\": [],\n",
    "    \"RF\": [],\n",
    "    \"XGB\": []\n",
    "}\n",
    "reports = {\n",
    "    # \"SVM\": [],\n",
    "    \"LogReg\": [],\n",
    "    \"RF\": [],\n",
    "    \"XGB\": []\n",
    "}\n",
    "num_iters = 1\n",
    "get_importance = True\n",
    "for _ in range(num_iters):\n",
    "    out = train.Train_Multi_Dataset.train_across_datasets(models, x_a, y_a, x_b, y_b, by_subject=True, save_metrics=True, test_size=test_size, is_resample=True, get_importance=get_importance, drop_subject=True)\n",
    "    for model_name in acc_results:\n",
    "        acc_results[model_name].append(out[model_name][0])\n",
    "        reports[model_name].append(out[model_name][1])\n",
    "        if get_importance:\n",
    "            try:\n",
    "                print(\"\")\n",
    "                feature_imp = list(zip(metrics + [\"lf_hf_ratio\"], out[model_name][2]))\n",
    "                feature_imp = sorted(feature_imp, key=lambda x: x[1], reverse=True)\n",
    "                print(feature_imp)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                # print(out[model_name][0][2])\n",
    "            print(\"\")\n",
    "\n",
    "for model_name in acc_results.keys():\n",
    "    print(f\"Model evaluation metrics for {model_name}:\")\n",
    "    for i in range(len(reports[model_name])):\n",
    "        report = reports[model_name][i]\n",
    "        acc = acc_results[model_name][i]\n",
    "        p = report[\"precision\"]\n",
    "        r = report[\"recall\"]\n",
    "        f1 = report[\"f1\"]\n",
    "        auc = report[\"auc\"]\n",
    "        print(f\"\\tAccuracy: {acc}\\n\\tPrecision: {p}\\n\\tRecall: {r}\\n\\tF1-score: {f1}\\n\\tAUC score: {auc}\\n\" + \"-\"*40)\n",
    "    print(\"\")\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
