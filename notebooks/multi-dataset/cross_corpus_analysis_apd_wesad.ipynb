{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we classify each phase as relatively low or high anxiety for each subject? ###\n",
    "#### APD, WESAD ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING MODULES\n",
    "import glob\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "cvx_path = os.path.abspath(os.path.join('..', '..', 'cvxEDA', 'src'))\n",
    "module_path = os.path.abspath(os.path.join('..', '..', 'src'))\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy.signal as ss\n",
    "import shap\n",
    "import sys\n",
    "sys.path.append(module_path)\n",
    "\n",
    "import tools.data_reader_apd as dr_a\n",
    "import tools.data_reader_wesad as dr_w\n",
    "import tools.display_tools as dt\n",
    "import tools.preprocessing as preprocessing\n",
    "import train\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from scipy.fft import fft, fftfreq, fftshift\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold\n",
    "from sklearn.preprocessing import normalize\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import cvxopt.solvers\n",
    "cvxopt.solvers.options['show_progress'] = False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \n",
    "    category=RuntimeWarning\n",
    ")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    train.Metrics.BPM, \n",
    "    train.Metrics.RMSSD, \n",
    "    train.Metrics.HF_RR, \n",
    "    train.Metrics.LF_RR, \n",
    "    train.Metrics.SDNN, \n",
    "    # train.Metrics.MEAN_SCL, \n",
    "    # train.Metrics.SCR_RATE,\n",
    "# ]\n",
    "] + train.Metrics.STATISTICAL\n",
    "\n",
    "model_phases_apd = [\n",
    "    \"Baseline_Rest\", \n",
    "    \"BugBox_Relax\", \"BugBox_Anticipate\", \"BugBox_Exposure\", \"BugBox_Break\",\n",
    "    \"Speech_Relax\", \"Speech_Anticipate\", \"Speech_Exposure\", \"Speech_Break\"\n",
    "]\n",
    "\n",
    "model_phases_wesad = dr_w.Phases.PHASE_ORDER\n",
    "\n",
    "anxiety_label_type = None\n",
    "wesad_label_type = \"stai\"\n",
    "\n",
    "models = {\n",
    "    \"SVM\": SVC(),\n",
    "    \"LGB\": LGBMClassifier(),\n",
    "    \"RF\": RandomForestClassifier(random_state=16),\n",
    "    \"XGB\": XGBClassifier(random_state=16),\n",
    "    # \"random\": None\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    \"SVM\": [{\n",
    "        \"kernel\": [\"rbf\", \"poly\", \"sigmoid\"],\n",
    "        \"C\": [0.1, 1, 10, 100],\n",
    "        \"gamma\": [1, 0.1, 0.01, 0.001, \"scale\", \"auto\"],\n",
    "    }],\n",
    "    \"LGB\": [{\n",
    "        \"objective\": [\"binary\"],\n",
    "        \"num_leaves\": [10, 20, 30, 40, 50],\n",
    "        \"max_depth\": [3, 4, 5, 6, 7],\n",
    "        \"metric\": [\"binary_logloss\"]\n",
    "    }],\n",
    "    \"RF\": [{\n",
    "        \"n_estimators\": [10, 20, 30, 40, 50],\n",
    "        \"max_features\": [\"sqrt\", \"0.4\"],\n",
    "        \"min_samples_split\": [3, 4, 5, 6, 7],\n",
    "        \"random_state\": [16]\n",
    "    }],\n",
    "    \"XGB\": [{\n",
    "        \"objective\": [\"binary:logistic\"],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.3, 0.5],\n",
    "        \"max_depth\": [4, 5, 6, 7],\n",
    "        \"n_estimators\": [10, 20, 30, 40],\n",
    "        \"eval_metric\": [\"error\", \"log_loss\"],\n",
    "        \"use_label_encoder\": [False],\n",
    "        \"random_state\": [16]\n",
    "    }],\n",
    "    \"random\": None\n",
    "}\n",
    "\n",
    "threshold = \"fixed\"\n",
    "test_size = 1.0\n",
    "\n",
    "percent_of_target_dataset = 0.0\n",
    "\n",
    "temp_a, _ = train.Train_APD.get_apd_data_ranking([train.Metrics.BPM], phases=dr_a.Phases.PHASES_LIST, normalize=False)\n",
    "idx = temp_a[temp_a[\"bpm\"] > 200].index \n",
    "invalid_apd_subjects = set(temp_a[\"subject\"].iloc[idx].tolist())\n",
    "idx = temp_a[temp_a[\"bpm\"] < 35].index \n",
    "invalid_apd_subjects.update(set(temp_a[\"subject\"].iloc[idx].tolist()))\n",
    "\n",
    "temp_a, _ = train.Train_WESAD.get_wesad_data([train.Metrics.BPM], phases=dr_w.Phases.PHASE_ORDER, normalize=False)\n",
    "idx = temp_a[temp_a[\"bpm\"] > 200].index \n",
    "invalid_wesad_subjects = set(temp_a[\"subject\"].iloc[idx].tolist())\n",
    "idx = temp_a[temp_a[\"bpm\"] < 35].index \n",
    "invalid_wesad_subjects.update(set(temp_a[\"subject\"].iloc[idx].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search for SVM ...\n",
      "Grid search for LGB ...\n",
      "Grid search for RF ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more of the test scores are non-finite: [0.54069603 0.58474335 0.58807804 0.59093686 0.58822629 0.56444981\n",
      " 0.57736226 0.58444926 0.59066571 0.59221279 0.56129983 0.57195351\n",
      " 0.57572956 0.58185966 0.58604255 0.55611161 0.5880324  0.59207695\n",
      " 0.59490232 0.59278285 0.57298959 0.58494157 0.58379885 0.59671798\n",
      " 0.59167253        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search for XGB ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more of the test scores are non-finite: [0.56645235 0.57483954 0.59021365 0.59398665 0.5389753  0.55478854\n",
      " 0.57351022 0.574695   0.52056676 0.53105607 0.5464833  0.55719086\n",
      " 0.52885674 0.53423043 0.538136   0.54487654 0.57767527 0.58960595\n",
      " 0.5936907  0.59195834 0.57065916 0.58424109 0.58133461 0.57797387\n",
      " 0.57639064 0.59185938 0.59380326 0.59196974 0.57086531 0.58099882\n",
      " 0.57967207 0.5788939  0.58742057 0.57915158 0.57152832 0.56750444\n",
      " 0.57675854 0.57733387 0.57435422 0.57507192 0.56241408 0.58135021\n",
      " 0.58435906 0.58666186 0.56971553 0.56576034 0.57029142 0.57508247\n",
      " 0.59251928 0.59087135 0.59993799 0.59783676 0.54532414 0.55366293\n",
      " 0.56456427 0.57368407 0.53187442 0.54106344 0.54469264 0.55386814\n",
      " 0.56751246 0.57508922 0.57286853 0.58170126        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: {'C': 100, 'gamma': 'auto', 'kernel': 'sigmoid'}\n",
      "LGB: {'max_depth': 7, 'metric': 'binary_logloss', 'num_leaves': 10, 'objective': 'binary'}\n",
      "RF: {'max_features': 'sqrt', 'min_samples_split': 7, 'n_estimators': 40, 'random_state': 16}\n",
      "XGB: {'eval_metric': 'error', 'learning_rate': 0.5, 'max_depth': 4, 'n_estimators': 30, 'objective': 'binary:logistic', 'random_state': 16, 'use_label_encoder': False}\n",
      "Model SVM, Actual: [0 1], [428  95], Predictions: [0 1], [ 35 488]\n",
      "coef_ only available for SVC with linear kernel\n",
      "Model LGB, Actual: [0 1], [428  95], Predictions: [0 1], [  9 514]\n",
      "Model RF, Actual: [0 1], [428  95], Predictions: [0 1], [ 22 501]\n",
      "Model XGB, Actual: [0 1], [428  95], Predictions: [0 1], [ 29 494]\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "[('ecg_kurtosis', 89), ('ecg_skew', 78), ('bpm', 72), ('ecg_iqr', 56), ('eda_std', 51), ('rmssd', 50), ('hf_rr', 49), ('eda_kurtosis', 47), ('sdnn', 41), ('ecg_var', 40), ('ecg_mean', 39), ('eda_skew', 38), ('lf_rr', 37), ('eda_var', 29), ('ecg_median', 28), ('ecg_rms', 28), ('eda_mean', 28), ('eda_rms', 26), ('eda_iqr', 24), ('eda_median', 23), ('ecg_std', 22)]\n",
      "\n",
      "\n",
      "[('lf_rr', 0.06814035062953297), ('ecg_var', 0.06633653812202839), ('ecg_kurtosis', 0.06534589903575375), ('ecg_iqr', 0.06037494748279436), ('bpm', 0.05899815544787997), ('ecg_skew', 0.056675306942622475), ('eda_skew', 0.05312897461760975), ('hf_rr', 0.04735080750840678), ('ecg_mean', 0.0469941278207371), ('eda_std', 0.04565083279212145), ('ecg_std', 0.04445792008412466), ('sdnn', 0.04429110202833001), ('eda_kurtosis', 0.04387686593741066), ('eda_var', 0.04343177864835673), ('eda_iqr', 0.04013496394630402), ('ecg_median', 0.03995299116047279), ('rmssd', 0.038440273970383035), ('eda_median', 0.03706301963320849), ('ecg_rms', 0.03475078791322764), ('eda_mean', 0.03336639760967906), ('eda_rms', 0.03123795866901593)]\n",
      "\n",
      "\n",
      "[('ecg_var', 0.10116366), ('ecg_std', 0.0751848), ('eda_rms', 0.06025514), ('bpm', 0.05666738), ('eda_std', 0.04885734), ('rmssd', 0.04795673), ('ecg_kurtosis', 0.045763817), ('ecg_iqr', 0.04408188), ('eda_kurtosis', 0.04352255), ('eda_var', 0.043040857), ('ecg_mean', 0.04253737), ('ecg_skew', 0.04218304), ('lf_rr', 0.042168695), ('eda_median', 0.04185306), ('eda_iqr', 0.041835483), ('hf_rr', 0.04050375), ('eda_skew', 0.039889008), ('ecg_median', 0.037877012), ('eda_mean', 0.03768058), ('ecg_rms', 0.03571834), ('sdnn', 0.03125949)]\n",
      "\n",
      "Model evaluation metrics for SVM:\n",
      "\tAccuracy: 0.1988527724665392\n",
      "\tPrecision: 0.1680327868852459\n",
      "\tRecall: 0.8631578947368421\n",
      "\tF1-score: 0.28130360205831906\n",
      "\tAUC score: 0.4572798819478603\n",
      "----------------------------------------\n",
      "Mean acc: 0.1988527724665392\n",
      "Mean F1-score: 0.28130360205831906\n",
      "Mean AUC score: 0.4572798819478603\n",
      "\n",
      "\n",
      "Model evaluation metrics for LGB:\n",
      "\tAccuracy: 0.18738049713193117\n",
      "\tPrecision: 0.17898832684824903\n",
      "\tRecall: 0.968421052631579\n",
      "\tF1-score: 0.3021346469622332\n",
      "\tAUC score: 0.491219872110182\n",
      "----------------------------------------\n",
      "Mean acc: 0.18738049713193117\n",
      "Mean F1-score: 0.3021346469622332\n",
      "Mean AUC score: 0.491219872110182\n",
      "\n",
      "\n",
      "Model evaluation metrics for RF:\n",
      "\tAccuracy: 0.18546845124282982\n",
      "\tPrecision: 0.16966067864271456\n",
      "\tRecall: 0.8947368421052632\n",
      "\tF1-score: 0.28523489932885904\n",
      "\tAUC score: 0.46138711264141663\n",
      "----------------------------------------\n",
      "Mean acc: 0.18546845124282982\n",
      "Mean F1-score: 0.28523489932885904\n",
      "Mean AUC score: 0.46138711264141663\n",
      "\n",
      "\n",
      "Model evaluation metrics for XGB:\n",
      "\tAccuracy: 0.1988527724665392\n",
      "\tPrecision: 0.1720647773279352\n",
      "\tRecall: 0.8947368421052632\n",
      "\tF1-score: 0.2886247877758913\n",
      "\tAUC score: 0.46956468273487456\n",
      "----------------------------------------\n",
      "Mean acc: 0.1988527724665392\n",
      "Mean F1-score: 0.2886247877758913\n",
      "Mean AUC score: 0.46956468273487456\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN ON APD AND TEST ON WESAD -- ALL\n",
    "importlib.reload(train)\n",
    "importlib.reload(dr_a)\n",
    "importlib.reload(dr_w)\n",
    "importlib.reload(dt)\n",
    "\n",
    "\n",
    "random.seed(56)\n",
    "\n",
    "x_a, y_a = train.Train_APD.get_apd_data_ranking(metrics, model_phases_apd, verbose=False, anxiety_label_type=anxiety_label_type, threshold=threshold, normalize=True, standardize=False)\n",
    "x_b, y_b = train.Train_WESAD.get_wesad_data(metrics, model_phases_wesad, verbose=False, label_type=wesad_label_type, threshold=threshold, normalize=True, standardize=False)\n",
    "# drop subjects with noisy data\n",
    "# x_a = x_a[~x_a[\"subject\"].isin(invalid_apd_subjects)].reset_index(drop=True)\n",
    "# y_a = y_a[~y_a[\"subject\"].isin(invalid_apd_subjects)].reset_index(drop=True)\n",
    "\n",
    "if anxiety_label_type is not None:\n",
    "    x_a = x_a.drop([\"anxietyGroup\"], axis=1)  # drop anxietyGroup column because WESAD doesn't have this feature\n",
    "\n",
    "x_a = x_a.drop([\"phaseId\"], axis=1)\n",
    "x_b = x_b.drop([\"phaseId\"], axis=1)\n",
    "\n",
    "inds = pd.isnull(x_a).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_a = x_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_a = y_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "inds = pd.isnull(x_b).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_b = x_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_b = y_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "\n",
    "# make sure subjects from different datasets aren't labeled with the same index\n",
    "x_b[\"subject\"] = x_b[\"subject\"] + 500\n",
    "y_b[\"subject\"] = y_b[\"subject\"] + 500\n",
    "\n",
    "acc_results = {\n",
    "    \"SVM\": [],\n",
    "    \"LGB\": [],\n",
    "    \"RF\": [],\n",
    "    \"XGB\": [],\n",
    "    # \"random\": []\n",
    "}\n",
    "reports = {\n",
    "    \"SVM\": [],\n",
    "    \"LGB\": [],\n",
    "    \"RF\": [],\n",
    "    \"XGB\": [],\n",
    "    # \"random\": []\n",
    "}\n",
    "best_models = {}\n",
    "\n",
    "num_iters = 1\n",
    "get_importance = True\n",
    "for _ in range(num_iters):\n",
    "    # include a subset of the target dataset in the training data\n",
    "    subjects = list(np.unique(x_b.loc[:, \"subject\"]))\n",
    "    train_subjects = random.sample(subjects, int(len(subjects) * percent_of_target_dataset))\n",
    "    x_train_addition = x_b[x_b[\"subject\"].isin(train_subjects)]\n",
    "    y_train_addition = y_b[y_b[\"subject\"].isin(train_subjects)]\n",
    "    x_train = pd.concat([x_a, x_train_addition])\n",
    "    y_train = pd.concat([y_a, y_train_addition])\n",
    "    x_test = x_b[~x_b[\"subject\"].isin(train_subjects)]\n",
    "    y_test = y_b[~y_b[\"subject\"].isin(train_subjects)]\n",
    "\n",
    "    # HYPERPARAMETER TUNING\n",
    "    model_data = train.grid_search_cv(\n",
    "        # models, parameters, x_a, y_a, by_subject=True, save_metrics=True, is_resample=True,\n",
    "        models, parameters, x_train, y_train, by_subject=True, save_metrics=True, is_resample=True,\n",
    "        get_importance=get_importance, drop_subject=True, test_size=0.0, folds=5\n",
    "    )\n",
    "\n",
    "    for model_name in models.keys():\n",
    "        best_models[model_name] = model_data[model_name][\"best_model\"]\n",
    "        print(f\"{model_name}: {model_data[model_name]['best_params']}\")\n",
    "\n",
    "    # # FEATURE SELECTION\n",
    "    features = {name: metrics for name in models.keys()}\n",
    "    # features = train.feature_selection(best_models, model_data[\"cv\"], x_a, y_a, n_features=5)\n",
    "    # features = train.feature_selection(best_models, model_data[\"cv\"], x_train, y_train, n_features=5)\n",
    "\n",
    "    # out = train.Train_Multi_Dataset.train_across_datasets(best_models, features, x_a, y_a, x_b, y_b, by_subject=True, save_metrics=True, test_size=test_size, is_resample=False, get_importance=get_importance, drop_subject=True)\n",
    "    out = train.Train_Multi_Dataset.train_across_datasets(best_models, features, x_train, y_train, x_test, y_test, by_subject=True, save_metrics=True, test_size=test_size, is_resample=False, get_importance=get_importance, drop_subject=True)\n",
    "    \n",
    "    for model_name in acc_results:\n",
    "        acc_results[model_name].append(out[model_name][\"performance\"][0])\n",
    "        reports[model_name].append(out[model_name][\"performance\"][1])\n",
    "        if get_importance:\n",
    "            try:\n",
    "                print(\"\")\n",
    "                feature_imp = list(zip(metrics + [\"lf_hf_ratio\"], out[model_name][\"performance\"][2]))\n",
    "                feature_imp = sorted(feature_imp, key=lambda x: x[1], reverse=True)\n",
    "                print(feature_imp)\n",
    "            except Exception as e:\n",
    "                print(out[model_name][\"performance\"][2])\n",
    "            print(\"\")\n",
    "\n",
    "for model_name in acc_results.keys():\n",
    "    print(f\"Model evaluation metrics for {model_name}:\")\n",
    "    for i in range(len(reports[model_name])):\n",
    "        report = reports[model_name][i]\n",
    "        acc = acc_results[model_name][i]\n",
    "        p = report[\"precision\"]\n",
    "        r = report[\"recall\"]\n",
    "        f1 = report[\"f1\"]\n",
    "        auc = report[\"auc\"]\n",
    "        print(f\"\\tAccuracy: {acc}\\n\\tPrecision: {p}\\n\\tRecall: {r}\\n\\tF1-score: {f1}\\n\\tAUC score: {auc}\\n\" + \"-\"*40)\n",
    "    print(f\"Mean acc: {np.mean([acc_results[model_name][i] for i in range(len(reports[model_name]))])}\")\n",
    "    print(f\"Mean F1-score: {np.mean([reports[model_name][i]['f1'] for i in range(len(reports[model_name]))])}\")\n",
    "    print(f\"Mean AUC score: {np.mean([reports[model_name][i]['auc'] for i in range(len(reports[model_name]))])}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search for SVM ...\n",
      "Grid search for LGB ...\n",
      "Grid search for RF ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more of the test scores are non-finite: [0.95715897 0.94921483 0.95302062 0.95261839 0.952116   0.9469893\n",
      " 0.94257447 0.95025508 0.9514099  0.95145041 0.94820691 0.93936029\n",
      " 0.95054392 0.95066483 0.95480324 0.9574854  0.94590746 0.95211075\n",
      " 0.95157894 0.95535059 0.95480941 0.94389545 0.9560559  0.95250748\n",
      " 0.95431616        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search for XGB ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more of the test scores are non-finite: [0.84628674 0.84398222 0.84397812 0.84232403 0.84445341 0.84240813\n",
      " 0.84730514 0.84378214 0.84345946 0.84154382 0.84730514 0.84355992\n",
      " 0.83948366 0.8411981  0.84695942 0.8432142  0.88206657 0.91808438\n",
      " 0.92595075 0.92883833 0.85663465 0.91613033 0.92430601 0.92540246\n",
      " 0.85602963 0.91171224 0.92435531 0.92510157 0.85404173 0.91121843\n",
      " 0.92583242 0.92510157 0.92053361 0.91914628 0.91992983 0.92114691\n",
      " 0.91634331 0.91807712 0.91658017 0.91789863 0.89845336 0.91565418\n",
      " 0.91845533 0.91827367 0.89862293 0.91780096 0.92093854 0.92019973\n",
      " 0.9350412  0.92286364 0.92002201 0.91616706 0.91817138 0.91508499\n",
      " 0.91290556 0.91037303 0.91414265 0.91465758 0.91329447 0.91224355\n",
      " 0.91556261 0.91580585 0.91513419 0.91321896        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: {'C': 100, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "LGB: {'max_depth': 5, 'metric': 'binary_logloss', 'num_leaves': 10, 'objective': 'binary'}\n",
      "RF: {'max_features': 'sqrt', 'min_samples_split': 6, 'n_estimators': 10, 'random_state': 16}\n",
      "XGB: {'eval_metric': 'error', 'learning_rate': 0.5, 'max_depth': 4, 'n_estimators': 10, 'objective': 'binary:logistic', 'random_state': 16, 'use_label_encoder': False}\n",
      "Model SVM, Actual: [0 1], [504 389], Predictions: [0 1], [ 79 814]\n",
      "coef_ only available for SVC with linear kernel\n",
      "Model LGB, Actual: [0 1], [504 389], Predictions: [0 1], [122 771]\n",
      "Model RF, Actual: [0 1], [504 389], Predictions: [0 1], [211 682]\n",
      "Model XGB, Actual: [0 1], [504 389], Predictions: [0 1], [202 691]\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "[('bpm', 152), ('ecg_mean', 143), ('ecg_kurtosis', 141), ('ecg_iqr', 84), ('eda_kurtosis', 38), ('eda_mean', 38), ('hf_rr', 36), ('ecg_skew', 36), ('ecg_median', 33), ('rmssd', 27), ('eda_skew', 23), ('ecg_rms', 22), ('ecg_std', 22), ('ecg_var', 22), ('eda_var', 21), ('sdnn', 20), ('eda_std', 11), ('eda_median', 9), ('lf_rr', 8), ('eda_iqr', 6), ('eda_rms', 5)]\n",
      "\n",
      "\n",
      "[('bpm', 0.22421026917660486), ('ecg_mean', 0.16303421955159378), ('ecg_iqr', 0.1474904383603721), ('ecg_kurtosis', 0.06389610648186013), ('ecg_std', 0.06258189139711087), ('hf_rr', 0.04494766912288693), ('rmssd', 0.03978084315411969), ('sdnn', 0.030879195259368365), ('ecg_median', 0.03079310955189371), ('eda_iqr', 0.027734276647320543), ('ecg_rms', 0.022197525366894635), ('ecg_var', 0.021429119402713737), ('eda_kurtosis', 0.019787306021591157), ('eda_median', 0.019052847672147176), ('eda_std', 0.018350362051287594), ('ecg_skew', 0.016165076143376665), ('lf_rr', 0.014215766038881472), ('eda_skew', 0.013200158066318588), ('eda_rms', 0.01157402233027908), ('eda_mean', 0.004762088602625438), ('eda_var', 0.0039177096007535364)]\n",
      "\n",
      "\n",
      "[('bpm', 0.15631275), ('ecg_rms', 0.15587476), ('eda_std', 0.14385328), ('ecg_var', 0.08203913), ('ecg_mean', 0.07364603), ('ecg_iqr', 0.06894928), ('ecg_kurtosis', 0.055829093), ('rmssd', 0.051091254), ('hf_rr', 0.042268064), ('eda_var', 0.03814755), ('eda_kurtosis', 0.034273714), ('lf_rr', 0.030856222), ('eda_rms', 0.028568102), ('ecg_std', 0.02752461), ('eda_skew', 0.008824135), ('ecg_median', 0.001942001), ('sdnn', 0.0), ('ecg_skew', 0.0), ('eda_iqr', 0.0), ('eda_mean', 0.0), ('eda_median', 0.0)]\n",
      "\n",
      "Model evaluation metrics for SVM:\n",
      "\tAccuracy: 0.44344904815229563\n",
      "\tPrecision: 0.43366093366093367\n",
      "\tRecall: 0.9074550128534704\n",
      "\tF1-score: 0.5868661679135495\n",
      "\tAUC score: 0.4963862365854654\n",
      "----------------------------------------\n",
      "Mean acc: 0.44344904815229563\n",
      "Mean F1-score: 0.5868661679135495\n",
      "Mean AUC score: 0.4963862365854654\n",
      "\n",
      "\n",
      "Model evaluation metrics for LGB:\n",
      "\tAccuracy: 0.45128779395296753\n",
      "\tPrecision: 0.4345006485084306\n",
      "\tRecall: 0.8611825192802056\n",
      "\tF1-score: 0.5775862068965517\n",
      "\tAUC score: 0.49805157710042025\n",
      "----------------------------------------\n",
      "Mean acc: 0.45128779395296753\n",
      "Mean F1-score: 0.5775862068965517\n",
      "Mean AUC score: 0.49805157710042025\n",
      "\n",
      "\n",
      "Model evaluation metrics for RF:\n",
      "\tAccuracy: 0.4479283314669653\n",
      "\tPrecision: 0.4237536656891496\n",
      "\tRecall: 0.7429305912596401\n",
      "\tF1-score: 0.5396825396825398\n",
      "\tAUC score: 0.4815843432488677\n",
      "----------------------------------------\n",
      "Mean acc: 0.4479283314669653\n",
      "Mean F1-score: 0.5396825396825398\n",
      "Mean AUC score: 0.4815843432488677\n",
      "\n",
      "\n",
      "Model evaluation metrics for XGB:\n",
      "\tAccuracy: 0.48936170212765956\n",
      "\tPrecision: 0.4515195369030391\n",
      "\tRecall: 0.8020565552699229\n",
      "\tF1-score: 0.5777777777777778\n",
      "\tAUC score: 0.5250362141428979\n",
      "----------------------------------------\n",
      "Mean acc: 0.48936170212765956\n",
      "Mean F1-score: 0.5777777777777778\n",
      "Mean AUC score: 0.5250362141428979\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN ON WESAD AND TEST ON APD -- ALL\n",
    "importlib.reload(train)\n",
    "importlib.reload(dr_a)\n",
    "importlib.reload(dr_w)\n",
    "importlib.reload(dt)\n",
    "\n",
    "\n",
    "random.seed(56)\n",
    "\n",
    "x_a, y_a = train.Train_WESAD.get_wesad_data(metrics, model_phases_wesad, verbose=False, label_type=wesad_label_type, threshold=threshold, normalize=True, standardize=False)\n",
    "x_b, y_b = train.Train_APD.get_apd_data_ranking(metrics, model_phases_apd, verbose=False, anxiety_label_type=anxiety_label_type, threshold=threshold, normalize=True, standardize=False)\n",
    "# drop subjects with noisy data\n",
    "x_b = x_b[~x_b[\"subject\"].isin(invalid_apd_subjects)].reset_index(drop=True)\n",
    "y_b = y_b[~y_b[\"subject\"].isin(invalid_apd_subjects)].reset_index(drop=True)\n",
    "if anxiety_label_type is not None:\n",
    "    x_b = x_b.drop([\"anxietyGroup\"], axis=1)  # drop anxietyGroup column because WESAD doesn't have this feature\n",
    "\n",
    "x_a = x_a.drop([\"phaseId\"], axis=1)\n",
    "x_b = x_b.drop([\"phaseId\"], axis=1)\n",
    "\n",
    "inds = pd.isnull(x_a).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_a = x_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_a = y_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "inds = pd.isnull(x_b).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_b = x_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_b = y_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "\n",
    "# make sure subjects from different datasets aren't labeled with the same index\n",
    "x_b[\"subject\"] = x_b[\"subject\"] + 500\n",
    "y_b[\"subject\"] = y_b[\"subject\"] + 500\n",
    "\n",
    "# augment smaller dataset by randomly duplicating a subset of subjects\n",
    "n_wesad = x_a.shape[0]\n",
    "n_apd = x_b.shape[0]\n",
    "n_samples_to_augment = int((0.7 - (n_wesad / n_apd)) * n_wesad)\n",
    "augment_indices = random.sample(range(n_wesad), n_samples_to_augment)\n",
    "\n",
    "duplicates = x_a.iloc[augment_indices, :].reset_index(drop=True)\n",
    "x_a = pd.concat([x_a, duplicates], axis=0).reset_index(drop=True)\n",
    "duplicates = y_a.iloc[augment_indices, :].reset_index(drop=True)\n",
    "y_a = pd.concat([y_a, duplicates], axis=0).reset_index(drop=True)\n",
    "\n",
    "acc_results = {\n",
    "    \"SVM\": [],\n",
    "    \"LGB\": [],\n",
    "    \"RF\": [],\n",
    "    \"XGB\": [],\n",
    "    # \"random\": []\n",
    "}\n",
    "reports = {\n",
    "    \"SVM\": [],\n",
    "    \"LGB\": [],\n",
    "    \"RF\": [],\n",
    "    \"XGB\": [],\n",
    "    # \"random\": []\n",
    "}\n",
    "best_models = {}\n",
    "\n",
    "num_iters = 1\n",
    "get_importance = True\n",
    "for _ in range(num_iters):\n",
    "    # include a subset of the target dataset in the training data\n",
    "    subjects = list(np.unique(x_b.loc[:, \"subject\"]))\n",
    "    train_subjects = random.sample(subjects, int(len(subjects) * percent_of_target_dataset))\n",
    "    x_train_addition = x_b[x_b[\"subject\"].isin(train_subjects)]\n",
    "    y_train_addition = y_b[y_b[\"subject\"].isin(train_subjects)]\n",
    "    x_train = pd.concat([x_a, x_train_addition])\n",
    "    y_train = pd.concat([y_a, y_train_addition])\n",
    "    x_test = x_b[~x_b[\"subject\"].isin(train_subjects)]\n",
    "    y_test = y_b[~y_b[\"subject\"].isin(train_subjects)]\n",
    "\n",
    "    # HYPERPARAMETER TUNING\n",
    "    model_data = train.grid_search_cv(\n",
    "        # models, parameters, x_a, y_a, by_subject=True, save_metrics=True, is_resample=True,\n",
    "        models, parameters, x_train, y_train, by_subject=True, save_metrics=True, is_resample=True,\n",
    "        get_importance=get_importance, drop_subject=True, test_size=0.0, folds=5\n",
    "    )\n",
    "\n",
    "    for model_name in models.keys():\n",
    "        best_models[model_name] = model_data[model_name][\"best_model\"]\n",
    "        print(f\"{model_name}: {model_data[model_name]['best_params']}\")\n",
    "\n",
    "    # FEATURE SELECTION\n",
    "    features = {name: metrics for name in models.keys()}\n",
    "    # features = train.feature_selection(best_models, model_data[\"cv\"], x_a, y_a, n_features=5)\n",
    "    # features = train.feature_selection(best_models, model_data[\"cv\"], x_train, y_train, n_features=5)\n",
    "\n",
    "    # out = train.Train_Multi_Dataset.train_across_datasets(best_models, features, x_a, y_a, x_b, y_b, by_subject=True, save_metrics=True, test_size=test_size, is_resample=False, get_importance=get_importance, drop_subject=True)\n",
    "    out = train.Train_Multi_Dataset.train_across_datasets(best_models, features, x_train, y_train, x_test, y_test, by_subject=True, save_metrics=True, test_size=test_size, is_resample=False, get_importance=get_importance, drop_subject=True)\n",
    "    \n",
    "    for model_name in acc_results:\n",
    "        acc_results[model_name].append(out[model_name][\"performance\"][0])\n",
    "        reports[model_name].append(out[model_name][\"performance\"][1])\n",
    "        if get_importance:\n",
    "            try:\n",
    "                print(\"\")\n",
    "                feature_imp = list(zip(metrics + [\"lf_hf_ratio\"], out[model_name][\"performance\"][2]))\n",
    "                feature_imp = sorted(feature_imp, key=lambda x: x[1], reverse=True)\n",
    "                print(feature_imp)\n",
    "            except Exception as e:\n",
    "                print(out[model_name][\"performance\"][2])\n",
    "            print(\"\")\n",
    "\n",
    "for model_name in acc_results.keys():\n",
    "    print(f\"Model evaluation metrics for {model_name}:\")\n",
    "    for i in range(len(reports[model_name])):\n",
    "        report = reports[model_name][i]\n",
    "        acc = acc_results[model_name][i]\n",
    "        p = report[\"precision\"]\n",
    "        r = report[\"recall\"]\n",
    "        f1 = report[\"f1\"]\n",
    "        auc = report[\"auc\"]\n",
    "        print(f\"\\tAccuracy: {acc}\\n\\tPrecision: {p}\\n\\tRecall: {r}\\n\\tF1-score: {f1}\\n\\tAUC score: {auc}\\n\" + \"-\"*40)\n",
    "    print(f\"Mean acc: {np.mean([acc_results[model_name][i] for i in range(len(reports[model_name]))])}\")\n",
    "    print(f\"Mean F1-score: {np.mean([reports[model_name][i]['f1'] for i in range(len(reports[model_name]))])}\")\n",
    "    print(f\"Mean AUC score: {np.mean([reports[model_name][i]['auc'] for i in range(len(reports[model_name]))])}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train APD, test WESAD\n",
      "\tAccuracy: 0.18164435946462715\n",
      "\tPrecision: 0.18164435946462715\n",
      "\tRecall: 1.0\n",
      "\tF1-score: 0.3074433656957929\n",
      "\tAUC score: 0.5\n",
      "----------------------------------------\n",
      "Train WESAD, test APD\n",
      "\tAccuracy: 0.48065764023210833\n",
      "\tPrecision: 0.44798890429958393\n",
      "\tRecall: 0.6991341991341992\n",
      "\tF1-score: 0.5460693153000845\n",
      "\tAUC score: 0.5016650016650017\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ENSEMBLE\n",
    "importlib.reload(train)\n",
    "importlib.reload(dr_a)\n",
    "importlib.reload(dr_w)\n",
    "importlib.reload(dt)\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "test_size = 0.15\n",
    "folds = 5\n",
    "\n",
    "x_a, y_a = train.Train_APD.get_apd_data_ranking(metrics, model_phases_apd, verbose=False, anxiety_label_type=anxiety_label_type, threshold=threshold, normalize=True, standardize=False)\n",
    "x_b, y_b = train.Train_WESAD.get_wesad_data(metrics, model_phases_wesad, verbose=False, label_type=wesad_label_type, threshold=threshold, normalize=True, standardize=False)\n",
    "# drop subjects with noisy data\n",
    "# x_a = x_a[~x_a[\"subject\"].isin(invalid_apd_subjects)].reset_index(drop=True)\n",
    "# y_a = y_a[~y_a[\"subject\"].isin(invalid_apd_subjects)].reset_index(drop=True)\n",
    "\n",
    "x_a = x_a.drop([\"phaseId\"], axis=1)\n",
    "x_b = x_b.drop([\"phaseId\"], axis=1)\n",
    "\n",
    "inds = pd.isnull(x_a).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_a = x_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_a = y_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "inds = pd.isnull(x_b).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_b = x_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_b = y_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "\n",
    "# make sure subjects from different datasets aren't labeled with the same index\n",
    "x_b[\"subject\"] = x_b[\"subject\"] + 500\n",
    "y_b[\"subject\"] = y_b[\"subject\"] + 500\n",
    "\n",
    "subjects = list(np.unique(x_b.loc[:, \"subject\"]))\n",
    "train_subjects = random.sample(subjects, int(len(subjects) * percent_of_target_dataset))\n",
    "x_train_addition = x_b[x_b[\"subject\"].isin(train_subjects)]\n",
    "y_train_addition = y_b[y_b[\"subject\"].isin(train_subjects)]\n",
    "x_train = pd.concat([x_a, x_train_addition])\n",
    "y_train = pd.concat([y_a, y_train_addition])\n",
    "x_test = x_b[~x_b[\"subject\"].isin(train_subjects)]\n",
    "y_test = y_b[~y_b[\"subject\"].isin(train_subjects)]\n",
    "\n",
    "y_train = y_train.loc[:, \"label\"]\n",
    "y_test = y_test.loc[:, \"label\"]\n",
    "\n",
    "estimators = [(name, best_models[name]) for name in best_models.keys()]\n",
    "ensemble = VotingClassifier(estimators, voting=\"hard\")\n",
    "ensemble.fit(x_train, y_train)\n",
    "y_pred = ensemble.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "p = precision_score(y_test, y_pred, zero_division=0)\n",
    "r = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"Train APD, test WESAD\")\n",
    "print(f\"\\tAccuracy: {acc}\\n\\tPrecision: {p}\\n\\tRecall: {r}\\n\\tF1-score: {f1}\\n\\tAUC score: {auc}\\n\" + \"-\"*40)\n",
    "\n",
    "\n",
    "x_a, y_a = train.Train_WESAD.get_wesad_data(metrics, model_phases_wesad, verbose=False, label_type=wesad_label_type, threshold=threshold, normalize=True, standardize=False)\n",
    "x_b, y_b = train.Train_APD.get_apd_data_ranking(metrics, model_phases_apd, verbose=False, anxiety_label_type=anxiety_label_type, threshold=threshold, normalize=True, standardize=False)\n",
    "# drop subjects with noisy data\n",
    "# x_a = x_a[~x_a[\"subject\"].isin(invalid_apd_subjects)].reset_index(drop=True)\n",
    "# y_a = y_a[~y_a[\"subject\"].isin(invalid_apd_subjects)].reset_index(drop=True)\n",
    "\n",
    "x_a = x_a.drop([\"phaseId\"], axis=1)\n",
    "x_b = x_b.drop([\"phaseId\"], axis=1)\n",
    "\n",
    "inds = pd.isnull(x_a).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_a = x_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_a = y_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "inds = pd.isnull(x_b).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_b = x_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_b = y_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "\n",
    "# make sure subjects from different datasets aren't labeled with the same index\n",
    "x_b[\"subject\"] = x_b[\"subject\"] + 500\n",
    "y_b[\"subject\"] = y_b[\"subject\"] + 500\n",
    "\n",
    "subjects = list(np.unique(x_b.loc[:, \"subject\"]))\n",
    "train_subjects = random.sample(subjects, int(len(subjects) * percent_of_target_dataset))\n",
    "x_train_addition = x_b[x_b[\"subject\"].isin(train_subjects)]\n",
    "y_train_addition = y_b[y_b[\"subject\"].isin(train_subjects)]\n",
    "x_train = pd.concat([x_a, x_train_addition])\n",
    "y_train = pd.concat([y_a, y_train_addition])\n",
    "x_test = x_b[~x_b[\"subject\"].isin(train_subjects)]\n",
    "y_test = y_b[~y_b[\"subject\"].isin(train_subjects)]\n",
    "\n",
    "y_train = y_train.loc[:, \"label\"]\n",
    "y_test = y_test.loc[:, \"label\"]\n",
    "\n",
    "estimators = [(name, best_models[name]) for name in best_models.keys()]\n",
    "ensemble = VotingClassifier(estimators, voting=\"hard\")\n",
    "ensemble.fit(x_train, y_train)\n",
    "y_pred = ensemble.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "p = precision_score(y_test, y_pred, zero_division=0)\n",
    "r = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"Train WESAD, test APD\")\n",
    "print(f\"\\tAccuracy: {acc}\\n\\tPrecision: {p}\\n\\tRecall: {r}\\n\\tF1-score: {f1}\\n\\tAUC score: {auc}\\n\" + \"-\"*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aecf4e853c2a06e9a3d98203b0bfcb89edde136ff484aed399b3da44301ece48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
