{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we classify each phase as relatively low or high anxiety for each subject? ###\n",
    "#### APD, WESAD ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING MODULES\n",
    "import glob\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "cvx_path = os.path.abspath(os.path.join('..', '..', 'cvxEDA', 'src'))\n",
    "module_path = os.path.abspath(os.path.join('..', '..', 'src'))\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy.signal as ss\n",
    "import shap\n",
    "import sys\n",
    "sys.path.append(module_path)\n",
    "\n",
    "import tools.data_reader_apd as dr_a\n",
    "import tools.data_reader_wesad as dr_w\n",
    "import tools.display_tools as dt\n",
    "import tools.preprocessing as preprocessing\n",
    "import train\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from scipy.fft import fft, fftfreq, fftshift\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold\n",
    "from sklearn.preprocessing import normalize\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import cvxopt.solvers\n",
    "cvxopt.solvers.options['show_progress'] = False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \n",
    "    category=RuntimeWarning\n",
    ")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    train.Metrics.BPM, \n",
    "    train.Metrics.RMSSD, \n",
    "    train.Metrics.HF_RR, \n",
    "    train.Metrics.LF_RR, \n",
    "    train.Metrics.SDNN, \n",
    "    train.Metrics.MEAN_SCL, \n",
    "    train.Metrics.SCR_RATE,\n",
    "# ]\n",
    "] + train.Metrics.STATISTICAL\n",
    "\n",
    "model_phases_apd = [\n",
    "    \"Baseline_Rest\", \n",
    "    \"BugBox_Relax\", \"BugBox_Anticipate\", \"BugBox_Exposure\", \"BugBox_Break\",\n",
    "    \"Speech_Relax\", \"Speech_Anticipate\", \"Speech_Exposure\", \"Speech_Break\"\n",
    "]\n",
    "\n",
    "model_phases_wesad = dr_w.Phases.PHASE_ORDER\n",
    "\n",
    "anxiety_label_type = None\n",
    "wesad_label_type = \"stai\"\n",
    "\n",
    "models = {\n",
    "    \"SVM\": SVC(),\n",
    "    \"LGB\": LGBMClassifier(),\n",
    "    \"RF\": RandomForestClassifier(random_state=16),\n",
    "    \"XGB\": XGBClassifier(random_state=16),\n",
    "    # \"random\": None\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    \"SVM\": [{\n",
    "        \"kernel\": [\"rbf\", \"poly\", \"sigmoid\"],\n",
    "        \"C\": [0.1, 1, 10, 100],\n",
    "        \"gamma\": [1, 0.1, 0.01, 0.001, \"scale\", \"auto\"],\n",
    "        \"probability\": [True]\n",
    "    }],\n",
    "    \"LGB\": [{\n",
    "        \"objective\": [\"binary\"],\n",
    "        \"num_leaves\": [10, 20, 30, 40, 50],\n",
    "        \"max_depth\": [3, 4, 5, 6, 7],\n",
    "        \"metric\": [\"binary_logloss\"]\n",
    "    }],\n",
    "    \"RF\": [{\n",
    "        \"n_estimators\": [10, 20, 30, 40, 50],\n",
    "        \"max_features\": [\"sqrt\", \"0.4\"],\n",
    "        \"min_samples_split\": [3, 4, 5, 6, 7],\n",
    "        \"random_state\": [16]\n",
    "    }],\n",
    "    \"XGB\": [{\n",
    "        \"objective\": [\"binary:logistic\"],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.3, 0.5],\n",
    "        \"max_depth\": [4, 5, 6, 7],\n",
    "        \"n_estimators\": [10, 20, 30, 40],\n",
    "        \"eval_metric\": [\"error\", \"log_loss\"],\n",
    "        \"use_label_encoder\": [False],\n",
    "        \"random_state\": [16]\n",
    "    }],\n",
    "    \"random\": None\n",
    "}\n",
    "\n",
    "threshold = \"fixed\"\n",
    "test_size = 1.0\n",
    "\n",
    "percent_of_target_dataset = 0.0\n",
    "\n",
    "temp_a, _ = train.Train_APD.get_apd_data_ranking([train.Metrics.BPM], phases=dr_a.Phases.PHASES_LIST, normalize=False)\n",
    "idx = temp_a[temp_a[\"bpm\"] > 200].index \n",
    "invalid_apd_subjects = set(temp_a[\"subject\"].iloc[idx].tolist())\n",
    "idx = temp_a[temp_a[\"bpm\"] < 35].index \n",
    "invalid_apd_subjects.update(set(temp_a[\"subject\"].iloc[idx].tolist()))\n",
    "\n",
    "temp_a, _ = train.Train_WESAD.get_wesad_data([train.Metrics.BPM], phases=dr_w.Phases.PHASE_ORDER, normalize=False)\n",
    "idx = temp_a[temp_a[\"bpm\"] > 200].index \n",
    "invalid_wesad_subjects = set(temp_a[\"subject\"].iloc[idx].tolist())\n",
    "idx = temp_a[temp_a[\"bpm\"] < 35].index \n",
    "invalid_wesad_subjects.update(set(temp_a[\"subject\"].iloc[idx].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search for SVM ...\n",
      "Grid search for LGB ...\n",
      "Grid search for RF ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more of the test scores are non-finite: [0.5870546  0.60185599 0.60632706 0.61425918 0.61005475 0.59035372\n",
      " 0.62558095 0.62087761 0.61885531 0.61547916 0.56533026 0.57099617\n",
      " 0.57706455 0.58687198 0.59793004 0.58734478 0.5915029  0.59534212\n",
      " 0.60018929 0.60989129 0.60578101 0.61180535 0.61114404 0.61463969\n",
      " 0.61307951        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search for XGB ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more of the test scores are non-finite: [0.52898514 0.53776002 0.55180529 0.56219925 0.51793317 0.54329759\n",
      " 0.54849579 0.54687018 0.5214941  0.53979535 0.54461715 0.54512948\n",
      " 0.5351084  0.55408634 0.55593588 0.56164594 0.56932724 0.58062873\n",
      " 0.58180389 0.58851694 0.56701235 0.5857344  0.58610764 0.58918384\n",
      " 0.57638655 0.58233763 0.58710752 0.58791005 0.57030759 0.59098998\n",
      " 0.59523831 0.59726785 0.58008722 0.58380717 0.58221183 0.57803844\n",
      " 0.56054517 0.5768482  0.57465849 0.57636271 0.56671504 0.57160481\n",
      " 0.56848067 0.57434935 0.56230445 0.57447331 0.5870704  0.58957508\n",
      " 0.55774447 0.56266479 0.54851077 0.55887734 0.57534547 0.59155432\n",
      " 0.58155632 0.57717691 0.57347105 0.57362336 0.5646759  0.56651192\n",
      " 0.58806526 0.58924416 0.59561489 0.59104572        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: {'C': 1, 'gamma': 1, 'kernel': 'poly', 'probability': True}\n",
      "LGB: {'max_depth': 6, 'metric': 'binary_logloss', 'num_leaves': 30, 'objective': 'binary'}\n",
      "RF: {'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 20, 'random_state': 16}\n",
      "XGB: {'eval_metric': 'error', 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 40, 'objective': 'binary:logistic', 'random_state': 16, 'use_label_encoder': False}\n",
      "Model SVM, Actual: [0 1], [428  95], Predictions: [0 1], [151 372]\n",
      "coef_ only available for SVC with linear kernel\n",
      "SVM\n",
      "Model LGB, Actual: [0 1], [428  95], Predictions: [0 1], [137 386]\n",
      "LGB\n",
      "['bpm', 'rmssd', 'hf_rr', 'lf_rr', 'sdnn', 'mean_SCL', 'SCR_rate', 'ecg_mean', 'ecg_median', 'ecg_std', 'ecg_var', 'eda_mean', 'eda_median', 'eda_std', 'eda_var', 'lf_hf_ratio']\n",
      "Model RF, Actual: [0 1], [428  95], Predictions: [0 1], [117 406]\n",
      "RF\n",
      "['bpm' 'rmssd' 'hf_rr' 'lf_rr' 'sdnn' 'mean_SCL' 'SCR_rate' 'ecg_mean'\n",
      " 'ecg_median' 'ecg_std' 'ecg_var' 'eda_mean' 'eda_median' 'eda_std'\n",
      " 'eda_var' 'lf_hf_ratio']\n",
      "Model XGB, Actual: [0 1], [428  95], Predictions: [0 1], [140 383]\n",
      "XGB\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "[('bpm', 170), ('lf_hf_ratio', 137), ('eda_std', 126), ('sdnn', 117), ('rmssd', 116), ('SCR_rate', 113), ('ecg_var', 103), ('ecg_std', 101), ('ecg_mean', 95), ('ecg_median', 84), ('mean_SCL', 75), ('hf_rr', 74), ('eda_median', 74), ('eda_var', 64), ('lf_rr', 39), ('eda_mean', 25)]\n",
      "\n",
      "\n",
      "[('bpm', 0.0854889212110334), ('ecg_var', 0.07922591091187305), ('eda_std', 0.0777533171572364), ('rmssd', 0.06621187478013446), ('lf_hf_ratio', 0.06617867780191099), ('sdnn', 0.06271485934040613), ('hf_rr', 0.06228002569390535), ('ecg_std', 0.062263767264886506), ('lf_rr', 0.05999705842219788), ('ecg_median', 0.055647468855944945), ('eda_var', 0.05559749441964621), ('SCR_rate', 0.055179233496664204), ('mean_SCL', 0.05509845904171495), ('eda_median', 0.053769742406680546), ('ecg_mean', 0.051395377339763404), ('eda_mean', 0.0511978118560016)]\n",
      "\n",
      "\n",
      "[('ecg_var', 0.09859758), ('eda_mean', 0.07027648), ('bpm', 0.06833921), ('rmssd', 0.06487445), ('eda_median', 0.06324239), ('lf_hf_ratio', 0.0614298), ('lf_rr', 0.060478922), ('eda_std', 0.060227275), ('SCR_rate', 0.059424516), ('eda_var', 0.058747195), ('mean_SCL', 0.05855664), ('sdnn', 0.058091182), ('ecg_std', 0.05597452), ('hf_rr', 0.05591288), ('ecg_mean', 0.05406045), ('ecg_median', 0.051766474)]\n",
      "\n",
      "Model evaluation metrics for SVM:\n",
      "\tAccuracy: 0.42447418738049714\n",
      "\tPrecision: 0.22311827956989247\n",
      "\tRecall: 0.8736842105263158\n",
      "\tF1-score: 0.3554603854389722\n",
      "\tAUC score: 0.5992252828332514\n",
      "----------------------------------------\n",
      "Mean acc: 0.42447418738049714\n",
      "Mean F1-score: 0.3554603854389722\n",
      "Mean AUC score: 0.5992252828332514\n",
      "\n",
      "\n",
      "Model evaluation metrics for LGB:\n",
      "\tAccuracy: 0.3326959847036329\n",
      "\tPrecision: 0.17098445595854922\n",
      "\tRecall: 0.6947368421052632\n",
      "\tF1-score: 0.27442827442827444\n",
      "\tAUC score: 0.47353664535169704\n",
      "----------------------------------------\n",
      "Mean acc: 0.3326959847036329\n",
      "Mean F1-score: 0.27442827442827444\n",
      "Mean AUC score: 0.47353664535169704\n",
      "\n",
      "\n",
      "Model evaluation metrics for RF:\n",
      "\tAccuracy: 0.30210325047801145\n",
      "\tPrecision: 0.16748768472906403\n",
      "\tRecall: 0.7157894736842105\n",
      "\tF1-score: 0.27145708582834327\n",
      "\tAUC score: 0.4630349237579931\n",
      "----------------------------------------\n",
      "Mean acc: 0.30210325047801145\n",
      "Mean F1-score: 0.27145708582834327\n",
      "Mean AUC score: 0.4630349237579931\n",
      "\n",
      "\n",
      "Model evaluation metrics for XGB:\n",
      "\tAccuracy: 0.32695984703632885\n",
      "\tPrecision: 0.16449086161879894\n",
      "\tRecall: 0.6631578947368421\n",
      "\tF1-score: 0.26359832635983266\n",
      "\tAUC score: 0.45774717166748646\n",
      "----------------------------------------\n",
      "Mean acc: 0.32695984703632885\n",
      "Mean F1-score: 0.26359832635983266\n",
      "Mean AUC score: 0.45774717166748646\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN ON APD AND TEST ON WESAD -- ALL\n",
    "importlib.reload(train)\n",
    "importlib.reload(dr_a)\n",
    "importlib.reload(dr_w)\n",
    "importlib.reload(dt)\n",
    "\n",
    "\n",
    "random.seed(38)\n",
    "\n",
    "x_a, y_a = train.Train_APD.get_apd_data_ranking(metrics, model_phases_apd, verbose=False, anxiety_label_type=anxiety_label_type, threshold=threshold, normalize=True, standardize=False)\n",
    "x_b, y_b = train.Train_WESAD.get_wesad_data(metrics, model_phases_wesad, verbose=False, label_type=wesad_label_type, threshold=threshold, normalize=True, standardize=False)\n",
    "# drop subjects with noisy data\n",
    "# x_a = x_a[~x_a[\"subject\"].isin(invalid_apd_subjects)].reset_index(drop=True)\n",
    "# y_a = y_a[~y_a[\"subject\"].isin(invalid_apd_subjects)].reset_index(drop=True)\n",
    "\n",
    "if anxiety_label_type is not None:\n",
    "    x_a = x_a.drop([\"anxietyGroup\"], axis=1)  # drop anxietyGroup column because WESAD doesn't have this feature\n",
    "\n",
    "x_a = x_a.drop([\"phaseId\"], axis=1)\n",
    "x_b = x_b.drop([\"phaseId\"], axis=1)\n",
    "\n",
    "inds = pd.isnull(x_a).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_a = x_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_a = y_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "inds = pd.isnull(x_b).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_b = x_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_b = y_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "\n",
    "# make sure subjects from different datasets aren't labeled with the same index\n",
    "x_b[\"subject\"] = x_b[\"subject\"] + 500\n",
    "y_b[\"subject\"] = y_b[\"subject\"] + 500\n",
    "\n",
    "acc_results = {\n",
    "    \"SVM\": [],\n",
    "    \"LGB\": [],\n",
    "    \"RF\": [],\n",
    "    \"XGB\": [],\n",
    "    # \"random\": []\n",
    "}\n",
    "reports = {\n",
    "    \"SVM\": [],\n",
    "    \"LGB\": [],\n",
    "    \"RF\": [],\n",
    "    \"XGB\": [],\n",
    "    # \"random\": []\n",
    "}\n",
    "best_models_apd_wesad = {}\n",
    "ensemble_weights_apd = {}\n",
    "\n",
    "num_iters = 1\n",
    "get_importance = True\n",
    "for _ in range(num_iters):\n",
    "    # include a subset of the target dataset in the training data\n",
    "    subjects = list(np.unique(x_b.loc[:, \"subject\"]))\n",
    "    train_subjects = random.sample(subjects, int(len(subjects) * percent_of_target_dataset))\n",
    "    x_train_addition = x_b[x_b[\"subject\"].isin(train_subjects)]\n",
    "    y_train_addition = y_b[y_b[\"subject\"].isin(train_subjects)]\n",
    "    x_train = pd.concat([x_a, x_train_addition])\n",
    "    y_train = pd.concat([y_a, y_train_addition])\n",
    "    x_test = x_b[~x_b[\"subject\"].isin(train_subjects)]\n",
    "    y_test = y_b[~y_b[\"subject\"].isin(train_subjects)]\n",
    "\n",
    "    # HYPERPARAMETER TUNING\n",
    "    model_data = train.grid_search_cv(\n",
    "        # models, parameters, x_a, y_a, by_subject=True, save_metrics=True, is_resample=True,\n",
    "        models, parameters, x_train, y_train, by_subject=True, save_metrics=True, is_resample=True,\n",
    "        get_importance=get_importance, drop_subject=True, test_size=0.0, folds=5\n",
    "    )\n",
    "\n",
    "    for model_name in models.keys():\n",
    "        best_models_apd_wesad[model_name] = model_data[model_name][\"best_model\"]\n",
    "        print(f\"{model_name}: {model_data[model_name]['best_params']}\")\n",
    "\n",
    "    # # FEATURE SELECTION\n",
    "    features = {name: metrics+[\"lf_hf_ratio\"] for name in models.keys()}\n",
    "    # features = {name: metrics for name in models.keys()}\n",
    "    # features = train.feature_selection(best_models_apd_wesad, model_data[\"cv\"], x_train, y_train, n_features=5)\n",
    "\n",
    "    # out = train.Train_Multi_Dataset.train_across_datasets(best_models_apd_wesad, features, x_a, y_a, x_b, y_b, by_subject=True, save_metrics=True, test_size=test_size, is_resample=False, get_importance=get_importance, drop_subject=True)\n",
    "    out = train.Train_Multi_Dataset.train_across_datasets(best_models_apd_wesad, features, x_train, y_train, x_test, y_test, by_subject=True, save_metrics=True, test_size=test_size, is_resample=False, get_importance=get_importance, drop_subject=True)\n",
    "    \n",
    "    for model_name in acc_results:\n",
    "        acc_results[model_name].append(out[model_name][\"performance\"][0])\n",
    "        reports[model_name].append(out[model_name][\"performance\"][1])\n",
    "        if get_importance:\n",
    "            try:\n",
    "                print(\"\")\n",
    "                feature_imp = list(zip(metrics + [\"lf_hf_ratio\"], out[model_name][\"performance\"][2]))\n",
    "                feature_imp = sorted(feature_imp, key=lambda x: x[1], reverse=True)\n",
    "                print(feature_imp)\n",
    "            except Exception as e:\n",
    "                print(out[model_name][\"performance\"][2])\n",
    "            print(\"\")\n",
    "\n",
    "for model_name in acc_results.keys():\n",
    "    print(f\"Model evaluation metrics for {model_name}:\")\n",
    "    for i in range(len(reports[model_name])):\n",
    "        report = reports[model_name][i]\n",
    "        acc = acc_results[model_name][i]\n",
    "        p = report[\"precision\"]\n",
    "        r = report[\"recall\"]\n",
    "        f1 = report[\"f1\"]\n",
    "        auc = report[\"auc\"]\n",
    "        ensemble_weights_apd[model_name] = acc\n",
    "        print(f\"\\tAccuracy: {acc}\\n\\tPrecision: {p}\\n\\tRecall: {r}\\n\\tF1-score: {f1}\\n\\tAUC score: {auc}\\n\" + \"-\"*40)\n",
    "    print(f\"Mean acc: {np.mean([acc_results[model_name][i] for i in range(len(reports[model_name]))])}\")\n",
    "    print(f\"Mean F1-score: {np.mean([reports[model_name][i]['f1'] for i in range(len(reports[model_name]))])}\")\n",
    "    print(f\"Mean AUC score: {np.mean([reports[model_name][i]['auc'] for i in range(len(reports[model_name]))])}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search for SVM ...\n",
      "Grid search for LGB ...\n",
      "Grid search for RF ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more of the test scores are non-finite: [0.9244296  0.93232432 0.93199461 0.9448352  0.94249978 0.93858043\n",
      " 0.93782946 0.94097083 0.95267656 0.9468334  0.9315668  0.93300963\n",
      " 0.93647834 0.94400457 0.9400404  0.92119087 0.92726491 0.93427318\n",
      " 0.94124931 0.93846467 0.92836783 0.93214659 0.93808791 0.9451647\n",
      " 0.94229006        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search for XGB ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more of the test scores are non-finite: [0.87406277 0.88661329 0.8843822  0.88586428 0.86623552 0.87898157\n",
      " 0.89071855 0.90043678 0.88311593 0.88798915 0.88753416 0.901817\n",
      " 0.88496778 0.89493359 0.89238342 0.90318219 0.91218107 0.92278483\n",
      " 0.92700481 0.92135143 0.91857911 0.92165681 0.91910485 0.92374773\n",
      " 0.90812212 0.90598341 0.9029324  0.91393994 0.91017973 0.91985124\n",
      " 0.92161751 0.92111887 0.91211347 0.91511044 0.92118663 0.92592766\n",
      " 0.90261003 0.91762107 0.92936192 0.9299833  0.89989562 0.91674721\n",
      " 0.92453376 0.92745748 0.90098739 0.91948668 0.9299675  0.93204172\n",
      " 0.93547787 0.93611628 0.93410024 0.92988114 0.9089701  0.91994953\n",
      " 0.92067476 0.92318777 0.9105591  0.91948737 0.91686637 0.91778492\n",
      " 0.90927361 0.92545169 0.92111027 0.92353573        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: {'C': 100, 'gamma': 0.01, 'kernel': 'rbf', 'probability': True}\n",
      "LGB: {'max_depth': 7, 'metric': 'binary_logloss', 'num_leaves': 10, 'objective': 'binary'}\n",
      "RF: {'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 40, 'random_state': 16}\n",
      "XGB: {'eval_metric': 'error', 'learning_rate': 0.5, 'max_depth': 4, 'n_estimators': 20, 'objective': 'binary:logistic', 'random_state': 16, 'use_label_encoder': False}\n",
      "Model SVM, Actual: [0 1], [570 462], Predictions: [0 1], [430 602]\n",
      "coef_ only available for SVC with linear kernel\n",
      "SVM\n",
      "Model LGB, Actual: [0 1], [570 462], Predictions: [0 1], [695 337]\n",
      "LGB\n",
      "['bpm', 'rmssd', 'hf_rr', 'lf_rr', 'sdnn', 'mean_SCL', 'SCR_rate', 'ecg_mean', 'ecg_median', 'ecg_std', 'ecg_var', 'eda_mean', 'eda_median', 'eda_std', 'eda_var', 'lf_hf_ratio']\n",
      "Model RF, Actual: [0 1], [570 462], Predictions: [0 1], [633 399]\n",
      "RF\n",
      "['bpm' 'rmssd' 'hf_rr' 'lf_rr' 'sdnn' 'mean_SCL' 'SCR_rate' 'ecg_mean'\n",
      " 'ecg_median' 'ecg_std' 'ecg_var' 'eda_mean' 'eda_median' 'eda_std'\n",
      " 'eda_var' 'lf_hf_ratio']\n",
      "Model XGB, Actual: [0 1], [570 462], Predictions: [0 1], [659 373]\n",
      "XGB\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "[('bpm', 227), ('ecg_mean', 132), ('ecg_std', 99), ('hf_rr', 48), ('rmssd', 43), ('ecg_var', 39), ('eda_mean', 37), ('eda_std', 37), ('mean_SCL', 36), ('SCR_rate', 35), ('lf_hf_ratio', 35), ('sdnn', 33), ('eda_median', 28), ('lf_rr', 26), ('ecg_median', 25), ('eda_var', 20)]\n",
      "\n",
      "\n",
      "[('bpm', 0.2598442602028346), ('ecg_mean', 0.10392713372003363), ('ecg_var', 0.0879620063691473), ('ecg_median', 0.08771105410361531), ('rmssd', 0.0770172455473092), ('sdnn', 0.07363900429939294), ('ecg_std', 0.06386461737280082), ('eda_var', 0.040805060716302424), ('SCR_rate', 0.040726306651983304), ('eda_std', 0.03991500390464338), ('eda_median', 0.029166418236464004), ('eda_mean', 0.02586509937982532), ('lf_hf_ratio', 0.020689515784500616), ('hf_rr', 0.016564969840268444), ('lf_rr', 0.016231768164575895), ('mean_SCL', 0.01607053570630288)]\n",
      "\n",
      "\n",
      "[('eda_std', 0.24991691), ('ecg_median', 0.19475691), ('bpm', 0.14886753), ('eda_mean', 0.064973086), ('ecg_mean', 0.060569864), ('rmssd', 0.042781956), ('mean_SCL', 0.034505147), ('ecg_var', 0.033051055), ('lf_hf_ratio', 0.02966489), ('hf_rr', 0.02717592), ('SCR_rate', 0.024652833), ('ecg_std', 0.024375325), ('eda_var', 0.024336804), ('sdnn', 0.021779703), ('eda_median', 0.013087842), ('lf_rr', 0.0055041723)]\n",
      "\n",
      "Model evaluation metrics for SVM:\n",
      "\tAccuracy: 0.5116279069767442\n",
      "\tPrecision: 0.46511627906976744\n",
      "\tRecall: 0.6060606060606061\n",
      "\tF1-score: 0.5263157894736842\n",
      "\tAUC score: 0.5205741626794258\n",
      "----------------------------------------\n",
      "Mean acc: 0.5116279069767442\n",
      "Mean F1-score: 0.5263157894736842\n",
      "Mean AUC score: 0.5205741626794258\n",
      "\n",
      "\n",
      "Model evaluation metrics for LGB:\n",
      "\tAccuracy: 0.5668604651162791\n",
      "\tPrecision: 0.5222551928783383\n",
      "\tRecall: 0.38095238095238093\n",
      "\tF1-score: 0.4405506883604506\n",
      "\tAUC score: 0.5492481203007519\n",
      "----------------------------------------\n",
      "Mean acc: 0.5668604651162791\n",
      "Mean F1-score: 0.4405506883604506\n",
      "Mean AUC score: 0.5492481203007519\n",
      "\n",
      "\n",
      "Model evaluation metrics for RF:\n",
      "\tAccuracy: 0.5513565891472868\n",
      "\tPrecision: 0.49874686716791977\n",
      "\tRecall: 0.43073593073593075\n",
      "\tF1-score: 0.46225319396051107\n",
      "\tAUC score: 0.5399293688767374\n",
      "----------------------------------------\n",
      "Mean acc: 0.5513565891472868\n",
      "Mean F1-score: 0.46225319396051107\n",
      "Mean AUC score: 0.5399293688767374\n",
      "\n",
      "\n",
      "Model evaluation metrics for XGB:\n",
      "\tAccuracy: 0.5474806201550387\n",
      "\tPrecision: 0.4932975871313673\n",
      "\tRecall: 0.39826839826839827\n",
      "\tF1-score: 0.44071856287425154\n",
      "\tAUC score: 0.5333447254499887\n",
      "----------------------------------------\n",
      "Mean acc: 0.5474806201550387\n",
      "Mean F1-score: 0.44071856287425154\n",
      "Mean AUC score: 0.5333447254499887\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN ON WESAD AND TEST ON APD -- ALL\n",
    "importlib.reload(train)\n",
    "importlib.reload(dr_a)\n",
    "importlib.reload(dr_w)\n",
    "importlib.reload(dt)\n",
    "\n",
    "\n",
    "random.seed(38)\n",
    "\n",
    "x_a, y_a = train.Train_WESAD.get_wesad_data(metrics, model_phases_wesad, verbose=False, label_type=wesad_label_type, threshold=threshold, normalize=True, standardize=False)\n",
    "x_b, y_b = train.Train_APD.get_apd_data_ranking(metrics, model_phases_apd, verbose=False, anxiety_label_type=anxiety_label_type, threshold=threshold, normalize=True, standardize=False)\n",
    "# drop subjects with noisy data\n",
    "x_b = x_b[~x_b[\"subject\"].isin(invalid_apd_subjects)].reset_index(drop=True)\n",
    "y_b = y_b[~y_b[\"subject\"].isin(invalid_apd_subjects)].reset_index(drop=True)\n",
    "if anxiety_label_type is not None:\n",
    "    x_b = x_b.drop([\"anxietyGroup\"], axis=1)  # drop anxietyGroup column because WESAD doesn't have this feature\n",
    "\n",
    "x_a = x_a.drop([\"phaseId\"], axis=1)\n",
    "x_b = x_b.drop([\"phaseId\"], axis=1)\n",
    "\n",
    "inds = pd.isnull(x_a).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_a = x_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_a = y_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "inds = pd.isnull(x_b).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_b = x_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_b = y_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "\n",
    "# make sure subjects from different datasets aren't labeled with the same index\n",
    "x_b[\"subject\"] = x_b[\"subject\"] + 500\n",
    "y_b[\"subject\"] = y_b[\"subject\"] + 500\n",
    "\n",
    "# augment smaller dataset by randomly duplicating a subset of subjects\n",
    "# n_wesad = x_a.shape[0]\n",
    "# n_apd = x_b.shape[0]\n",
    "# n_samples_to_augment = int((0.7 - (n_wesad / n_apd)) * n_wesad)\n",
    "# print(n_wesad / n_apd)\n",
    "# augment_indices = random.sample(range(n_wesad), n_samples_to_augment)\n",
    "\n",
    "# duplicates = x_a.iloc[augment_indices, :].reset_index(drop=True)\n",
    "# x_a = pd.concat([x_a, duplicates], axis=0).reset_index(drop=True)\n",
    "# duplicates = y_a.iloc[augment_indices, :].reset_index(drop=True)\n",
    "# y_a = pd.concat([y_a, duplicates], axis=0).reset_index(drop=True)\n",
    "\n",
    "acc_results = {\n",
    "    \"SVM\": [],\n",
    "    \"LGB\": [],\n",
    "    \"RF\": [],\n",
    "    \"XGB\": [],\n",
    "    # \"random\": []\n",
    "}\n",
    "reports = {\n",
    "    \"SVM\": [],\n",
    "    \"LGB\": [],\n",
    "    \"RF\": [],\n",
    "    \"XGB\": [],\n",
    "    # \"random\": []\n",
    "}\n",
    "best_models_wesad_apd = {}\n",
    "ensemble_weights_wesad = {}\n",
    "\n",
    "num_iters = 1\n",
    "get_importance = True\n",
    "for _ in range(num_iters):\n",
    "    # include a subset of the target dataset in the training data\n",
    "    subjects = list(np.unique(x_b.loc[:, \"subject\"]))\n",
    "    train_subjects = random.sample(subjects, int(len(subjects) * percent_of_target_dataset))\n",
    "    x_train_addition = x_b[x_b[\"subject\"].isin(train_subjects)]\n",
    "    y_train_addition = y_b[y_b[\"subject\"].isin(train_subjects)]\n",
    "    x_train = pd.concat([x_a, x_train_addition])\n",
    "    y_train = pd.concat([y_a, y_train_addition])\n",
    "    x_test = x_b[~x_b[\"subject\"].isin(train_subjects)]\n",
    "    y_test = y_b[~y_b[\"subject\"].isin(train_subjects)]\n",
    "\n",
    "    # HYPERPARAMETER TUNING\n",
    "    model_data = train.grid_search_cv(\n",
    "        # models, parameters, x_a, y_a, by_subject=True, save_metrics=True, is_resample=True,\n",
    "        models, parameters, x_train, y_train, by_subject=True, save_metrics=True, is_resample=True,\n",
    "        get_importance=get_importance, drop_subject=True, test_size=0.0, folds=5\n",
    "    )\n",
    "\n",
    "    for model_name in models.keys():\n",
    "        best_models_wesad_apd[model_name] = model_data[model_name][\"best_model\"]\n",
    "        print(f\"{model_name}: {model_data[model_name]['best_params']}\")\n",
    "\n",
    "    # FEATURE SELECTION\n",
    "    features = {name: metrics+[\"lf_hf_ratio\"] for name in models.keys()}\n",
    "    # features = {name: metrics for name in models.keys()}\n",
    "    # features = train.feature_selection(best_models_wesad_apd, model_data[\"cv\"], x_train, y_train, n_features=5)\n",
    "\n",
    "    # out = train.Train_Multi_Dataset.train_across_datasets(best_models_wesad_apd, features, x_a, y_a, x_b, y_b, by_subject=True, save_metrics=True, test_size=test_size, is_resample=False, get_importance=get_importance, drop_subject=True)\n",
    "    out = train.Train_Multi_Dataset.train_across_datasets(best_models_wesad_apd, features, x_train, y_train, x_test, y_test, by_subject=True, save_metrics=True, test_size=test_size, is_resample=False, get_importance=get_importance, drop_subject=True)\n",
    "    \n",
    "    for model_name in acc_results:\n",
    "        acc_results[model_name].append(out[model_name][\"performance\"][0])\n",
    "        reports[model_name].append(out[model_name][\"performance\"][1])\n",
    "        if get_importance:\n",
    "            try:\n",
    "                print(\"\")\n",
    "                feature_imp = list(zip(metrics + [\"lf_hf_ratio\"], out[model_name][\"performance\"][2]))\n",
    "                feature_imp = sorted(feature_imp, key=lambda x: x[1], reverse=True)\n",
    "                print(feature_imp)\n",
    "            except Exception as e:\n",
    "                print(out[model_name][\"performance\"][2])\n",
    "            print(\"\")\n",
    "\n",
    "for model_name in acc_results.keys():\n",
    "    print(f\"Model evaluation metrics for {model_name}:\")\n",
    "    for i in range(len(reports[model_name])):\n",
    "        report = reports[model_name][i]\n",
    "        acc = acc_results[model_name][i]\n",
    "        p = report[\"precision\"]\n",
    "        r = report[\"recall\"]\n",
    "        f1 = report[\"f1\"]\n",
    "        auc = report[\"auc\"]\n",
    "        ensemble_weights_wesad[model_name] = acc\n",
    "        print(f\"\\tAccuracy: {acc}\\n\\tPrecision: {p}\\n\\tRecall: {r}\\n\\tF1-score: {f1}\\n\\tAUC score: {auc}\\n\" + \"-\"*40)\n",
    "    print(f\"Mean acc: {np.mean([acc_results[model_name][i] for i in range(len(reports[model_name]))])}\")\n",
    "    print(f\"Mean F1-score: {np.mean([reports[model_name][i]['f1'] for i in range(len(reports[model_name]))])}\")\n",
    "    print(f\"Mean AUC score: {np.mean([reports[model_name][i]['auc'] for i in range(len(reports[model_name]))])}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search for SVM ...\n",
      "Grid search for LGB ...\n",
      "Grid search for RF ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more of the test scores are non-finite: [0.90267697 0.91739911 0.91984496 0.9166367  0.913923   0.90540593\n",
      " 0.92092195 0.91074674 0.90872557 0.91018345 0.90199968 0.9184706\n",
      " 0.90846153 0.90662251 0.9081248  0.90710317 0.92408465 0.91345147\n",
      " 0.90597159 0.90678906 0.89254209 0.90335975 0.89663465 0.89489961\n",
      " 0.90033259        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search for XGB ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more of the test scores are non-finite: [0.87970325 0.88423    0.88829211 0.9002864  0.88335378 0.88999854\n",
      " 0.886285   0.89348158 0.87972692 0.87957318 0.88613975 0.89043935\n",
      " 0.88467993 0.88278386 0.88508146 0.89003939 0.90914273 0.94787234\n",
      " 0.94061798 0.93393987 0.90421062 0.9378251  0.93679833 0.92741159\n",
      " 0.89847011 0.93650024 0.92918678 0.92372758 0.90392697 0.94537151\n",
      " 0.9345893  0.92517628 0.94038001 0.93328945 0.93158295 0.93307957\n",
      " 0.9431582  0.92730459 0.92985187 0.93017889 0.91584525 0.92206586\n",
      " 0.92481546 0.9262759  0.92637895 0.92362357 0.92594384 0.92628388\n",
      " 0.93409738 0.93039177 0.9299775  0.92911825 0.93757703 0.93077824\n",
      " 0.92955941 0.92769658 0.93423947 0.93403514 0.93045815 0.93025283\n",
      " 0.92715457 0.92811647 0.92647456 0.92498742        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: {'C': 100, 'gamma': 0.01, 'kernel': 'rbf', 'probability': True}\n",
      "LGB: {'max_depth': 3, 'metric': 'binary_logloss', 'num_leaves': 10, 'objective': 'binary'}\n",
      "RF: {'max_features': 'sqrt', 'min_samples_split': 6, 'n_estimators': 20, 'random_state': 16}\n",
      "XGB: {'eval_metric': 'error', 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 20, 'objective': 'binary:logistic', 'random_state': 16, 'use_label_encoder': False}\n",
      "Model SVM, Actual: [0 1], [341 243], Predictions: [0 1], [269 315]\n",
      "coef_ only available for SVC with linear kernel\n",
      "SVM\n",
      "Model LGB, Actual: [0 1], [341 243], Predictions: [0 1], [394 190]\n",
      "LGB\n",
      "['bpm', 'rmssd', 'hf_rr', 'lf_rr', 'sdnn', 'mean_SCL', 'SCR_rate', 'ecg_mean', 'ecg_median', 'ecg_std', 'ecg_var', 'eda_mean', 'eda_median', 'eda_std', 'eda_var', 'lf_hf_ratio']\n",
      "Model RF, Actual: [0 1], [341 243], Predictions: [0 1], [422 162]\n",
      "RF\n",
      "['bpm' 'rmssd' 'hf_rr' 'lf_rr' 'sdnn' 'mean_SCL' 'SCR_rate' 'ecg_mean'\n",
      " 'ecg_median' 'ecg_std' 'ecg_var' 'eda_mean' 'eda_median' 'eda_std'\n",
      " 'eda_var' 'lf_hf_ratio']\n",
      "Model XGB, Actual: [0 1], [341 243], Predictions: [0 1], [404 180]\n",
      "XGB\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "[('bpm', 149), ('ecg_mean', 82), ('ecg_std', 54), ('sdnn', 37), ('rmssd', 26), ('hf_rr', 25), ('eda_mean', 25), ('mean_SCL', 22), ('SCR_rate', 19), ('eda_median', 18), ('eda_std', 18), ('eda_var', 15), ('lf_hf_ratio', 14), ('ecg_var', 13), ('lf_rr', 9), ('ecg_median', 9)]\n",
      "\n",
      "\n",
      "[('bpm', 0.2816102138168804), ('ecg_median', 0.12826600225958637), ('ecg_mean', 0.10704381466985727), ('sdnn', 0.08839160066315045), ('rmssd', 0.07069783959286909), ('ecg_var', 0.0696483144038915), ('ecg_std', 0.05327439195551883), ('eda_std', 0.03780483313008749), ('eda_var', 0.03693492942234254), ('SCR_rate', 0.025716665741571155), ('eda_mean', 0.024037704355975683), ('eda_median', 0.02342323273835722), ('mean_SCL', 0.01715076863160933), ('lf_hf_ratio', 0.014230904886980741), ('hf_rr', 0.012064564896972326), ('lf_rr', 0.009704218834349626)]\n",
      "\n",
      "\n",
      "[('ecg_median', 0.25933063), ('bpm', 0.14176993), ('eda_std', 0.13320507), ('ecg_mean', 0.059689146), ('ecg_var', 0.05544988), ('rmssd', 0.053583216), ('hf_rr', 0.05173569), ('eda_var', 0.04844367), ('ecg_std', 0.038126882), ('eda_mean', 0.037856437), ('sdnn', 0.030537711), ('mean_SCL', 0.02547977), ('lf_rr', 0.024991935), ('lf_hf_ratio', 0.022010967), ('SCR_rate', 0.017789159), ('eda_median', 0.0)]\n",
      "\n",
      "Model evaluation metrics for SVM:\n",
      "\tAccuracy: 0.5102739726027398\n",
      "\tPrecision: 0.43174603174603177\n",
      "\tRecall: 0.5596707818930041\n",
      "\tF1-score: 0.4874551971326165\n",
      "\tAUC score: 0.5173720478379977\n",
      "----------------------------------------\n",
      "Mean acc: 0.5102739726027398\n",
      "Mean F1-score: 0.4874551971326165\n",
      "Mean AUC score: 0.5173720478379977\n",
      "\n",
      "\n",
      "Model evaluation metrics for LGB:\n",
      "\tAccuracy: 0.5667808219178082\n",
      "\tPrecision: 0.47368421052631576\n",
      "\tRecall: 0.37037037037037035\n",
      "\tF1-score: 0.4157043879907621\n",
      "\tAUC score: 0.5385576192027804\n",
      "----------------------------------------\n",
      "Mean acc: 0.5667808219178082\n",
      "Mean F1-score: 0.4157043879907621\n",
      "Mean AUC score: 0.5385576192027804\n",
      "\n",
      "\n",
      "Model evaluation metrics for RF:\n",
      "\tAccuracy: 0.5667808219178082\n",
      "\tPrecision: 0.4691358024691358\n",
      "\tRecall: 0.31275720164609055\n",
      "\tF1-score: 0.37530864197530867\n",
      "\tAUC score: 0.5302788940781772\n",
      "----------------------------------------\n",
      "Mean acc: 0.5667808219178082\n",
      "Mean F1-score: 0.37530864197530867\n",
      "Mean AUC score: 0.5302788940781772\n",
      "\n",
      "\n",
      "Model evaluation metrics for XGB:\n",
      "\tAccuracy: 0.5462328767123288\n",
      "\tPrecision: 0.4388888888888889\n",
      "\tRecall: 0.32510288065843623\n",
      "\tF1-score: 0.3735224586288416\n",
      "\tAUC score: 0.5144575986869894\n",
      "----------------------------------------\n",
      "Mean acc: 0.5462328767123288\n",
      "Mean F1-score: 0.3735224586288416\n",
      "Mean AUC score: 0.5144575986869894\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN ON WESAD AND TEST ON APD -- SPEECH TASK ONLY\n",
    "importlib.reload(train)\n",
    "importlib.reload(dr_a)\n",
    "importlib.reload(dr_w)\n",
    "importlib.reload(dt)\n",
    "\n",
    "\n",
    "random.seed(38)\n",
    "\n",
    "model_phases_apd_speech = [\n",
    "    \"Baseline_Rest\", \n",
    "    # \"BugBox_Relax\", \"BugBox_Anticipate\", \"BugBox_Exposure\", \"BugBox_Break\",\n",
    "    \"Speech_Relax\", \"Speech_Anticipate\", \"Speech_Exposure\", \"Speech_Break\"\n",
    "]\n",
    "\n",
    "x_a, y_a = train.Train_WESAD.get_wesad_data(metrics, model_phases_wesad, verbose=False, label_type=wesad_label_type, threshold=threshold, normalize=True, standardize=False)\n",
    "x_b, y_b = train.Train_APD.get_apd_data_ranking(metrics, model_phases_apd_speech, verbose=False, anxiety_label_type=anxiety_label_type, threshold=threshold, normalize=True, standardize=False)\n",
    "# drop subjects with noisy data\n",
    "x_b = x_b[~x_b[\"subject\"].isin(invalid_apd_subjects)].reset_index(drop=True)\n",
    "y_b = y_b[~y_b[\"subject\"].isin(invalid_apd_subjects)].reset_index(drop=True)\n",
    "if anxiety_label_type is not None:\n",
    "    x_b = x_b.drop([\"anxietyGroup\"], axis=1)  # drop anxietyGroup column because WESAD doesn't have this feature\n",
    "\n",
    "x_a = x_a.drop([\"phaseId\"], axis=1)\n",
    "x_b = x_b.drop([\"phaseId\"], axis=1)\n",
    "\n",
    "inds = pd.isnull(x_a).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_a = x_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_a = y_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "inds = pd.isnull(x_b).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_b = x_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_b = y_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "\n",
    "# make sure subjects from different datasets aren't labeled with the same index\n",
    "x_b[\"subject\"] = x_b[\"subject\"] + 500\n",
    "y_b[\"subject\"] = y_b[\"subject\"] + 500\n",
    "\n",
    "# augment smaller dataset by randomly duplicating a subset of subjects\n",
    "# n_wesad = x_a.shape[0]\n",
    "# n_apd = x_b.shape[0]\n",
    "# n_samples_to_augment = int((0.7 - (n_wesad / n_apd)) * n_wesad)\n",
    "# augment_indices = random.sample(range(n_wesad), n_samples_to_augment)\n",
    "\n",
    "# duplicates = x_a.iloc[augment_indices, :].reset_index(drop=True)\n",
    "# x_a = pd.concat([x_a, duplicates], axis=0).reset_index(drop=True)\n",
    "# duplicates = y_a.iloc[augment_indices, :].reset_index(drop=True)\n",
    "# y_a = pd.concat([y_a, duplicates], axis=0).reset_index(drop=True)\n",
    "\n",
    "acc_results = {\n",
    "    \"SVM\": [],\n",
    "    \"LGB\": [],\n",
    "    \"RF\": [],\n",
    "    \"XGB\": [],\n",
    "    # \"random\": []\n",
    "}\n",
    "reports = {\n",
    "    \"SVM\": [],\n",
    "    \"LGB\": [],\n",
    "    \"RF\": [],\n",
    "    \"XGB\": [],\n",
    "    # \"random\": []\n",
    "}\n",
    "best_models_wesad_apd = {}\n",
    "\n",
    "num_iters = 1\n",
    "get_importance = True\n",
    "for _ in range(num_iters):\n",
    "    # include a subset of the target dataset in the training data\n",
    "    subjects = list(np.unique(x_b.loc[:, \"subject\"]))\n",
    "    train_subjects = random.sample(subjects, int(len(subjects) * percent_of_target_dataset))\n",
    "    x_train_addition = x_b[x_b[\"subject\"].isin(train_subjects)]\n",
    "    y_train_addition = y_b[y_b[\"subject\"].isin(train_subjects)]\n",
    "    x_train = pd.concat([x_a, x_train_addition])\n",
    "    y_train = pd.concat([y_a, y_train_addition])\n",
    "    x_test = x_b[~x_b[\"subject\"].isin(train_subjects)]\n",
    "    y_test = y_b[~y_b[\"subject\"].isin(train_subjects)]\n",
    "\n",
    "    # HYPERPARAMETER TUNING\n",
    "    model_data = train.grid_search_cv(\n",
    "        # models, parameters, x_a, y_a, by_subject=True, save_metrics=True, is_resample=True,\n",
    "        models, parameters, x_train, y_train, by_subject=True, save_metrics=True, is_resample=True,\n",
    "        get_importance=get_importance, drop_subject=True, test_size=0.0, folds=5\n",
    "    )\n",
    "\n",
    "    for model_name in models.keys():\n",
    "        best_models_wesad_apd[model_name] = model_data[model_name][\"best_model\"]\n",
    "        print(f\"{model_name}: {model_data[model_name]['best_params']}\")\n",
    "\n",
    "    # FEATURE SELECTION\n",
    "    features = {name: metrics+[\"lf_hf_ratio\"] for name in models.keys()}\n",
    "    # features = {name: metrics for name in models.keys()}\n",
    "    # features = train.feature_selection(best_models_wesad_apd, model_data[\"cv\"], x_train, y_train, n_features=5)\n",
    "\n",
    "    # out = train.Train_Multi_Dataset.train_across_datasets(best_models_wesad_apd, features, x_a, y_a, x_b, y_b, by_subject=True, save_metrics=True, test_size=test_size, is_resample=False, get_importance=get_importance, drop_subject=True)\n",
    "    out = train.Train_Multi_Dataset.train_across_datasets(best_models_wesad_apd, features, x_train, y_train, x_test, y_test, by_subject=True, save_metrics=True, test_size=test_size, is_resample=False, get_importance=get_importance, drop_subject=True)\n",
    "    \n",
    "    for model_name in acc_results:\n",
    "        acc_results[model_name].append(out[model_name][\"performance\"][0])\n",
    "        reports[model_name].append(out[model_name][\"performance\"][1])\n",
    "        if get_importance:\n",
    "            try:\n",
    "                print(\"\")\n",
    "                feature_imp = list(zip(metrics + [\"lf_hf_ratio\"], out[model_name][\"performance\"][2]))\n",
    "                feature_imp = sorted(feature_imp, key=lambda x: x[1], reverse=True)\n",
    "                print(feature_imp)\n",
    "            except Exception as e:\n",
    "                print(out[model_name][\"performance\"][2])\n",
    "            print(\"\")\n",
    "\n",
    "for model_name in acc_results.keys():\n",
    "    print(f\"Model evaluation metrics for {model_name}:\")\n",
    "    for i in range(len(reports[model_name])):\n",
    "        report = reports[model_name][i]\n",
    "        acc = acc_results[model_name][i]\n",
    "        p = report[\"precision\"]\n",
    "        r = report[\"recall\"]\n",
    "        f1 = report[\"f1\"]\n",
    "        auc = report[\"auc\"]\n",
    "        print(f\"\\tAccuracy: {acc}\\n\\tPrecision: {p}\\n\\tRecall: {r}\\n\\tF1-score: {f1}\\n\\tAUC score: {auc}\\n\" + \"-\"*40)\n",
    "    print(f\"Mean acc: {np.mean([acc_results[model_name][i] for i in range(len(reports[model_name]))])}\")\n",
    "    print(f\"Mean F1-score: {np.mean([reports[model_name][i]['f1'] for i in range(len(reports[model_name]))])}\")\n",
    "    print(f\"Mean AUC score: {np.mean([reports[model_name][i]['auc'] for i in range(len(reports[model_name]))])}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search for SVM ...\n",
      "Grid search for LGB ...\n",
      "Grid search for RF ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more of the test scores are non-finite: [0.88358063 0.90541408 0.91086633 0.91211001 0.91441431 0.8796707\n",
      " 0.90425145 0.9097068  0.91181717 0.91608554 0.88287428 0.90614195\n",
      " 0.91154083 0.90716994 0.9126924  0.88425733 0.90889546 0.91090658\n",
      " 0.90681366 0.9120619  0.89062089 0.91586398 0.91438891 0.90958999\n",
      " 0.91538002        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search for XGB ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more of the test scores are non-finite: [0.87509256 0.90374647 0.90484339 0.91429447 0.8774994  0.8821436\n",
      " 0.90427749 0.9111168  0.89038213 0.89856448 0.91791841 0.92350361\n",
      " 0.88982033 0.89856448 0.91791841 0.92336316 0.92086048 0.91704387\n",
      " 0.91308555 0.91677551 0.9227308  0.92839734 0.921489   0.91601966\n",
      " 0.926908   0.92409714 0.91807174 0.91819346 0.92729178 0.9235131\n",
      " 0.92192157 0.91999285 0.9210014  0.91530254 0.90872717 0.90444826\n",
      " 0.91500491 0.91013015 0.91095997 0.91111797 0.91054447 0.91070657\n",
      " 0.91006694 0.90965379 0.91289946 0.91168972 0.91143387 0.91133918\n",
      " 0.91425444 0.89919031 0.89728253 0.89585346 0.91523384 0.9072569\n",
      " 0.90371055 0.90478394 0.91326239 0.90706787 0.90565858 0.90635498\n",
      " 0.91838389 0.91067216 0.90867333 0.90838658        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: {'C': 100, 'gamma': 0.01, 'kernel': 'rbf', 'probability': True}\n",
      "LGB: {'max_depth': 5, 'metric': 'binary_logloss', 'num_leaves': 20, 'objective': 'binary'}\n",
      "RF: {'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 50, 'random_state': 16}\n",
      "XGB: {'eval_metric': 'error', 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 20, 'objective': 'binary:logistic', 'random_state': 16, 'use_label_encoder': False}\n",
      "Model SVM, Actual: [0 1], [297 255], Predictions: [0 1], [219 333]\n",
      "coef_ only available for SVC with linear kernel\n",
      "SVM\n",
      "Model LGB, Actual: [0 1], [297 255], Predictions: [0 1], [338 214]\n",
      "LGB\n",
      "['bpm', 'rmssd', 'hf_rr', 'lf_rr', 'sdnn', 'mean_SCL', 'SCR_rate', 'ecg_mean', 'ecg_median', 'ecg_std', 'ecg_var', 'eda_mean', 'eda_median', 'eda_std', 'eda_var', 'lf_hf_ratio']\n",
      "Model RF, Actual: [0 1], [297 255], Predictions: [0 1], [327 225]\n",
      "RF\n",
      "['bpm' 'rmssd' 'hf_rr' 'lf_rr' 'sdnn' 'mean_SCL' 'SCR_rate' 'ecg_mean'\n",
      " 'ecg_median' 'ecg_std' 'ecg_var' 'eda_mean' 'eda_median' 'eda_std'\n",
      " 'eda_var' 'lf_hf_ratio']\n",
      "Model XGB, Actual: [0 1], [297 255], Predictions: [0 1], [363 189]\n",
      "XGB\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "[('bpm', 267), ('ecg_mean', 126), ('ecg_std', 104), ('rmssd', 86), ('eda_mean', 69), ('lf_hf_ratio', 65), ('sdnn', 60), ('SCR_rate', 59), ('mean_SCL', 53), ('eda_std', 53), ('ecg_var', 46), ('hf_rr', 45), ('lf_rr', 33), ('ecg_median', 33), ('eda_median', 31), ('eda_var', 26)]\n",
      "\n",
      "\n",
      "[('bpm', 0.24460061521384413), ('ecg_mean', 0.1101979992633423), ('ecg_median', 0.09064903879764546), ('rmssd', 0.08487395447350345), ('ecg_var', 0.08461542443260159), ('sdnn', 0.07214604943107879), ('ecg_std', 0.06475588913261746), ('SCR_rate', 0.04696285048484688), ('eda_std', 0.040450711723884744), ('eda_var', 0.034927373705906345), ('eda_mean', 0.029845619539732388), ('eda_median', 0.02674315172198516), ('lf_hf_ratio', 0.020950653785348328), ('hf_rr', 0.01730317652105101), ('mean_SCL', 0.01574248780071168), ('lf_rr', 0.015235003971900345)]\n",
      "\n",
      "\n",
      "[('ecg_median', 0.34279546), ('bpm', 0.14141406), ('eda_std', 0.12332749), ('ecg_mean', 0.054172475), ('eda_var', 0.051247954), ('rmssd', 0.050426297), ('ecg_var', 0.04867664), ('eda_mean', 0.045268618), ('ecg_std', 0.036565427), ('lf_hf_ratio', 0.024543332), ('mean_SCL', 0.02045513), ('hf_rr', 0.020289931), ('SCR_rate', 0.014656204), ('lf_rr', 0.013182632), ('sdnn', 0.012978342), ('eda_median', 0.0)]\n",
      "\n",
      "Model evaluation metrics for SVM:\n",
      "\tAccuracy: 0.5181159420289855\n",
      "\tPrecision: 0.48348348348348347\n",
      "\tRecall: 0.6313725490196078\n",
      "\tF1-score: 0.5476190476190476\n",
      "\tAUC score: 0.5261239849475143\n",
      "----------------------------------------\n",
      "Mean acc: 0.5181159420289855\n",
      "Mean F1-score: 0.5476190476190476\n",
      "Mean AUC score: 0.5261239849475143\n",
      "\n",
      "\n",
      "Model evaluation metrics for LGB:\n",
      "\tAccuracy: 0.5597826086956522\n",
      "\tPrecision: 0.5280373831775701\n",
      "\tRecall: 0.44313725490196076\n",
      "\tF1-score: 0.48187633262260127\n",
      "\tAUC score: 0.5515349574173103\n",
      "----------------------------------------\n",
      "Mean acc: 0.5597826086956522\n",
      "Mean F1-score: 0.48187633262260127\n",
      "Mean AUC score: 0.5515349574173103\n",
      "\n",
      "\n",
      "Model evaluation metrics for RF:\n",
      "\tAccuracy: 0.5398550724637681\n",
      "\tPrecision: 0.5022222222222222\n",
      "\tRecall: 0.44313725490196076\n",
      "\tF1-score: 0.4708333333333333\n",
      "\tAUC score: 0.5330164388987918\n",
      "----------------------------------------\n",
      "Mean acc: 0.5398550724637681\n",
      "Mean F1-score: 0.4708333333333333\n",
      "Mean AUC score: 0.5330164388987918\n",
      "\n",
      "\n",
      "Model evaluation metrics for XGB:\n",
      "\tAccuracy: 0.5507246376811594\n",
      "\tPrecision: 0.5185185185185185\n",
      "\tRecall: 0.3843137254901961\n",
      "\tF1-score: 0.44144144144144143\n",
      "\tAUC score: 0.5389582095464448\n",
      "----------------------------------------\n",
      "Mean acc: 0.5507246376811594\n",
      "Mean F1-score: 0.44144144144144143\n",
      "Mean AUC score: 0.5389582095464448\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN ON WESAD AND TEST ON APD -- BUG TASK ONLY\n",
    "importlib.reload(train)\n",
    "importlib.reload(dr_a)\n",
    "importlib.reload(dr_w)\n",
    "importlib.reload(dt)\n",
    "\n",
    "\n",
    "random.seed(38)\n",
    "\n",
    "model_phases_apd_speech = [\n",
    "    \"Baseline_Rest\", \n",
    "    \"BugBox_Relax\", \"BugBox_Anticipate\", \"BugBox_Exposure\", \"BugBox_Break\",\n",
    "    # \"Speech_Relax\", \"Speech_Anticipate\", \"Speech_Exposure\", \"Speech_Break\"\n",
    "]\n",
    "\n",
    "x_a, y_a = train.Train_WESAD.get_wesad_data(metrics, model_phases_wesad, verbose=False, label_type=wesad_label_type, threshold=threshold, normalize=True, standardize=False)\n",
    "x_b, y_b = train.Train_APD.get_apd_data_ranking(metrics, model_phases_apd_speech, verbose=False, anxiety_label_type=anxiety_label_type, threshold=threshold, normalize=True, standardize=False)\n",
    "# drop subjects with noisy data\n",
    "x_b = x_b[~x_b[\"subject\"].isin(invalid_apd_subjects)].reset_index(drop=True)\n",
    "y_b = y_b[~y_b[\"subject\"].isin(invalid_apd_subjects)].reset_index(drop=True)\n",
    "if anxiety_label_type is not None:\n",
    "    x_b = x_b.drop([\"anxietyGroup\"], axis=1)  # drop anxietyGroup column because WESAD doesn't have this feature\n",
    "\n",
    "x_a = x_a.drop([\"phaseId\"], axis=1)\n",
    "x_b = x_b.drop([\"phaseId\"], axis=1)\n",
    "\n",
    "inds = pd.isnull(x_a).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_a = x_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_a = y_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "inds = pd.isnull(x_b).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_b = x_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_b = y_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "\n",
    "# make sure subjects from different datasets aren't labeled with the same index\n",
    "x_b[\"subject\"] = x_b[\"subject\"] + 500\n",
    "y_b[\"subject\"] = y_b[\"subject\"] + 500\n",
    "\n",
    "# augment smaller dataset by randomly duplicating a subset of subjects\n",
    "# n_wesad = x_a.shape[0]\n",
    "# n_apd = x_b.shape[0]\n",
    "# n_samples_to_augment = int((0.7 - (n_wesad / n_apd)) * n_wesad)\n",
    "# augment_indices = random.sample(range(n_wesad), n_samples_to_augment)\n",
    "\n",
    "# duplicates = x_a.iloc[augment_indices, :].reset_index(drop=True)\n",
    "# x_a = pd.concat([x_a, duplicates], axis=0).reset_index(drop=True)\n",
    "# duplicates = y_a.iloc[augment_indices, :].reset_index(drop=True)\n",
    "# y_a = pd.concat([y_a, duplicates], axis=0).reset_index(drop=True)\n",
    "\n",
    "acc_results = {\n",
    "    \"SVM\": [],\n",
    "    \"LGB\": [],\n",
    "    \"RF\": [],\n",
    "    \"XGB\": [],\n",
    "    # \"random\": []\n",
    "}\n",
    "reports = {\n",
    "    \"SVM\": [],\n",
    "    \"LGB\": [],\n",
    "    \"RF\": [],\n",
    "    \"XGB\": [],\n",
    "    # \"random\": []\n",
    "}\n",
    "best_models_wesad_apd = {}\n",
    "\n",
    "num_iters = 1\n",
    "get_importance = True\n",
    "for _ in range(num_iters):\n",
    "    # include a subset of the target dataset in the training data\n",
    "    subjects = list(np.unique(x_b.loc[:, \"subject\"]))\n",
    "    train_subjects = random.sample(subjects, int(len(subjects) * percent_of_target_dataset))\n",
    "    x_train_addition = x_b[x_b[\"subject\"].isin(train_subjects)]\n",
    "    y_train_addition = y_b[y_b[\"subject\"].isin(train_subjects)]\n",
    "    x_train = pd.concat([x_a, x_train_addition])\n",
    "    y_train = pd.concat([y_a, y_train_addition])\n",
    "    x_test = x_b[~x_b[\"subject\"].isin(train_subjects)]\n",
    "    y_test = y_b[~y_b[\"subject\"].isin(train_subjects)]\n",
    "\n",
    "    # HYPERPARAMETER TUNING\n",
    "    model_data = train.grid_search_cv(\n",
    "        # models, parameters, x_a, y_a, by_subject=True, save_metrics=True, is_resample=True,\n",
    "        models, parameters, x_train, y_train, by_subject=True, save_metrics=True, is_resample=True,\n",
    "        get_importance=get_importance, drop_subject=True, test_size=0.0, folds=5\n",
    "    )\n",
    "\n",
    "    for model_name in models.keys():\n",
    "        best_models_wesad_apd[model_name] = model_data[model_name][\"best_model\"]\n",
    "        print(f\"{model_name}: {model_data[model_name]['best_params']}\")\n",
    "\n",
    "    # FEATURE SELECTION\n",
    "    features = {name: metrics+[\"lf_hf_ratio\"] for name in models.keys()}\n",
    "    # features = {name: metrics for name in models.keys()}\n",
    "    # features = train.feature_selection(best_models_wesad_apd, model_data[\"cv\"], x_train, y_train, n_features=5)\n",
    "\n",
    "    # out = train.Train_Multi_Dataset.train_across_datasets(best_models_wesad_apd, features, x_a, y_a, x_b, y_b, by_subject=True, save_metrics=True, test_size=test_size, is_resample=False, get_importance=get_importance, drop_subject=True)\n",
    "    out = train.Train_Multi_Dataset.train_across_datasets(best_models_wesad_apd, features, x_train, y_train, x_test, y_test, by_subject=True, save_metrics=True, test_size=test_size, is_resample=False, get_importance=get_importance, drop_subject=True)\n",
    "    \n",
    "    for model_name in acc_results:\n",
    "        acc_results[model_name].append(out[model_name][\"performance\"][0])\n",
    "        reports[model_name].append(out[model_name][\"performance\"][1])\n",
    "        if get_importance:\n",
    "            try:\n",
    "                print(\"\")\n",
    "                feature_imp = list(zip(metrics + [\"lf_hf_ratio\"], out[model_name][\"performance\"][2]))\n",
    "                feature_imp = sorted(feature_imp, key=lambda x: x[1], reverse=True)\n",
    "                print(feature_imp)\n",
    "            except Exception as e:\n",
    "                print(out[model_name][\"performance\"][2])\n",
    "            print(\"\")\n",
    "\n",
    "for model_name in acc_results.keys():\n",
    "    print(f\"Model evaluation metrics for {model_name}:\")\n",
    "    for i in range(len(reports[model_name])):\n",
    "        report = reports[model_name][i]\n",
    "        acc = acc_results[model_name][i]\n",
    "        p = report[\"precision\"]\n",
    "        r = report[\"recall\"]\n",
    "        f1 = report[\"f1\"]\n",
    "        auc = report[\"auc\"]\n",
    "        print(f\"\\tAccuracy: {acc}\\n\\tPrecision: {p}\\n\\tRecall: {r}\\n\\tF1-score: {f1}\\n\\tAUC score: {auc}\\n\" + \"-\"*40)\n",
    "    print(f\"Mean acc: {np.mean([acc_results[model_name][i] for i in range(len(reports[model_name]))])}\")\n",
    "    print(f\"Mean F1-score: {np.mean([reports[model_name][i]['f1'] for i in range(len(reports[model_name]))])}\")\n",
    "    print(f\"Mean AUC score: {np.mean([reports[model_name][i]['auc'] for i in range(len(reports[model_name]))])}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSEMBLE\n",
    "importlib.reload(train)\n",
    "importlib.reload(dr_a)\n",
    "importlib.reload(dr_w)\n",
    "importlib.reload(dt)\n",
    "\n",
    "from scipy.stats import mode\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "random.seed(38)\n",
    "\n",
    "percent_of_target_dataset = 0.0\n",
    "folds = 5\n",
    "ensemble_models = [\n",
    "    \"SVM\", \n",
    "    \"LGB\",\n",
    "    \"RF\",\n",
    "    \"XGB\"\n",
    "]\n",
    "\n",
    "\n",
    "def train_predict_ensemble(ensemble_models, x_train, y_train, x_test, y_test, features, type=\"majority_vote\", weights=None):\n",
    "    y_preds = []\n",
    "    if type == \"majority_vote\":\n",
    "        # features = list(features.values())[0]\n",
    "        # ensemble_models = [(key, ensemble_models[key]) for key in ensemble_models.keys()]\n",
    "        # ensemble = VotingClassifier(estimators=ensemble_models, voting='hard', weights=weights)\n",
    "        # ensemble.fit(x_train.loc[:, features], y_train)\n",
    "        # y_preds = ensemble.predict(x_test.loc[:, features])\n",
    "        for model_name in ensemble_models:\n",
    "            x_test_temp = x_test.loc[:, features[model_name]]\n",
    "            y_pred = ensemble_models[model_name].predict(x_test_temp)\n",
    "            y_preds.append(y_pred)\n",
    "        y_preds = mode(y_preds, axis=0)[0]\n",
    "        y_preds = np.reshape(y_preds, (y_preds.shape[1], 1))\n",
    "    elif type == \"weighted_avg\":\n",
    "        features = list(features.values())[0]\n",
    "        ensemble_models = [(key, ensemble_models[key]) for key in ensemble_models.keys()]\n",
    "        ensemble = VotingClassifier(estimators=ensemble_models, voting='soft', weights=weights)\n",
    "        ensemble.fit(x_train.loc[:, features], y_train)\n",
    "        y_preds = ensemble.predict(x_test.loc[:, features])\n",
    "\n",
    "    return y_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train APD, test WESAD\n",
      "APD size: 1032\n",
      "0    570\n",
      "1    462\n",
      "Name: label, dtype: int64\n",
      "WESAD size: 523\n",
      "0    428\n",
      "1     95\n",
      "Name: label, dtype: int64\n",
      "Predictions:\n",
      "[[  0 183]\n",
      " [  1 340]]\n",
      "\tAccuracy: 0.40152963671128106\n",
      "\tPrecision: 0.17941176470588235\n",
      "\tRecall: 0.6421052631578947\n",
      "\tF1-score: 0.28045977011494255\n",
      "\tAUC score: 0.49511805213969495\n",
      "----------------------------------------\n",
      "Train WESAD, test APD\n",
      "WESAD size: 523\n",
      "0    428\n",
      "1     95\n",
      "Name: label, dtype: int64\n",
      "APD size: 1032\n",
      "0    570\n",
      "1    462\n",
      "Name: label, dtype: int64\n",
      "Predictions:\n",
      "[[  0 662]\n",
      " [  1 370]]\n",
      "\tAccuracy: 0.5542635658914729\n",
      "\tPrecision: 0.5027027027027027\n",
      "\tRecall: 0.4025974025974026\n",
      "\tF1-score: 0.44711538461538464\n",
      "\tAUC score: 0.5398951925267715\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ENSEMBLE\n",
    "importlib.reload(train)\n",
    "importlib.reload(dr_a)\n",
    "importlib.reload(dr_w)\n",
    "importlib.reload(dt)\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "\n",
    "random.seed(38)\n",
    "\n",
    "model_phases_apd = [\n",
    "    \"Baseline_Rest\", \n",
    "    \"BugBox_Relax\", \"BugBox_Anticipate\", \"BugBox_Exposure\", \"BugBox_Break\",\n",
    "    \"Speech_Relax\", \"Speech_Anticipate\", \"Speech_Exposure\", \"Speech_Break\"\n",
    "]\n",
    "\n",
    "voting_type = \"weighted_avg\"\n",
    "# voting_type = \"majority_vote\"\n",
    "\n",
    "x_a, y_a = train.Train_APD.get_apd_data_ranking(metrics, model_phases_apd, verbose=False, anxiety_label_type=anxiety_label_type, threshold=threshold, normalize=True, standardize=False)\n",
    "x_b, y_b = train.Train_WESAD.get_wesad_data(metrics, model_phases_wesad, verbose=False, label_type=wesad_label_type, threshold=threshold, normalize=True, standardize=False)\n",
    "# drop subjects with noisy data\n",
    "# x_a = x_a[~x_a[\"subject\"].isin(invalid_apd_subjects)].reset_index(drop=True)\n",
    "# y_a = y_a[~y_a[\"subject\"].isin(invalid_apd_subjects)].reset_index(drop=True)\n",
    "\n",
    "x_a = x_a.drop([\"phaseId\"], axis=1)\n",
    "x_b = x_b.drop([\"phaseId\"], axis=1)\n",
    "\n",
    "inds = pd.isnull(x_a).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_a = x_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_a = y_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "inds = pd.isnull(x_b).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_b = x_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_b = y_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "\n",
    "# make sure subjects from different datasets aren't labeled with the same index\n",
    "x_b[\"subject\"] = x_b[\"subject\"] + 500\n",
    "y_b[\"subject\"] = y_b[\"subject\"] + 500\n",
    "\n",
    "subjects = list(np.unique(x_b.loc[:, \"subject\"]))\n",
    "train_subjects = random.sample(subjects, int(len(subjects) * percent_of_target_dataset))\n",
    "x_train_addition = x_b[x_b[\"subject\"].isin(train_subjects)]\n",
    "y_train_addition = y_b[y_b[\"subject\"].isin(train_subjects)]\n",
    "x_train = pd.concat([x_a, x_train_addition])\n",
    "y_train = pd.concat([y_a, y_train_addition])\n",
    "x_test = x_b[~x_b[\"subject\"].isin(train_subjects)]\n",
    "y_test = y_b[~y_b[\"subject\"].isin(train_subjects)]\n",
    "\n",
    "y_train = y_train.loc[:, \"label\"]\n",
    "y_test = y_test.loc[:, \"label\"]\n",
    "\n",
    "estimators = {name: best_models_apd_wesad[name] for name in ensemble_models}\n",
    "weights = [ensemble_weights_apd[model_name] for model_name in estimators.keys()]\n",
    "# weights = np.divide(weights, np.sum(weights))\n",
    "weights = np.divide(weights, len(weights))\n",
    "# weights = [1 for _ in range(len(list(estimators.keys())))]\n",
    "y_pred = train_predict_ensemble(estimators, x_train, y_train, x_test, y_test, features, type=voting_type, weights=weights)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "p = precision_score(y_test, y_pred, zero_division=0)\n",
    "r = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"Train APD, test WESAD\")\n",
    "print(f\"APD size: {x_train.shape[0]}\\n{y_train.value_counts()}\\nWESAD size: {x_test.shape[0]}\\n{y_test.value_counts()}\")\n",
    "print(\"Predictions:\")\n",
    "print(f\"{np.array(np.unique(y_pred, return_counts=True)).T}\")\n",
    "print(f\"\\tAccuracy: {acc}\\n\\tPrecision: {p}\\n\\tRecall: {r}\\n\\tF1-score: {f1}\\n\\tAUC score: {auc}\\n\" + \"-\"*40)\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "model_phases_apd = [\n",
    "    \"Baseline_Rest\", \n",
    "    \"BugBox_Relax\", \"BugBox_Anticipate\", \"BugBox_Exposure\", \"BugBox_Break\",\n",
    "    \"Speech_Relax\", \"Speech_Anticipate\", \"Speech_Exposure\", \"Speech_Break\"\n",
    "]\n",
    "\n",
    "x_a, y_a = train.Train_WESAD.get_wesad_data(metrics, model_phases_wesad, verbose=False, label_type=wesad_label_type, threshold=threshold, normalize=True, standardize=False)\n",
    "x_b, y_b = train.Train_APD.get_apd_data_ranking(metrics, model_phases_apd, verbose=False, anxiety_label_type=anxiety_label_type, threshold=threshold, normalize=True, standardize=False)\n",
    "# drop subjects with noisy data\n",
    "# x_a = x_a[~x_a[\"subject\"].isin(invalid_apd_subjects)].reset_index(drop=True)\n",
    "# y_a = y_a[~y_a[\"subject\"].isin(invalid_apd_subjects)].reset_index(drop=True)\n",
    "\n",
    "x_a = x_a.drop([\"phaseId\"], axis=1)\n",
    "x_b = x_b.drop([\"phaseId\"], axis=1)\n",
    "\n",
    "inds = pd.isnull(x_a).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_a = x_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_a = y_a.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "inds = pd.isnull(x_b).any(axis=1).to_numpy().nonzero()[0]\n",
    "x_b = x_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "y_b = y_b.drop(labels=inds, axis=0).reset_index(drop=True)\n",
    "\n",
    "# make sure subjects from different datasets aren't labeled with the same index\n",
    "x_b[\"subject\"] = x_b[\"subject\"] + 500\n",
    "y_b[\"subject\"] = y_b[\"subject\"] + 500\n",
    "\n",
    "# augment smaller dataset by randomly duplicating a subset of subjects\n",
    "# n_wesad = x_a.shape[0]\n",
    "# n_apd = x_b.shape[0]\n",
    "# n_samples_to_augment = int((0.7 - (n_wesad / n_apd)) * n_wesad)\n",
    "# augment_indices = random.sample(range(n_wesad), n_samples_to_augment)\n",
    "\n",
    "# duplicates = x_a.iloc[augment_indices, :].reset_index(drop=True)\n",
    "# x_a = pd.concat([x_a, duplicates], axis=0).reset_index(drop=True)\n",
    "# duplicates = y_a.iloc[augment_indices, :].reset_index(drop=True)\n",
    "# y_a = pd.concat([y_a, duplicates], axis=0).reset_index(drop=True)\n",
    "\n",
    "subjects = list(np.unique(x_b.loc[:, \"subject\"]))\n",
    "train_subjects = random.sample(subjects, int(len(subjects) * percent_of_target_dataset))\n",
    "x_train_addition = x_b[x_b[\"subject\"].isin(train_subjects)]\n",
    "y_train_addition = y_b[y_b[\"subject\"].isin(train_subjects)]\n",
    "x_train = pd.concat([x_a, x_train_addition])\n",
    "y_train = pd.concat([y_a, y_train_addition])\n",
    "x_test = x_b[~x_b[\"subject\"].isin(train_subjects)]\n",
    "y_test = y_b[~y_b[\"subject\"].isin(train_subjects)]\n",
    "\n",
    "y_train = y_train.loc[:, \"label\"]\n",
    "y_test = y_test.loc[:, \"label\"]\n",
    "\n",
    "estimators = {name: best_models_wesad_apd[name] for name in ensemble_models}\n",
    "weights = [ensemble_weights_wesad[model_name] for model_name in estimators.keys()]\n",
    "# weights = np.divide(weights, np.sum(weights))\n",
    "weights = np.divide(weights, len(weights))\n",
    "# weights = [1 for _ in range(len(list(estimators.keys())))]\n",
    "y_pred = train_predict_ensemble(estimators, x_train, y_train, x_test, y_test, features, type=voting_type, weights=weights)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "p = precision_score(y_test, y_pred, zero_division=0)\n",
    "r = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"Train WESAD, test APD\")\n",
    "print(f\"WESAD size: {x_train.shape[0]}\\n{y_train.value_counts()}\\nAPD size: {x_test.shape[0]}\\n{y_test.value_counts()}\")\n",
    "print(\"Predictions:\")\n",
    "print(f\"{np.array(np.unique(y_pred, return_counts=True)).T}\")\n",
    "print(f\"\\tAccuracy: {acc}\\n\\tPrecision: {p}\\n\\tRecall: {r}\\n\\tF1-score: {f1}\\n\\tAUC score: {auc}\\n\" + \"-\"*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aecf4e853c2a06e9a3d98203b0bfcb89edde136ff484aed399b3da44301ece48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
