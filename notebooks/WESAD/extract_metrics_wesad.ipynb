{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING MODULES\n",
    "import glob\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.signal as ss\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..', '..', 'src'))\n",
    "sys.path.append(module_path)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tools.data_reader_wesad as dr\n",
    "import tools.display_tools as dt\n",
    "import tools.preprocessing as preprocessing\n",
    "\n",
    "from scipy.fft import fft, fftfreq, fftshift\n",
    "\n",
    "import cvxopt.solvers\n",
    "cvxopt.solvers.options['show_progress'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER METHODS\n",
    "import samplerate\n",
    "import scipy\n",
    "\n",
    "def get_time_segments(subject, sr_to_convert=0):\n",
    "    data = dr.get_participant_data(subject)\n",
    "    labels = data[\"label\"]\n",
    "    if sr_to_convert > 0:\n",
    "        step = int(700/sr_to_convert)\n",
    "        resampled = []\n",
    "        for i in range(0, labels.size, step):\n",
    "            resampled.append(labels[i])\n",
    "        labels = np.asarray(resampled)\n",
    "        # labels = samplerate.resample(labels, sr_to_convert/700.0)\n",
    "        # labels = scipy.signal.decimate(labels, int(700.0/sr_to_convert))\n",
    "    curr_value = labels[0]\n",
    "    start_indices = [0]\n",
    "    end_indices = []\n",
    "    i = 1\n",
    "    while i < len(labels):\n",
    "        if labels[i] != curr_value:\n",
    "            end_indices.append(i-1)\n",
    "            start_indices.append(i)\n",
    "            curr_value = labels[i]\n",
    "        i += 1\n",
    "\n",
    "    end_indices.append(len(labels)-1)\n",
    "\n",
    "    time_segments = {}\n",
    "    for i in range(len(start_indices)):\n",
    "        label = labels[start_indices[i]]\n",
    "        if label not in time_segments.keys():\n",
    "            time_segments[label] = [(start_indices[i], end_indices[i])]\n",
    "        else:\n",
    "            time_segments[label].append((start_indices[i], end_indices[i]))\n",
    "    return time_segments\n",
    "\n",
    "\n",
    "# ts = get_time_segments(2)\n",
    "# print(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ECG'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m signal \u001b[39min\u001b[39;00m chest_signals:\n\u001b[0;32m     32\u001b[0m \u001b[39m# for signal in wrist_signals:\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[39mif\u001b[39;00m location \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwrist\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 34\u001b[0m         sr \u001b[39m=\u001b[39m dr\u001b[39m.\u001b[39;49mFS_DICT[\u001b[39m\"\u001b[39;49m\u001b[39mwrist\u001b[39;49m\u001b[39m\"\u001b[39;49m][signal]\n\u001b[0;32m     35\u001b[0m     time_segments \u001b[39m=\u001b[39m get_time_segments(subject, sr)\n\u001b[0;32m     36\u001b[0m     signal_data \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39msignal\u001b[39m\u001b[39m\"\u001b[39m][location][signal]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ECG'"
     ]
    }
   ],
   "source": [
    "# SAVE SIGNALS INTO INDIVIDUAL CSV FILES\n",
    "importlib.reload(dr)\n",
    "importlib.reload(dt)\n",
    "importlib.reload(preprocessing)\n",
    "\n",
    "\n",
    "subject_indices = list(range(2, 12)) + list(range(13, 18))\n",
    "subject_indices = [str(i) for i in subject_indices]\n",
    "\n",
    "locations = [dr.WESADKeys.CHEST]\n",
    "# locations = [dr.WESADKeys.WRIST]\n",
    "chest_signals = [\n",
    "    dr.Signals.ACC, dr.Signals.ECG, dr.Signals.EDA, dr.Signals.EMG, \"Resp\", \"Temp\"\n",
    "]\n",
    "wrist_signals = [\n",
    "    dr.Signals.ACC, dr.Signals.BVP, dr.Signals.EDA, dr.Signals.TEMP\n",
    "]\n",
    "phase_label_dict = {\n",
    "    1: dr.Phases.BASE,\n",
    "    2: dr.Phases.TSST,\n",
    "    3: dr.Phases.FUN,\n",
    "    4: [dr.Phases.MEDI_1, dr.Phases.MEDI_2]\n",
    "}\n",
    "\n",
    "phases = dr.Phases.PHASE_ORDER\n",
    "# location = \"wrist\"\n",
    "location = \"chest\"\n",
    "\n",
    "for subject in subject_indices:\n",
    "    data = dr.get_participant_data(subject)\n",
    "    \n",
    "    for signal in chest_signals:\n",
    "    # for signal in wrist_signals:\n",
    "        if location == \"wrist\":\n",
    "            sr = dr.FS_DICT[\"wrist\"][signal]\n",
    "        time_segments = get_time_segments(subject, sr)\n",
    "        signal_data = data[\"signal\"][location][signal]\n",
    "        for label in list(phase_label_dict.keys()):\n",
    "            if signal == \"Temp\" or signal == \"Resp\":\n",
    "                signal = signal.upper()\n",
    "            phase = phase_label_dict[label]\n",
    "            if label == 4:  # 4 corresponds to MEDI 1 and MEDI 2; includes 2 time segment tuples\n",
    "                if signal == dr.Signals.ACC:\n",
    "                    columns_1 = [f\"{location}_{signal}_X_{phase[0]}\", f\"{location}_{signal}_Y_{phase[0]}\", f\"{location}_{signal}_Z_{phase[0]}\"]\n",
    "                else: \n",
    "                    columns_1 = [f\"{location}_{signal}_{phase[0]}\"]\n",
    "                file_name_1 = f\"S{subject}_{location}_{signal}_{phase[0]}.csv\"\n",
    "                file_name_1 = os.path.join(dr.Paths.WESAD, f\"S{subject}\", file_name_1)\n",
    "                start = time_segments[label][0][0]\n",
    "                end = time_segments[label][0][1]\n",
    "                phase_data = signal_data[start:end]\n",
    "                phase_data = pd.DataFrame(data=phase_data, columns=columns_1)\n",
    "                phase_data.to_csv(file_name_1)\n",
    "\n",
    "                if signal == dr.Signals.ACC:\n",
    "                    columns_2 = [f\"{location}_{signal}_X_{phase[1]}\", f\"{location}_{signal}_Y_{phase[1]}\", f\"{location}_{signal}_Z_{phase[1]}\"]\n",
    "                else: \n",
    "                    columns_2 = [f\"{location}_{signal}_{phase[1]}\"]\n",
    "                file_name_2 = f\"S{subject}_{location}_{signal}_{phase[1]}.csv\"\n",
    "                file_name_2 = os.path.join(dr.Paths.WESAD, f\"S{subject}\", file_name_2)\n",
    "                start = time_segments[label][1][0]\n",
    "                end = time_segments[label][1][1]\n",
    "                phase_data = signal_data[start:end]\n",
    "                phase_data = pd.DataFrame(data=phase_data, columns=columns_2)\n",
    "                phase_data.to_csv(file_name_2)\n",
    "            else:\n",
    "                if signal == dr.Signals.ACC:\n",
    "                    columns = [f\"{location}_{signal}_X_{phase}\", f\"{location}_{signal}_Y_{phase}\", f\"{location}_{signal}_Z_{phase}\"]\n",
    "                else: \n",
    "                    columns = [f\"{location}_{signal}_{phase}\"]\n",
    "                file_name = f\"S{subject}_{location}_{signal}_{phase}.csv\"\n",
    "                file_name = os.path.join(dr.Paths.WESAD, f\"S{subject}\", file_name)\n",
    "                start = time_segments[label][0][0]\n",
    "                end = time_segments[label][0][1]\n",
    "                phase_data = signal_data[start:end]\n",
    "                phase_data = pd.DataFrame(data=phase_data, columns=columns)\n",
    "                phase_data.to_csv(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECG METRICS\n",
    "importlib.reload(dr)\n",
    "importlib.reload(dt)\n",
    "importlib.reload(preprocessing)\n",
    "\n",
    "import heartpy as hp\n",
    "\n",
    "phases = dr.Phases.PHASE_ORDER\n",
    "metrics = [\"bpm\", \"rmssd\", \"ibi\"]\n",
    "subject_indices = [3]\n",
    "subject_indices = [str(i) for i in subject_indices]\n",
    "fs = 700.0\n",
    "\n",
    "for phase in phases:\n",
    "    data_files = {\n",
    "        s: dr.Paths.WESAD + f\"/S{s}/S{s}_chest_ECG_{phase}.csv\" for s in subject_indices\n",
    "    }\n",
    "    df_dict = {s: pd.read_csv(data_files[s]) for s in list(data_files.keys())}\n",
    "    metrics_dict = {\n",
    "        \"bpm\": [],\n",
    "        \"rmssd\": [],\n",
    "        \"ibi\": []\n",
    "    }\n",
    "    for s in list(df_dict.keys()):\n",
    "        df = df_dict[s]\n",
    "        ecg_signal = df.iloc[:, -1]\n",
    "        ecg_signal = preprocessing.clean_ecg(ecg_signal).to_numpy().flatten()\n",
    "        ecg_signal = hp.enhance_ecg_peaks(ecg_signal, fs)\n",
    "        working_data, measures = hp.process_segmentwise(ecg_signal, fs, segment_width=55, segment_overlap=5/55)\n",
    "        print(np.isnan(measures[\"bpm\"][0]))\n",
    "        print(measures[\"bpm\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECG METRICS\n",
    "importlib.reload(dr)\n",
    "importlib.reload(dt)\n",
    "importlib.reload(preprocessing)\n",
    "\n",
    "import heartpy as hp\n",
    "\n",
    "phases = dr.Phases.PHASE_ORDER\n",
    "# metrics = [\"bpm\", \"rmssd\", \"ibi\"]\n",
    "metrics = [\"sdnn\", \"breathingrate\"]\n",
    "subject_indices = dr.SUBJECTS\n",
    "fs = 700.0\n",
    "\n",
    "for phase in phases:\n",
    "    data_files = {\n",
    "        s: dr.Paths.WESAD + f\"/S{s}/S{s}_chest_ECG_{phase}.csv\" for s in subject_indices\n",
    "    }\n",
    "    df_dict = {s: pd.read_csv(data_files[s]) for s in list(data_files.keys())}\n",
    "    metrics_dict = {\n",
    "        # \"bpm\": [],\n",
    "        # \"rmssd\": [],\n",
    "        # \"ibi\": [],\n",
    "        \"sdnn\": [],\n",
    "        \"breathingrate\": []\n",
    "    }\n",
    "    for s in list(df_dict.keys()):\n",
    "        df = df_dict[s]\n",
    "        ecg_signal = df.iloc[:, -1]\n",
    "        ecg_signal = preprocessing.clean_ecg(ecg_signal).to_numpy().flatten()\n",
    "        working_data, measures = hp.process_segmentwise(ecg_signal, fs, segment_width=55, segment_overlap=5/55)\n",
    "        threshold = 180\n",
    "        # if any(bpm > threshold for bpm in measures[\"bpm\"]):\n",
    "        #     print(f\"BPM for subject {s} contains values greater than {threshold}\") \n",
    "        #     valid_mean = np.mean([bpm for bpm in measures[\"bpm\"] if bpm < threshold])\n",
    "        #     for i in range(len(measures[\"bpm\"])):\n",
    "        #         if measures[\"bpm\"][i] > threshold:\n",
    "        #             measures[\"bpm\"][i] = valid_mean\n",
    "        for metric in list(metrics_dict.keys()):\n",
    "            if np.isnan(np.sum(measures[metric])):\n",
    "                mean = np.nanmean(measures[metric])\n",
    "                measures[metric] = np.nan_to_num(measures[metric], nan=mean).tolist()\n",
    "            measures[metric].insert(0, int(s))\n",
    "            metrics_dict[metric].append(pd.DataFrame(measures[metric]))\n",
    "    for metric in list(metrics_dict.keys()):\n",
    "        df = pd.concat(metrics_dict[metric], axis=1)\n",
    "        df = df.transpose()\n",
    "        metrics_dict[metric] = df\n",
    "    for metric in list(metrics_dict.keys()):\n",
    "        file_name = os.path.join(dr.Paths.METRICS, f\"{metric}_{phase}.csv\")\n",
    "        metrics_dict[metric].to_csv(file_name)\n",
    "        \n",
    "# for phase in phases:\n",
    "#     data_2 = dr.get_data_for_phase(subject, phase, location, dr.Signals.ECG)\n",
    "#     fs = dr.FS_DICT[location][modality]\n",
    "#     wd, m = hp.process(data_2, sample_rate=fs)\n",
    "#     hp.plotter(wd, m, title=f\"Subject {subject}, {phase}, {modality} {location}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\zhoux\\\\Desktop\\\\Projects\\\\anxiety\\\\data\\\\WESAD/S2/S2_chest_ECG_Base.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m phase \u001b[39min\u001b[39;00m phases:\n\u001b[0;32m     11\u001b[0m     data_files \u001b[39m=\u001b[39m {\n\u001b[0;32m     12\u001b[0m         s: dr\u001b[39m.\u001b[39mPaths\u001b[39m.\u001b[39mWESAD \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/S\u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m}\u001b[39;00m\u001b[39m/S\u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m}\u001b[39;00m\u001b[39m_chest_ECG_\u001b[39m\u001b[39m{\u001b[39;00mphase\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m subject_indices\n\u001b[0;32m     13\u001b[0m     }\n\u001b[1;32m---> 14\u001b[0m     df_dict \u001b[39m=\u001b[39m {s: pd\u001b[39m.\u001b[39mread_csv(data_files[s]) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(data_files\u001b[39m.\u001b[39mkeys())}\n\u001b[0;32m     15\u001b[0m     metrics_dict \u001b[39m=\u001b[39m {\n\u001b[0;32m     16\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlf_rr\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[0;32m     17\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhf_rr\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[0;32m     18\u001b[0m     }\n\u001b[0;32m     19\u001b[0m     \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(df_dict\u001b[39m.\u001b[39mkeys()):\n",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m phase \u001b[39min\u001b[39;00m phases:\n\u001b[0;32m     11\u001b[0m     data_files \u001b[39m=\u001b[39m {\n\u001b[0;32m     12\u001b[0m         s: dr\u001b[39m.\u001b[39mPaths\u001b[39m.\u001b[39mWESAD \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/S\u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m}\u001b[39;00m\u001b[39m/S\u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m}\u001b[39;00m\u001b[39m_chest_ECG_\u001b[39m\u001b[39m{\u001b[39;00mphase\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m subject_indices\n\u001b[0;32m     13\u001b[0m     }\n\u001b[1;32m---> 14\u001b[0m     df_dict \u001b[39m=\u001b[39m {s: pd\u001b[39m.\u001b[39;49mread_csv(data_files[s]) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(data_files\u001b[39m.\u001b[39mkeys())}\n\u001b[0;32m     15\u001b[0m     metrics_dict \u001b[39m=\u001b[39m {\n\u001b[0;32m     16\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlf_rr\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[0;32m     17\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhf_rr\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[0;32m     18\u001b[0m     }\n\u001b[0;32m     19\u001b[0m     \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(df_dict\u001b[39m.\u001b[39mkeys()):\n",
      "File \u001b[1;32mc:\\Users\\zhoux\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zhoux\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zhoux\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\zhoux\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\zhoux\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\zhoux\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\zhoux\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\zhoux\\\\Desktop\\\\Projects\\\\anxiety\\\\data\\\\WESAD/S2/S2_chest_ECG_Base.csv'"
     ]
    }
   ],
   "source": [
    "# HF and LF metrics\n",
    "importlib.reload(dr)\n",
    "importlib.reload(dt)\n",
    "importlib.reload(preprocessing)\n",
    "\n",
    "phases = dr.Phases.PHASE_ORDER\n",
    "subject_indices = dr.SUBJECTS\n",
    "fs = 700.0\n",
    "\n",
    "for phase in phases:\n",
    "    data_files = {\n",
    "        s: dr.Paths.WESAD + f\"/S{s}/S{s}_chest_ECG_{phase}.csv\" for s in subject_indices\n",
    "    }\n",
    "    df_dict = {s: pd.read_csv(data_files[s]) for s in list(data_files.keys())}\n",
    "    metrics_dict = {\n",
    "        \"lf_rr\": [],\n",
    "        \"hf_rr\": [],\n",
    "    }\n",
    "    for s in list(df_dict.keys()):\n",
    "        df = df_dict[s]\n",
    "        ecg_signal = df.iloc[:, -1]\n",
    "        ecg_signal = preprocessing.clean_ecg(ecg_signal)\n",
    "        lf_rr = preprocessing.get_lf_rr(ecg_signal, fs)\n",
    "        lf_rr = np.insert(lf_rr, 0, s)\n",
    "        lf_rr = pd.DataFrame(lf_rr).dropna(axis=1)\n",
    "\n",
    "        hf_rr = preprocessing.get_hf_rr(ecg_signal, fs)\n",
    "        hf_rr = np.insert(hf_rr, 0, s)\n",
    "        hf_rr = pd.DataFrame(hf_rr).dropna(axis=1)\n",
    "\n",
    "        metrics_dict[\"lf_rr\"].append(lf_rr)\n",
    "        metrics_dict[\"hf_rr\"].append(hf_rr)\n",
    "    for metric in list(metrics_dict.keys()):\n",
    "        df = pd.concat(metrics_dict[metric], axis=1)\n",
    "        df = df.transpose()\n",
    "        metrics_dict[metric] = df\n",
    "    for metric in list(metrics_dict.keys()):\n",
    "        file_name = os.path.join(dr.Paths.METRICS, f\"{metric}_{phase}.csv\")\n",
    "        metrics_dict[metric].to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE EXTRACTION - EDA\n",
    "importlib.reload(dr)\n",
    "importlib.reload(dt)\n",
    "importlib.reload(preprocessing)\n",
    "\n",
    "\n",
    "phases = dr.Phases.PHASE_ORDER\n",
    "subject_indices = dr.SUBJECTS\n",
    "fs = 700.0\n",
    "\n",
    "for phase in phases:\n",
    "    data_files = {\n",
    "        s: dr.Paths.WESAD + f\"/S{s}/S{s}_chest_EDA_{phase}.csv\" for s in subject_indices\n",
    "    }\n",
    "    df_dict = {s: pd.read_csv(data_files[s]) for s in list(data_files.keys())}\n",
    "    metrics_dict = {\n",
    "        \"mean_SCL\": [],\n",
    "        \"SCR_rate\": [],\n",
    "    }\n",
    "    for s in list(df_dict.keys()):\n",
    "        df = df_dict[s]\n",
    "        eda_signal = df.iloc[:, -1]\n",
    "        mean_scl = preprocessing.get_mean_SCL(eda_signal, fs)\n",
    "        mean_scl = np.insert(mean_scl, 0, s)\n",
    "        mean_scl = pd.DataFrame(mean_scl).dropna(axis=1)\n",
    "\n",
    "        scr_rate = preprocessing.get_SCR_rate(eda_signal, fs)\n",
    "        scr_rate = np.insert(scr_rate, 0, s)\n",
    "        scr_rate = pd.DataFrame(scr_rate).dropna(axis=1)\n",
    "\n",
    "        metrics_dict[\"mean_SCL\"].append(mean_scl)\n",
    "        metrics_dict[\"SCR_rate\"].append(scr_rate)\n",
    "    for metric in list(metrics_dict.keys()):\n",
    "        df = pd.concat(metrics_dict[metric], axis=1)\n",
    "        df = df.transpose()\n",
    "        metrics_dict[metric] = df\n",
    "    for metric in list(metrics_dict.keys()):\n",
    "        file_name = os.path.join(dr.Paths.METRICS, f\"{metric}_{phase}.csv\")\n",
    "        metrics_dict[metric].to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE EXTRACTION - WRIST ACC\n",
    "importlib.reload(dr)\n",
    "importlib.reload(dt)\n",
    "importlib.reload(preprocessing)\n",
    "\n",
    "\n",
    "phases = dr.Phases.PHASE_ORDER\n",
    "subject_indices = dr.SUBJECTS\n",
    "\n",
    "for phase in phases:\n",
    "    data_files = {\n",
    "        s: dr.Paths.WESAD + f\"/S{s}/S{s}_wrist_ACC_{phase}.csv\" for s in subject_indices\n",
    "    }\n",
    "    df_dict = {s: pd.read_csv(data_files[s]) for s in list(data_files.keys())}\n",
    "    metrics_dict = {\n",
    "        \"peak_wrist_acc\": [],\n",
    "        \"mean_wrist_activity\": [],\n",
    "    }\n",
    "    for s in list(df_dict.keys()):\n",
    "        df = df_dict[s]\n",
    "        data = df.iloc[:, 1:]\n",
    "        peak_acc = preprocessing.get_peak_acc_value(data, acc_type=\"wrist\")\n",
    "        peak_acc = np.insert(peak_acc, 0, s)\n",
    "        peak_acc = pd.DataFrame(peak_acc).dropna(axis=1)\n",
    "\n",
    "        mean_act = preprocessing.get_mean_ankle_activity(data)\n",
    "        mean_act = np.insert(mean_act, 0, s)\n",
    "        mean_act = pd.DataFrame(mean_act).dropna(axis=1)\n",
    "\n",
    "        metrics_dict[\"peak_wrist_acc\"].append(peak_acc)\n",
    "        metrics_dict[\"mean_wrist_activity\"].append(mean_act)\n",
    "    for metric in list(metrics_dict.keys()):\n",
    "        df = pd.concat(metrics_dict[metric], axis=1)\n",
    "        df = df.transpose()\n",
    "        metrics_dict[metric] = df\n",
    "    for metric in list(metrics_dict.keys()):\n",
    "        file_name = os.path.join(dr.Paths.METRICS, f\"{metric}_{phase}.csv\")\n",
    "        metrics_dict[metric].to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE EXTRACTION - TORSO ACC\n",
    "importlib.reload(dr)\n",
    "importlib.reload(dt)\n",
    "importlib.reload(preprocessing)\n",
    "\n",
    "\n",
    "phases = dr.Phases.PHASE_ORDER\n",
    "subject_indices = dr.SUBJECTS\n",
    "fs = 700.0\n",
    "\n",
    "for phase in phases:\n",
    "    data_files = {\n",
    "        s: dr.Paths.WESAD + f\"/S{s}/S{s}_chest_ACC_{phase}.csv\" for s in subject_indices\n",
    "    }\n",
    "    df_dict = {s: pd.read_csv(data_files[s]) for s in list(data_files.keys())}\n",
    "    metrics_dict = {\n",
    "        \"mean_post\": [],\n",
    "        \"mean_post_act\": [],\n",
    "    }\n",
    "    for s in list(df_dict.keys()):\n",
    "        df = df_dict[s]\n",
    "        data = df.iloc[:, 1:]\n",
    "        mean_post = preprocessing.get_mean_posture(data)\n",
    "        mean_post = np.insert(mean_post, 0, s)\n",
    "        mean_post = pd.DataFrame(mean_post).dropna(axis=1)\n",
    "\n",
    "        mean_act = preprocessing.get_mean_activity_torso(data)\n",
    "        mean_act = np.insert(mean_act, 0, s)\n",
    "        mean_act = pd.DataFrame(mean_act).dropna(axis=1)\n",
    "\n",
    "        metrics_dict[\"mean_post\"].append(mean_post)\n",
    "        metrics_dict[\"mean_post_act\"].append(mean_act)\n",
    "    for metric in list(metrics_dict.keys()):\n",
    "        df = pd.concat(metrics_dict[metric], axis=1)\n",
    "        df = df.transpose()\n",
    "        metrics_dict[metric] = df\n",
    "    for metric in list(metrics_dict.keys()):\n",
    "        file_name = os.path.join(dr.Paths.METRICS, f\"{metric}_{phase}.csv\")\n",
    "        metrics_dict[metric].to_csv(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aecf4e853c2a06e9a3d98203b0bfcb89edde136ff484aed399b3da44301ece48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
