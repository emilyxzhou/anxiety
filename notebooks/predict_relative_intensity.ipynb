{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING MODULES\n",
    "import glob\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "cvx_path = os.path.abspath(os.path.join('..', 'cvxEDA', 'src'))\n",
    "module_path = os.path.abspath(os.path.join('..', 'src'))\n",
    "sys.path.append(module_path)\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy.signal as ss\n",
    "import sys\n",
    "\n",
    "import tools.data_reader_apd as dr\n",
    "import tools.display_tools as dt\n",
    "import tools.preprocessing as preprocessing\n",
    "\n",
    "from scipy.fft import fft, fftfreq, fftshift\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import cvxopt.solvers\n",
    "cvxopt.solvers.options['show_progress'] = False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \n",
    "    category=RuntimeWarning\n",
    ")\n",
    "\n",
    "\n",
    "metrics = [\n",
    "    # \"rmssd\", \"hf_rr\", \"lf_rr\", \"ibi\", \n",
    "    \"bpm\", \"rmssd\", \"hf_rr\", \"lf_rr\", \"ibi\", \n",
    "    \"mean_SCL\", \"SCR_rate\"\n",
    "]\n",
    "\n",
    "phases = {\n",
    "    \"Baseline\": [dr.Phases.BASE_REST, dr.Phases.BASE_SPEECH],\n",
    "    \"Bug baseline\": [dr.Phases.BUG_RELAX],\n",
    "    \"Speech baseline\": [dr.Phases.SPEECH_RELAX],\n",
    "    \"Bug all\": [dr.Phases.BUG_RELAX, dr.Phases.BUG_ANTICIPATE, dr.Phases.BUG_EXPOSURE, dr.Phases.BUG_BREAK, dr.Phases.BUG_REFLECT],\n",
    "    \"Speech all\": [dr.Phases.SPEECH_RELAX, dr.Phases.SPEECH_ANTICIPATE, dr.Phases.SPEECH_EXPOSURE, dr.Phases.SPEECH_BREAK, dr.Phases.SPEECH_REFLECT],\n",
    "    \"Bug pre-anxiety\": [dr.Phases.BUG_RELAX, dr.Phases.BUG_ANTICIPATE],\n",
    "    \"Speech pre-anxiety\": [dr.Phases.SPEECH_RELAX, dr.Phases.SPEECH_ANTICIPATE],\n",
    "    \"Bug anxiety\": [dr.Phases.BUG_EXPOSURE],\n",
    "    \"Speech anxiety\": [dr.Phases.SPEECH_EXPOSURE],\n",
    "    \"Bug post-anxiety\": [dr.Phases.BUG_BREAK, dr.Phases.BUG_REFLECT],\n",
    "    \"Speech post-anxiety\": [dr.Phases.SPEECH_BREAK, dr.Phases.SPEECH_REFLECT],\n",
    "}\n",
    "\n",
    "test_phases = [\n",
    "    phases[\"Baseline\"],\n",
    "    phases[\"Bug baseline\"],\n",
    "    phases[\"Speech baseline\"],\n",
    "    phases[\"Bug baseline\"] + phases[\"Speech baseline\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug baseline\"],\n",
    "    phases[\"Baseline\"] + phases[\"Speech baseline\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug baseline\"] + phases[\"Speech baseline\"],\n",
    "\n",
    "    phases[\"Bug all\"],\n",
    "    phases[\"Speech all\"],\n",
    "    phases[\"Bug all\"] + phases[\"Speech all\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug all\"],\n",
    "    phases[\"Baseline\"] + phases[\"Speech all\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug all\"] + phases[\"Speech all\"],\n",
    "\n",
    "    phases[\"Bug pre-anxiety\"],\n",
    "    phases[\"Speech pre-anxiety\"],\n",
    "    phases[\"Bug pre-anxiety\"] + phases[\"Speech pre-anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug pre-anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Speech pre-anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug pre-anxiety\"] + phases[\"Speech pre-anxiety\"],\n",
    "\n",
    "    phases[\"Bug anxiety\"],\n",
    "    phases[\"Speech anxiety\"],\n",
    "    phases[\"Bug pre-anxiety\"] + phases[\"Speech anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Speech anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug anxiety\"] + phases[\"Speech anxiety\"],\n",
    "\n",
    "    phases[\"Bug post-anxiety\"],\n",
    "    phases[\"Speech post-anxiety\"],\n",
    "    phases[\"Bug post-anxiety\"] + phases[\"Speech post-anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug post-anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Speech post-anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug post-anxiety\"] + phases[\"Speech post-anxiety\"],\n",
    "\n",
    "    phases[\"Bug pre-anxiety\"] + phases[\"Bug anxiety\"],\n",
    "    phases[\"Speech pre-anxiety\"] + phases[\"Speech anxiety\"],\n",
    "    phases[\"Bug pre-anxiety\"] + phases[\"Bug anxiety\"] + phases[\"Speech pre-anxiety\"] + phases[\"Speech anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug pre-anxiety\"] + phases[\"Bug anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Speech pre-anxiety\"] + phases[\"Speech anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug pre-anxiety\"] + phases[\"Bug anxiety\"] + phases[\"Speech pre-anxiety\"] + phases[\"Speech anxiety\"],\n",
    "\n",
    "    phases[\"Bug post-anxiety\"] + phases[\"Bug anxiety\"],\n",
    "    phases[\"Speech post-anxiety\"] + phases[\"Speech anxiety\"],\n",
    "    phases[\"Bug post-anxiety\"] + phases[\"Bug anxiety\"] + phases[\"Speech post-anxiety\"] + phases[\"Speech anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug post-anxiety\"] + phases[\"Bug anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Speech post-anxiety\"] + phases[\"Speech anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug post-anxiety\"] + phases[\"Bug anxiety\"] + phases[\"Speech post-anxiety\"] + phases[\"Speech anxiety\"],\n",
    "\n",
    "    phases[\"Bug pre-anxiety\"] + phases[\"Bug post-anxiety\"],\n",
    "    phases[\"Speech pre-anxiety\"] + phases[\"Speech post-anxiety\"],\n",
    "    phases[\"Bug pre-anxiety\"] + phases[\"Bug post-anxiety\"] + phases[\"Speech pre-anxiety\"] + phases[\"Speech post-anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug pre-anxiety\"] + phases[\"Bug post-anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Speech pre-anxiety\"] + phases[\"Speech post-anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug pre-anxiety\"] + phases[\"Bug post-anxiety\"] + phases[\"Speech pre-anxiety\"] + phases[\"Speech post-anxiety\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANKING PHASES BY HIGH TO LOW DISTRESS\n",
    "SUDS_labels = [\n",
    "    \"Participant\",\n",
    "    \"Baseline_SUDS\",\n",
    "    \"BugBox_Relax_SUDS\", \"BugBox_Preparation_SUDS\", \"BugBox_Exposure_SUDS\", \"BugBox_Break_SUDS\",\n",
    "    \"Speech_Relax_SUDS\", \"Speech_SUDS\", \"Speech_Exposure_SUDS\", \"Speech_Break_SUDS\"\n",
    "]\n",
    "\n",
    "\n",
    "ha_participant_indices = [\n",
    "    'P4', 'P6', 'P7', 'P8', 'P10', 'P12', 'P15', 'P16', 'P18', 'P22', 'P26', 'P27', 'P29', 'P31', 'P32', 'P33', 'P35', 'P42', 'P45', 'P47', 'P48', 'P49', 'P54', 'P55', 'P66', 'P69'\n",
    "]\n",
    "\n",
    "la_participant_indices = [\n",
    "    'P14', 'P21', 'P23', 'P25', 'P34', 'P39', 'P43', 'P46', 'P51', 'P57', 'P71', 'P72', 'P77', 'P78', 'P79', 'P80', 'P82', 'P83', 'P84', 'P85', 'P87', 'P88', 'P89', 'P91', 'P92', 'P93'\n",
    "]\n",
    "\n",
    "participant_file = os.path.join(dr.Paths.DATA_DIR, \"participants_details.csv\")\n",
    "df = pd.read_csv(participant_file)\n",
    "\n",
    "suds_df = df[SUDS_labels]\n",
    "ha_suds_df = suds_df.loc[suds_df['Participant'].isin(ha_participant_indices)]\n",
    "la_suds_df = suds_df.loc[suds_df['Participant'].isin(la_participant_indices)]\n",
    "\n",
    "ha_ranked = {}\n",
    "la_ranked = {}\n",
    "\n",
    "for i in range(ha_suds_df.shape[0]):\n",
    "    phases_ranked = []\n",
    "    for j in range(1, ha_suds_df.shape[1]):\n",
    "        phases_ranked.append((ha_suds_df.iloc[i, j], ha_suds_df.columns[j]))\n",
    "    ha_ranked[ha_suds_df.iloc[i, 0]] = phases_ranked\n",
    "\n",
    "for i in range(la_suds_df.shape[0]):\n",
    "    phases_ranked = []\n",
    "    for j in range(1, la_suds_df.shape[1]):\n",
    "        phases_ranked.append((la_suds_df.iloc[i, j], la_suds_df.columns[j]))\n",
    "    la_ranked[la_suds_df.iloc[i, 0]] = phases_ranked\n",
    "\n",
    "ha_labels = {}\n",
    "la_labels = {}\n",
    "\n",
    "for p in ha_ranked.keys():\n",
    "    suds = ha_ranked[p]\n",
    "    suds = suds.sort(key=lambda x:x[0])\n",
    "    ha_labels[p] = [SUDS_labels.index(phase[1]) for phase in ha_ranked[p]]\n",
    "\n",
    "for p in la_ranked.keys():\n",
    "    suds = la_ranked[p]\n",
    "    suds = suds.sort(key=lambda x:x[0])\n",
    "    la_labels[p] = [SUDS_labels.index(phase[1]) for phase in la_ranked[p]]\n",
    "\n",
    "# Phases ranked from low to high anxiety. Phase ID corresponds to phase index in SUDS_labels\n",
    "ha_labels = pd.DataFrame.from_dict(ha_labels, orient='index')\n",
    "la_labels = pd.DataFrame.from_dict(la_labels, orient='index')\n",
    "# ha_labels = np.vstack(ha_labels)\n",
    "# la_labels = np.vstack(la_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SUBJECTS = 52\n",
    "\n",
    "def get_apd_data_feature_fusion(metrics, phases):\n",
    "    \"\"\"\n",
    "    Combines features s.t. each feature vector represents each of the metrics for a single phase.\n",
    "    Return: x_train, y_train, x_test, y_test\n",
    "    \"\"\"\n",
    "    metrics_folder = os.path.join(dr.Paths.DATA_DIR, \"metrics\")\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    for phase in phases:\n",
    "        ha_features = []\n",
    "        la_features = []\n",
    "        for i in range(len(metrics)):\n",
    "            metric = metrics[1]\n",
    "            file = os.path.join(metrics_folder, f\"{metric}_{phase}_ha.csv\")\n",
    "            arr = pd.read_csv(file, index_col=[0]).to_numpy()\n",
    "\n",
    "            if i == 0:  # subject IDs\n",
    "                ha_features.append(arr[:, 0])\n",
    "\n",
    "            # arr = arr[1:, 1:]\n",
    "            col_mean = np.nanmean(arr, axis=1)\n",
    "            idx = np.where(np.isnan(arr))\n",
    "            arr[idx] = np.take(col_mean, idx[0])\n",
    "            arr = np.nan_to_num(arr)\n",
    "            print(arr)\n",
    "            # arr = normalize(arr)\n",
    "            arr = np.mean(arr[:, 1:], axis=1)\n",
    "            # arr = np.reshape(arr, (arr.size, 1))\n",
    "            print(arr)\n",
    "            ha_features.append(arr)\n",
    "\n",
    "            file = os.path.join(metrics_folder, f\"{metric}_{phase}_la.csv\")\n",
    "            arr = pd.read_csv(file, index_col=[0]).to_numpy()\n",
    "            # arr = arr[1:, 1:]/\n",
    "            col_mean = np.nanmean(arr, axis=1)\n",
    "            idx = np.where(np.isnan(arr))\n",
    "            arr[idx] = np.take(col_mean, idx[0])\n",
    "            arr = np.nan_to_num(arr)\n",
    "            # arr = normalize(arr)\n",
    "            arr = np.mean(arr, axis=1)\n",
    "            arr = np.reshape(arr, (arr.size, 1))\n",
    "            la_features.append(arr)\n",
    "\n",
    "        ha_features = np.hstack(ha_features)\n",
    "        la_features = np.hstack(la_features)\n",
    "        x = np.vstack([ha_features, la_features])\n",
    "        y = np.vstack([ha_labels, la_labels])\n",
    "        # print(f\"x: {x.shape}\")\n",
    "        # print(f\"y: {y.shape}\")\n",
    "\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "        # print(f\"x: {x.shape}\")\n",
    "        # print(f\"y: {y.shape}\")\n",
    "    \n",
    "    data_x = np.vstack(data_x)\n",
    "    data_y = np.vstack(data_y)\n",
    "\n",
    "    print(f\"data_x: {data_x.shape}\")\n",
    "    print(data_x)\n",
    "    print(f\"data_y: {data_y.shape}\")\n",
    "    test_size = 0.1\n",
    "    test_indices = random.sample(range(NUM_SUBJECTS), int(NUM_SUBJECTS*test_size))\n",
    "    # print(test_indices)\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for i in range(y.shape[0]):\n",
    "        if i%NUM_SUBJECTS in test_indices:\n",
    "            x_test.append(x[i, :])\n",
    "            y_test.append(y[i, :])\n",
    "        else:\n",
    "            x_train.append(x[i, :])\n",
    "            y_train.append(y[i, :])\n",
    "    \n",
    "    x_train = np.vstack(x_train)\n",
    "    y_train = np.asarray(y_train).flatten()\n",
    "    x_test = np.vstack(x_test)\n",
    "    y_test = np.asarray(y_test).flatten()\n",
    "    # print(f\"x_train: {x_train.shape}\")\n",
    "    # print(f\"y_train: {y_train.shape}\")\n",
    "    # print(f\"x_test: {x_test.shape}\")\n",
    "    # print(f\"y_test: {y_test.shape}\")\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def get_apd_data_ensemble(metrics, phases):\n",
    "    \"\"\"\n",
    "    Combines features s.t. each feature vector represents each of the phases for a single metric.\n",
    "    Return: x_trian, y_train, x_test, y_test\n",
    "    \"\"\"\n",
    "    metrics_folder = os.path.join(dr.Paths.DATA_DIR, \"metrics\")\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    for phase in phases:\n",
    "        ha_features = []\n",
    "        la_features = []\n",
    "        for metric in metrics:\n",
    "            file = os.path.join(metrics_folder, f\"{metric}_{phase}_ha.csv\")\n",
    "            arr = pd.read_csv(file, header=None, index_col=[0]).to_numpy()\n",
    "            arr = arr[1:, 1:]\n",
    "            col_mean = np.nanmean(arr, axis=1)\n",
    "            idx = np.where(np.isnan(arr))\n",
    "            arr[idx] = np.take(col_mean, idx[0])\n",
    "            arr = np.nan_to_num(arr)\n",
    "            # arr = normalize(arr)\n",
    "            arr = np.mean(arr, axis=1)\n",
    "            arr = np.reshape(arr, (arr.size, 1))\n",
    "            ha_features.append(arr)\n",
    "\n",
    "            file = os.path.join(metrics_folder, f\"{metric}_{phase}_la.csv\")\n",
    "            arr = pd.read_csv(file, header=None, index_col=[0]).to_numpy()\n",
    "            arr = arr[1:, 1:]\n",
    "            col_mean = np.nanmean(arr, axis=1)\n",
    "            idx = np.where(np.isnan(arr))\n",
    "            arr[idx] = np.take(col_mean, idx[0])\n",
    "            arr = np.nan_to_num(arr)\n",
    "            # arr = normalize(arr)\n",
    "            arr = np.mean(arr, axis=1)\n",
    "            arr = np.reshape(arr, (arr.size, 1))\n",
    "            la_features.append(arr)\n",
    "\n",
    "        ha_features = np.hstack(ha_features)\n",
    "        la_features = np.hstack(la_features)\n",
    "        x = np.vstack([ha_features, la_features])\n",
    "        y = np.asarray([ha_labels, la_labels])\n",
    "        # print(x.shape)\n",
    "        # print(y.shape)\n",
    "\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "        # print(f\"x: {x.shape}\")\n",
    "        # print(f\"y: {y.shape}\")\n",
    "    \n",
    "    data_x = np.vstack(data_x)\n",
    "    data_y = np.vstack(data_y)\n",
    "\n",
    "    # print(f\"data_x: {data_x.shape}\")\n",
    "    # print(f\"data_y: {data_y.shape}\")\n",
    "    test_size = 0.1\n",
    "    test_indices = random.sample(range(NUM_SUBJECTS), int(NUM_SUBJECTS*test_size))\n",
    "    # print(test_indices)\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for i in range(y.size):\n",
    "        if i%NUM_SUBJECTS in test_indices:\n",
    "            x_test.append(x[i, :])\n",
    "            y_test.append(y[i, :])\n",
    "        else:\n",
    "            x_train.append(x[i, :])\n",
    "            y_train.append(y[i, :])\n",
    "    \n",
    "    x_train = np.asarray(x_train)\n",
    "    y_train = np.asarray(y_train).flatten()\n",
    "    x_test = np.asarray(x_test)\n",
    "    y_test = np.asarray(y_test).flatten()\n",
    "    # print(f\"x_train: {x_train.shape}\")\n",
    "    # print(f\"y_train: {y_train.shape}\")\n",
    "    # print(f\"x_test: {x_test.shape}\")\n",
    "    # print(f\"y_test: {y_test.shape}\")\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 14.         153.78548896 118.65342163 144.96439471]\n",
      " [ 21.          92.31795937  87.45387454  88.5668277 ]\n",
      " [ 23.          89.17431193  88.66024518  88.17046289]\n",
      " [ 25.          94.19600381  90.33203125  90.47954157]\n",
      " [ 34.          56.85196195  56.46173149  56.9550931 ]\n",
      " [ 39.         158.54779412 147.65489288  79.25275969]\n",
      " [ 43.         126.56744404 128.79484821 129.60712839]\n",
      " [ 46.          89.32163837  91.50012039  92.03159752]\n",
      " [ 51.          76.29367536  75.39917209  77.72596047]\n",
      " [ 57.         133.78803778 157.67634855 173.85444744]\n",
      " [ 71.         145.50822438 179.60602549 129.00703675]\n",
      " [ 72.         108.43373494  71.68801376 211.26760563]\n",
      " [ 77.         104.23080306 105.20670022  98.78760665]\n",
      " [ 78.          88.22163752  95.19705398  88.03235784]\n",
      " [ 79.          66.9111816   69.17868708  79.91803279]\n",
      " [ 80.         144.78764479 157.3976915  152.80135823]\n",
      " [ 82.         151.59574468 116.49839121 101.1040093 ]\n",
      " [ 83.         191.90717048 123.00388433 221.34776193]\n",
      " [ 84.         500.         468.75       476.19047619]\n",
      " [ 85.          79.83719474  81.18081181  82.11909374]\n",
      " [ 87.          86.45063071 106.55090766 112.92962357]\n",
      " [ 88.         104.94145372 102.04081633 106.47639956]\n",
      " [ 89.          75.57864903  85.24194937  79.62763502]\n",
      " [ 91.          96.26023457  81.85538881 199.11504425]\n",
      " [ 92.         106.00706714 105.07299808 105.58684055]\n",
      " [ 93.          98.8262022   99.59686981  99.85120213]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (26,) into shape (26,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [69], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mxgboost\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mxgb\u001b[39;00m\n\u001b[0;32m      3\u001b[0m model_phases \u001b[39m=\u001b[39m phases[\u001b[39m\"\u001b[39m\u001b[39mBaseline\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m x_train, y_train, x_test, y_test \u001b[39m=\u001b[39m get_apd_data_feature_fusion(metrics, model_phases)\n\u001b[0;32m      6\u001b[0m groups \u001b[39m=\u001b[39m [x_train\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]]\n\u001b[0;32m      8\u001b[0m model \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBRanker(  \n\u001b[0;32m      9\u001b[0m     \u001b[39m# tree_method='gpu_hist',\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     booster\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgbtree\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     subsample\u001b[39m=\u001b[39m\u001b[39m0.75\u001b[39m \n\u001b[0;32m     19\u001b[0m )\n",
      "Cell \u001b[1;32mIn [68], line 24\u001b[0m, in \u001b[0;36mget_apd_data_feature_fusion\u001b[1;34m(metrics, phases)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mprint\u001b[39m(arr)\n\u001b[0;32m     23\u001b[0m \u001b[39m# arr = normalize(arr)\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m arr[:, \u001b[39m1\u001b[39;49m:] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(arr, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[39m# arr = np.reshape(arr, (arr.size, 1))\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39mprint\u001b[39m(arr)\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (26,) into shape (26,3)"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model_phases = phases[\"Baseline\"]\n",
    "\n",
    "x_train, y_train, x_test, y_test = get_apd_data_feature_fusion(metrics, model_phases)\n",
    "groups = [x_train.shape[0]]\n",
    "\n",
    "model = xgb.XGBRanker(  \n",
    "    # tree_method='gpu_hist',\n",
    "    booster='gbtree',\n",
    "    objective='rank:map',\n",
    "    random_state=42, \n",
    "    learning_rate=0.1,\n",
    "    colsample_bytree=0.9, \n",
    "    eta=0.05, \n",
    "    max_depth=6, \n",
    "    n_estimators=110, \n",
    "    subsample=0.75 \n",
    ")\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, group=groups, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0877744b127b3e06bbb46a4d58ff9f96d2144cab151d4d0cf00260a00e80ed7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
