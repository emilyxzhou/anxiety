{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING MODULES\n",
    "import glob\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "cvx_path = os.path.abspath(os.path.join('..', 'cvxEDA', 'src'))\n",
    "module_path = os.path.abspath(os.path.join('..', 'src'))\n",
    "sys.path.append(module_path)\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy.signal as ss\n",
    "import sys\n",
    "\n",
    "import tools.data_reader_apd as dr\n",
    "import tools.display_tools as dt\n",
    "import tools.preprocessing as preprocessing\n",
    "\n",
    "from scipy.fft import fft, fftfreq, fftshift\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import cvxopt.solvers\n",
    "cvxopt.solvers.options['show_progress'] = False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \n",
    "    category=RuntimeWarning\n",
    ")\n",
    "\n",
    "\n",
    "metrics = [\n",
    "    # \"rmssd\", \"hf_rr\", \"lf_rr\", \"ibi\", \n",
    "    \"bpm\", \"rmssd\", \"hf_rr\", \"lf_rr\", \"ibi\", \n",
    "    \"mean_SCL\", \"SCR_rate\"\n",
    "]\n",
    "\n",
    "phases = {\n",
    "    \"Baseline\": [dr.Phases.BASE_REST, dr.Phases.BASE_SPEECH],\n",
    "    \"Bug baseline\": [dr.Phases.BUG_RELAX],\n",
    "    \"Speech baseline\": [dr.Phases.SPEECH_RELAX],\n",
    "    \"Bug all\": [dr.Phases.BUG_RELAX, dr.Phases.BUG_ANTICIPATE, dr.Phases.BUG_EXPOSURE, dr.Phases.BUG_BREAK, dr.Phases.BUG_REFLECT],\n",
    "    \"Speech all\": [dr.Phases.SPEECH_RELAX, dr.Phases.SPEECH_ANTICIPATE, dr.Phases.SPEECH_EXPOSURE, dr.Phases.SPEECH_BREAK, dr.Phases.SPEECH_REFLECT],\n",
    "    \"Bug pre-anxiety\": [dr.Phases.BUG_RELAX, dr.Phases.BUG_ANTICIPATE],\n",
    "    \"Speech pre-anxiety\": [dr.Phases.SPEECH_RELAX, dr.Phases.SPEECH_ANTICIPATE],\n",
    "    \"Bug anxiety\": [dr.Phases.BUG_EXPOSURE],\n",
    "    \"Speech anxiety\": [dr.Phases.SPEECH_EXPOSURE],\n",
    "    \"Bug post-anxiety\": [dr.Phases.BUG_BREAK, dr.Phases.BUG_REFLECT],\n",
    "    \"Speech post-anxiety\": [dr.Phases.SPEECH_BREAK, dr.Phases.SPEECH_REFLECT],\n",
    "}\n",
    "\n",
    "test_phases = [\n",
    "    phases[\"Baseline\"],\n",
    "    phases[\"Bug baseline\"],\n",
    "    phases[\"Speech baseline\"],\n",
    "    phases[\"Bug baseline\"] + phases[\"Speech baseline\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug baseline\"],\n",
    "    phases[\"Baseline\"] + phases[\"Speech baseline\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug baseline\"] + phases[\"Speech baseline\"],\n",
    "\n",
    "    phases[\"Bug all\"],\n",
    "    phases[\"Speech all\"],\n",
    "    phases[\"Bug all\"] + phases[\"Speech all\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug all\"],\n",
    "    phases[\"Baseline\"] + phases[\"Speech all\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug all\"] + phases[\"Speech all\"],\n",
    "\n",
    "    phases[\"Bug pre-anxiety\"],\n",
    "    phases[\"Speech pre-anxiety\"],\n",
    "    phases[\"Bug pre-anxiety\"] + phases[\"Speech pre-anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug pre-anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Speech pre-anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug pre-anxiety\"] + phases[\"Speech pre-anxiety\"],\n",
    "\n",
    "    phases[\"Bug anxiety\"],\n",
    "    phases[\"Speech anxiety\"],\n",
    "    phases[\"Bug pre-anxiety\"] + phases[\"Speech anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Speech anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug anxiety\"] + phases[\"Speech anxiety\"],\n",
    "\n",
    "    phases[\"Bug post-anxiety\"],\n",
    "    phases[\"Speech post-anxiety\"],\n",
    "    phases[\"Bug post-anxiety\"] + phases[\"Speech post-anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug post-anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Speech post-anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug post-anxiety\"] + phases[\"Speech post-anxiety\"],\n",
    "\n",
    "    phases[\"Bug pre-anxiety\"] + phases[\"Bug anxiety\"],\n",
    "    phases[\"Speech pre-anxiety\"] + phases[\"Speech anxiety\"],\n",
    "    phases[\"Bug pre-anxiety\"] + phases[\"Bug anxiety\"] + phases[\"Speech pre-anxiety\"] + phases[\"Speech anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug pre-anxiety\"] + phases[\"Bug anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Speech pre-anxiety\"] + phases[\"Speech anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug pre-anxiety\"] + phases[\"Bug anxiety\"] + phases[\"Speech pre-anxiety\"] + phases[\"Speech anxiety\"],\n",
    "\n",
    "    phases[\"Bug post-anxiety\"] + phases[\"Bug anxiety\"],\n",
    "    phases[\"Speech post-anxiety\"] + phases[\"Speech anxiety\"],\n",
    "    phases[\"Bug post-anxiety\"] + phases[\"Bug anxiety\"] + phases[\"Speech post-anxiety\"] + phases[\"Speech anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug post-anxiety\"] + phases[\"Bug anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Speech post-anxiety\"] + phases[\"Speech anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug post-anxiety\"] + phases[\"Bug anxiety\"] + phases[\"Speech post-anxiety\"] + phases[\"Speech anxiety\"],\n",
    "\n",
    "    phases[\"Bug pre-anxiety\"] + phases[\"Bug post-anxiety\"],\n",
    "    phases[\"Speech pre-anxiety\"] + phases[\"Speech post-anxiety\"],\n",
    "    phases[\"Bug pre-anxiety\"] + phases[\"Bug post-anxiety\"] + phases[\"Speech pre-anxiety\"] + phases[\"Speech post-anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug pre-anxiety\"] + phases[\"Bug post-anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Speech pre-anxiety\"] + phases[\"Speech post-anxiety\"],\n",
    "    phases[\"Baseline\"] + phases[\"Bug pre-anxiety\"] + phases[\"Bug post-anxiety\"] + phases[\"Speech pre-anxiety\"] + phases[\"Speech post-anxiety\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANKING PHASES BY HIGH TO LOW DISTRESS\n",
    "SUDS_labels = [\n",
    "    \"Participant\",\n",
    "    \"Baseline_SUDS\",\n",
    "    \"BugBox_Relax_SUDS\", \"BugBox_Preparation_SUDS\", \"BugBox_Exposure_SUDS\", \"BugBox_Break_SUDS\",\n",
    "    \"Speech_Relax_SUDS\", \"Speech_SUDS\", \"Speech_Exposure_SUDS\", \"Speech_Break_SUDS\"\n",
    "]\n",
    "\n",
    "\n",
    "ha_participant_indices = [\n",
    "    'P4', 'P6', 'P7', 'P8', 'P10', 'P12', 'P15', 'P16', 'P18', 'P22', 'P26', 'P27', 'P29', 'P31', 'P32', 'P33', 'P35', 'P42', 'P45', 'P47', 'P48', 'P49', 'P54', 'P55', 'P66', 'P69'\n",
    "]\n",
    "\n",
    "la_participant_indices = [\n",
    "    'P14', 'P21', 'P23', 'P25', 'P34', 'P39', 'P43', 'P46', 'P51', 'P57', 'P71', 'P72', 'P77', 'P78', 'P79', 'P80', 'P82', 'P83', 'P84', 'P85', 'P87', 'P88', 'P89', 'P91', 'P92', 'P93'\n",
    "]\n",
    "\n",
    "participant_file = os.path.join(dr.Paths.DATA_DIR, \"participants_details.csv\")\n",
    "df = pd.read_csv(participant_file)\n",
    "\n",
    "suds_df = df[SUDS_labels]\n",
    "ha_suds_df = suds_df.loc[suds_df['Participant'].isin(ha_participant_indices)]\n",
    "la_suds_df = suds_df.loc[suds_df['Participant'].isin(la_participant_indices)]\n",
    "\n",
    "ha_ranked = {}\n",
    "la_ranked = {}\n",
    "\n",
    "for i in range(ha_suds_df.shape[0]):\n",
    "    phases_ranked = []\n",
    "    for j in range(1, ha_suds_df.shape[1]):\n",
    "        phases_ranked.append((ha_suds_df.iloc[i, j], ha_suds_df.columns[j]))\n",
    "    ha_ranked[ha_suds_df.iloc[i, 0]] = phases_ranked\n",
    "\n",
    "for i in range(la_suds_df.shape[0]):\n",
    "    phases_ranked = []\n",
    "    for j in range(1, la_suds_df.shape[1]):\n",
    "        phases_ranked.append((la_suds_df.iloc[i, j], la_suds_df.columns[j]))\n",
    "    la_ranked[la_suds_df.iloc[i, 0]] = phases_ranked\n",
    "\n",
    "ha_labels = {}\n",
    "la_labels = {}\n",
    "\n",
    "for p in ha_ranked.keys():\n",
    "    suds = ha_ranked[p]\n",
    "    suds = suds.sort(key=lambda x:x[0])\n",
    "    ha_labels[p] = [SUDS_labels.index(phase[1]) for phase in ha_ranked[p]]\n",
    "\n",
    "for p in la_ranked.keys():\n",
    "    suds = la_ranked[p]\n",
    "    suds = suds.sort(key=lambda x:x[0])\n",
    "    la_labels[p] = [SUDS_labels.index(phase[1]) for phase in la_ranked[p]]\n",
    "\n",
    "# Phases ranked from low to high anxiety. Phase ID corresponds to phase index in SUDS_labels\n",
    "ha_labels = pd.DataFrame.from_dict(ha_labels, orient='index')\n",
    "la_labels = pd.DataFrame.from_dict(la_labels, orient='index')\n",
    "# ha_labels = np.vstack(ha_labels)\n",
    "# la_labels = np.vstack(la_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SUBJECTS = 52\n",
    "\n",
    "def get_apd_data_feature_fusion(metrics, phases):\n",
    "    \"\"\"\n",
    "    Combines features s.t. each feature vector represents each of the metrics for a single phase.\n",
    "    If there are 2 phases, then the first half of the rows correspond to the first phase, and the second half to the second phase.\n",
    "    Number of columns = subject column + number of metrics\n",
    "    Return: feature DataFrame, label DataFrame\n",
    "    \"\"\"\n",
    "    metrics_folder = os.path.join(dr.Paths.DATA_DIR, \"metrics\")\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    for phase in phases:\n",
    "        print(f\"Generating features for phase {phase} \" + \"-\"*30)\n",
    "        ha_features = []\n",
    "        la_features = []\n",
    "        for i in range(len(metrics)):\n",
    "            metric = metrics[i]\n",
    "            print(f\"Generating features for metric {metric}\")\n",
    "            file = os.path.join(metrics_folder, f\"{metric}_{phase}_ha.csv\")\n",
    "            arr = pd.read_csv(file, index_col=[0]).to_numpy()\n",
    "\n",
    "            if i == 0:  # subject IDs\n",
    "                ids = np.reshape(arr[:, 0], (arr[:, 0].size, 1))\n",
    "                ha_features.append(ids)\n",
    "\n",
    "            # arr = arr[1:, 1:]\n",
    "            col_mean = np.nanmean(arr, axis=1)\n",
    "            idx = np.where(np.isnan(arr))\n",
    "            arr[idx] = np.take(col_mean, idx[0])\n",
    "            arr = np.nan_to_num(arr)\n",
    "            arr = np.mean(arr[:, 1:], axis=1)\n",
    "            arr = np.reshape(arr, (arr.size, 1))\n",
    "            # print(arr)\n",
    "            # arr = normalize(arr)\n",
    "            # arr = np.reshape(arr, (arr.size, 1))\n",
    "            ha_features.append(arr)\n",
    "\n",
    "            file = os.path.join(metrics_folder, f\"{metric}_{phase}_la.csv\")\n",
    "            arr = pd.read_csv(file, index_col=[0]).to_numpy()\n",
    "\n",
    "            if i == 0:  # subject IDs\n",
    "                ids = np.reshape(arr[:, 0], (arr[:, 0].size, 1))\n",
    "                la_features.append(ids)\n",
    "\n",
    "            # arr = arr[1:, 1:]\n",
    "            col_mean = np.nanmean(arr, axis=1)\n",
    "            idx = np.where(np.isnan(arr))\n",
    "            arr[idx] = np.take(col_mean, idx[0])\n",
    "            arr = np.nan_to_num(arr)\n",
    "            arr = np.mean(arr[:, 1:], axis=1)\n",
    "            arr = np.reshape(arr, (arr.size, 1))\n",
    "            # print(arr)\n",
    "            # arr = normalize(arr)\n",
    "            # arr = np.reshape(arr, (arr.size, 1))\n",
    "            la_features.append(arr)\n",
    "\n",
    "        ha_features = np.hstack(ha_features)  # horizontally concatenate metrics for one phase\n",
    "        la_features = np.hstack(la_features)  # horizontally concatenate metrics for one phase\n",
    "        x = np.vstack([ha_features, la_features])  # vertically concatenate metrics for one phase for HA and LA\n",
    "        # print(f\"x: {x.shape}\")\n",
    "        # print(f\"y: {y.shape}\")\n",
    "\n",
    "        data_x.append(x)\n",
    "        # print(f\"x: {x.shape}\")\n",
    "        # print(f\"y: {y.shape}\")\n",
    "    \n",
    "    # phases concatenated vertically\n",
    "    data_x = np.vstack(data_x)\n",
    "    data_y = np.vstack([ha_labels, la_labels])\n",
    "    # print(f\"data_x: {data_x.shape}\")\n",
    "    # print(f\"data_y: {data_y.shape}\")\n",
    "\n",
    "    return data_x, data_y\n",
    "\n",
    "\n",
    "def format_input_for_ranking(x, y, metrics):\n",
    "    num_subjects = y.shape[0]\n",
    "    num_phases = x.shape[0] // num_subjects\n",
    "    num_rankings = y.shape[1]\n",
    "    x = np.repeat(x, repeats=num_rankings, axis=0)\n",
    "    subject_col = x[:, 0]\n",
    "    phases = np.reshape(y, (y.size, 1))\n",
    "    phases = np.vstack([phases for _ in range(num_phases)]).flatten()\n",
    "    rankings = np.asarray(list(range(1, 10))*num_subjects*num_phases)\n",
    "    x = np.insert(x, 1, phases, axis=1)\n",
    "\n",
    "    # print(x.shape)\n",
    "    # print(y.shape)\n",
    "    columns = metrics.copy()\n",
    "    columns.insert(0, \"subject\")\n",
    "    columns.insert(1, \"phaseId\")\n",
    "    x = pd.DataFrame(data=x, columns=columns)\n",
    "    y = pd.DataFrame({\"subject\": subject_col, \"ranking\": rankings})\n",
    "    # print(x.head())\n",
    "    # print(y.head())\n",
    "    \n",
    "    # train test split\n",
    "    test_size = 0.1\n",
    "    subjects = x.loc[:, \"subject\"].unique().tolist()\n",
    "    test_subjects = random.sample(subjects, int(NUM_SUBJECTS*test_size))\n",
    "    test = x.index[x[\"subject\"].isin(test_subjects)].tolist()\n",
    "    train = x.index[~x[\"subject\"].isin(test_subjects)].tolist()\n",
    "\n",
    "    print(f\"test subjects: {test_subjects}\")\n",
    "\n",
    "    x_train = x.iloc[train, :].reset_index(drop=True)\n",
    "    y_train = y.iloc[train, :].reset_index(drop=True)\n",
    "    x_test = x.iloc[test, :].reset_index(drop=True)\n",
    "    y_test = y.iloc[test, :].reset_index(drop=True)\n",
    "    # print(f\"x_train: {x_train.shape}\")\n",
    "    # print(f\"y_train: {y_train.shape}\")\n",
    "    # print(f\"x_test: {x_test.shape}\")\n",
    "    # print(f\"y_test: {y_test.shape}\")\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, test_subjects\n",
    "\n",
    "\n",
    "def get_apd_data_ensemble(metrics, phases):\n",
    "    \"\"\"\n",
    "    Combines features s.t. each feature vector represents each of the phases for a single metric.\n",
    "    Return: x_trian, y_train, x_test, y_test\n",
    "    \"\"\"\n",
    "    metrics_folder = os.path.join(dr.Paths.DATA_DIR, \"metrics\")\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    for phase in phases:\n",
    "        ha_features = []\n",
    "        la_features = []\n",
    "        for metric in metrics:\n",
    "            file = os.path.join(metrics_folder, f\"{metric}_{phase}_ha.csv\")\n",
    "            arr = pd.read_csv(file, header=None, index_col=[0]).to_numpy()\n",
    "            arr = arr[1:, 1:]\n",
    "            col_mean = np.nanmean(arr, axis=1)\n",
    "            idx = np.where(np.isnan(arr))\n",
    "            arr[idx] = np.take(col_mean, idx[0])\n",
    "            arr = np.nan_to_num(arr)\n",
    "            # arr = normalize(arr)\n",
    "            arr = np.mean(arr, axis=1)\n",
    "            arr = np.reshape(arr, (arr.size, 1))\n",
    "            ha_features.append(arr)\n",
    "\n",
    "            file = os.path.join(metrics_folder, f\"{metric}_{phase}_la.csv\")\n",
    "            arr = pd.read_csv(file, header=None, index_col=[0]).to_numpy()\n",
    "            arr = arr[1:, 1:]\n",
    "            col_mean = np.nanmean(arr, axis=1)\n",
    "            idx = np.where(np.isnan(arr))\n",
    "            arr[idx] = np.take(col_mean, idx[0])\n",
    "            arr = np.nan_to_num(arr)\n",
    "            # arr = normalize(arr)\n",
    "            arr = np.mean(arr, axis=1)\n",
    "            arr = np.reshape(arr, (arr.size, 1))\n",
    "            la_features.append(arr)\n",
    "\n",
    "        ha_features = np.hstack(ha_features)\n",
    "        la_features = np.hstack(la_features)\n",
    "        x = np.vstack([ha_features, la_features])\n",
    "        y = np.asarray([ha_labels, la_labels])\n",
    "        # print(x.shape)\n",
    "        # print(y.shape)\n",
    "\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "        # print(f\"x: {x.shape}\")\n",
    "        # print(f\"y: {y.shape}\")\n",
    "    \n",
    "    data_x = np.vstack(data_x)\n",
    "    data_y = np.vstack(data_y)\n",
    "\n",
    "    # print(f\"data_x: {data_x.shape}\")\n",
    "    # print(f\"data_y: {data_y.shape}\")\n",
    "    test_size = 0.1\n",
    "    test_indices = random.sample(range(NUM_SUBJECTS), int(NUM_SUBJECTS*test_size))\n",
    "    # print(test_indices)\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for i in range(y.size):\n",
    "        if i%NUM_SUBJECTS in test_indices:\n",
    "            x_test.append(x[i, :])\n",
    "            y_test.append(y[i, :])\n",
    "        else:\n",
    "            x_train.append(x[i, :])\n",
    "            y_train.append(y[i, :])\n",
    "    \n",
    "    x_train = np.asarray(x_train)\n",
    "    y_train = np.asarray(y_train).flatten()\n",
    "    x_test = np.asarray(x_test)\n",
    "    y_test = np.asarray(y_test).flatten()\n",
    "    # print(f\"x_train: {x_train.shape}\")\n",
    "    # print(f\"y_train: {y_train.shape}\")\n",
    "    # print(f\"x_test: {x_test.shape}\")\n",
    "    # print(f\"y_test: {y_test.shape}\")\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating features for phase Baseline_Rest ------------------------------\n",
      "Generating features for metric bpm\n",
      "Generating features for metric rmssd\n",
      "Generating features for metric hf_rr\n",
      "Generating features for metric lf_rr\n",
      "Generating features for metric ibi\n",
      "Generating features for metric mean_SCL\n",
      "Generating features for metric SCR_rate\n",
      "test subjects: [22.0, 35.0, 18.0, 85.0, 88.0]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model_phases = phases[\"Baseline\"][0]\n",
    "if type(model_phases) != list:\n",
    "    model_phases = [model_phases]\n",
    "\n",
    "x, y = get_apd_data_feature_fusion(metrics, model_phases)\n",
    "num_subjects = y.shape[0]\n",
    "num_phases = x.shape[0] // num_subjects\n",
    "num_rankings = y.shape[1]\n",
    "x_train, y_train, x_test, y_test, test_subjects = format_input_for_ranking(x, y, metrics)\n",
    "num_train_subjects = num_subjects - len(test_subjects)\n",
    "groups = [y_train.shape[0]//num_train_subjects//num_phases for _ in range(num_train_subjects*num_phases)]\n",
    "\n",
    "# print(len(groups))\n",
    "# print(groups[0])\n",
    "\n",
    "# print(x_train.head())\n",
    "# print(y_train.head())\n",
    "# print(x_test.head())\n",
    "# print(y_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRanker(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None, colsample_bylevel=1,\n",
       "          colsample_bynode=1, colsample_bytree=1, early_stopping_rounds=None,\n",
       "          enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "          gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;, importance_type=None,\n",
       "          interaction_constraints=&#x27;&#x27;, learning_rate=0.1, max_bin=256,\n",
       "          max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
       "          max_depth=6, max_leaves=0, min_child_weight=1, missing=nan,\n",
       "          monotone_constraints=&#x27;()&#x27;, n_estimators=100, n_jobs=0,\n",
       "          num_parallel_tree=1, objective=&#x27;rank:ndcg&#x27;, predictor=&#x27;auto&#x27;, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRanker</label><div class=\"sk-toggleable__content\"><pre>XGBRanker(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None, colsample_bylevel=1,\n",
       "          colsample_bynode=1, colsample_bytree=1, early_stopping_rounds=None,\n",
       "          enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "          gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;, importance_type=None,\n",
       "          interaction_constraints=&#x27;&#x27;, learning_rate=0.1, max_bin=256,\n",
       "          max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
       "          max_depth=6, max_leaves=0, min_child_weight=1, missing=nan,\n",
       "          monotone_constraints=&#x27;()&#x27;, n_estimators=100, n_jobs=0,\n",
       "          num_parallel_tree=1, objective=&#x27;rank:ndcg&#x27;, predictor=&#x27;auto&#x27;, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRanker(base_score=0.5, booster='gbtree', callbacks=None, colsample_bylevel=1,\n",
       "          colsample_bynode=1, colsample_bytree=1, early_stopping_rounds=None,\n",
       "          enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "          gamma=0, gpu_id=-1, grow_policy='depthwise', importance_type=None,\n",
       "          interaction_constraints='', learning_rate=0.1, max_bin=256,\n",
       "          max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
       "          max_depth=6, max_leaves=0, min_child_weight=1, missing=nan,\n",
       "          monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "          num_parallel_tree=1, objective='rank:ndcg', predictor='auto', ...)"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBRanker(  \n",
    "    # tree_method='gpu_hist',\n",
    "    # booster='gbtree',\n",
    "    objective='rank:ndcg',\n",
    "    n_estimators=100, \n",
    "    random_state=42, \n",
    "    learning_rate=0.1,\n",
    "    # colsample_bytree=0.9, \n",
    "    # eta=0.05, \n",
    "    # max_depth=6, \n",
    "    # subsample=0.75 \n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train.iloc[:, 1], group=groups, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, model):\n",
    "    subjects = data.loc[:, \"subject\"].unique()\n",
    "    subject_list = list()\n",
    "    phase_ids = list()\n",
    "    ranks = list()\n",
    "    for subject in subjects:\n",
    "        df = data.loc[data[\"subject\"] == subject]\n",
    "        pred = model.predict(df)\n",
    "        phaseId = np.array(df.reset_index()['phaseId'])\n",
    "        pred = np.argsort(pred)  # lowest to highest anxiety\n",
    "        phase_ids.extend(list(phaseId[pred]))\n",
    "        subject_list.extend([subject]*len(pred))\n",
    "        ranks.extend(list(range(1, len(pred)+1)))\n",
    "\n",
    "    results = pd.DataFrame({'subject': subject_list, 'phaseId': phase_ids, 'ranking': ranks})\n",
    "    results = results.sort_values(by=[\"subject\", \"phaseId\"])\n",
    "    \n",
    "    return results\n",
    "\n",
    "predicted = predict(x_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subject  phaseId  ranking  actual  subject  phaseId  ranking  actual  \\\n",
      "0     18.0      1.0        4       1     22.0      1.0        4       1   \n",
      "1     18.0      2.0        3       2     22.0      2.0        3       2   \n",
      "2     18.0      3.0        6       3     22.0      3.0        6       3   \n",
      "3     18.0      4.0        8       4     22.0      4.0        8       4   \n",
      "4     18.0      5.0        2       5     22.0      5.0        2       5   \n",
      "5     18.0      6.0        1       6     22.0      6.0        1       6   \n",
      "6     18.0      7.0        7       7     22.0      7.0        7       7   \n",
      "7     18.0      8.0        9       8     22.0      8.0        9       8   \n",
      "8     18.0      9.0        5       9     22.0      9.0        5       9   \n",
      "\n",
      "   subject  phaseId  ranking  actual  subject  phaseId  ranking  actual  \\\n",
      "0     35.0      1.0        4       1     85.0      1.0        4       1   \n",
      "1     35.0      2.0        3       2     85.0      2.0        3       2   \n",
      "2     35.0      3.0        6       3     85.0      3.0        6       3   \n",
      "3     35.0      4.0        8       4     85.0      4.0        8       4   \n",
      "4     35.0      5.0        2       5     85.0      5.0        2       5   \n",
      "5     35.0      6.0        1       6     85.0      6.0        1       6   \n",
      "6     35.0      7.0        7       7     85.0      7.0        7       7   \n",
      "7     35.0      8.0        9       8     85.0      8.0        9       8   \n",
      "8     35.0      9.0        5       9     85.0      9.0        5       9   \n",
      "\n",
      "   subject  phaseId  ranking  actual  \n",
      "0     88.0      1.0        5       1  \n",
      "1     88.0      2.0        4       2  \n",
      "2     88.0      3.0        7       3  \n",
      "3     88.0      4.0        9       4  \n",
      "4     88.0      5.0        2       5  \n",
      "5     88.0      6.0        1       6  \n",
      "6     88.0      7.0        6       7  \n",
      "7     88.0      8.0        8       8  \n",
      "8     88.0      9.0        3       9  \n"
     ]
    }
   ],
   "source": [
    "subjects = np.sort(predicted.loc[:, 'subject'].unique())\n",
    "dfs = []\n",
    "for subject in subjects:\n",
    "    subject_df = predicted.loc[predicted['subject'] == subject].reset_index(drop=True)\n",
    "    label = y_test[y_test['subject'] == subject].iloc[:, 1].reset_index(drop=True)\n",
    "    subject_df['actual'] = label\n",
    "    dfs.append(subject_df)\n",
    "\n",
    "df_view = pd.concat(dfs, axis=1)\n",
    "print(df_view)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0877744b127b3e06bbb46a4d58ff9f96d2144cab151d4d0cf00260a00e80ed7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
